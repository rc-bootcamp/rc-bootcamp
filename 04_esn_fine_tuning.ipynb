{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Fine-Tuning the Echo State Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter you will learn how to tune ESN parameters and hyperparameters to maximize ESN performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit the ESN setup for solving NARMA10 and list its parameters.\n",
    "We consider the task where an input $u[k] \\sim \\mathcal{U}([-1,1])$ is linearly projected into an ESN and the NARMA10 series $y[k]$ is reconstructed from the reservoir state $x[k]$.\n",
    "The following parameters appear in this setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $v[k] = \\sigma u[k] + \\phi$\n",
    "  - $\\sigma \\in \\mathbb{R}$: Input scale\n",
    "  - $\\phi \\in \\mathbb{R}$: Bias\n",
    "- $x[k+1] = (1-a)~x[k] + a~\\tanh\\left(\\rho W^\\mathrm{rec} x[k] + W^\\mathrm{in} v[k+1]\\right)$\n",
    "  - $N \\in \\mathbb{Z}^+$: Number of ESN nodes\n",
    "  - $\\rho \\in \\mathbb{R}^+$: Spectral radius\n",
    "  - $a \\in \\mathbb{R}^+$: Leaky rate\n",
    "  - $x[0]$: Initial state of ESN\n",
    "- $y[k+1] = \\mathrm{NARMA10}(\\nu[k], \\nu[k-9],~y[k],~\\ldots,~y[k-9]; \\alpha, \\beta, \\gamma, \\delta),~\\nu[k] = \\mu u[k] + \\kappa$\n",
    "  - $\\alpha, \\beta, \\gamma, \\delta \\in \\mathbb{R}$: Function parameters\n",
    "  - $\\mu, \\kappa \\in \\mathbb{R}$: Scaling parameters\n",
    "- Miscellaneous\n",
    "  - $T_\\mathrm{washout} \\in \\mathbb{Z}^+$: Total time steps of washout\n",
    "  - $T_\\mathrm{train} \\in \\mathbb{Z}^+$: Total time steps of training data\n",
    "  - $T_\\mathrm{eval} \\in \\mathbb{Z}^+$: Total time steps of evaluation (validation) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a fair comparison, we fix the parameters of the target NARMA10 to typical values $(\\alpha,\\beta,\\gamma,\\delta,\\mu,\\kappa)=(0.3,0.05,1.5,0.1,0.25,0.25)$, and use the same input $u[k]$ and the corresponding time series $y[k]$ in the following runs.\n",
    "Below, we will optimize the remaining important parameters individually or collectively: input scaling $(\\sigma, \\phi)$, ESN parameters $(N, \\rho, a)$, and the length of training data $T_\\mathrm{train}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises and demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move to exercises and demonstrations.\n",
    "You can import the previously implemented `ESN`, `Linear`, and `narma_func` here as before.\n",
    "Please run the next cell first.\n",
    "If you want to inspect their implementations, uncomment the lines after `import inspect` or use `...?? / ??...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import drive  # type: ignore\n",
    "\n",
    "    if False:  # Set to True if you want to use Google Drive and save your work there.\n",
    "        drive.mount(\"/content/gdrive\")\n",
    "        %cd /content/gdrive/My Drive/rc-bootcamp/\n",
    "        # NOTE: Change it to your own path if you put the zip file elsewhere.\n",
    "        # e.g., %cd /content/gdrive/My Drive/[PATH_TO_EXTRACT]/rc-bootcamp/\n",
    "    else:\n",
    "        pass\n",
    "        %cd /content/\n",
    "        !git clone --branch en https://github.com/rc-bootcamp/rc-bootcamp.git\n",
    "        %cd /content/rc-bootcamp/\n",
    "else:\n",
    "    sys.path.append(\".\")\n",
    "\n",
    "from utils.reservoir import ESN, Linear\n",
    "from utils.style_config import Figure, plt\n",
    "from utils.task import narma_func\n",
    "from utils.tester import load_from_chapter_name\n",
    "from utils.tqdm import tqdm, trange\n",
    "\n",
    "test_func, show_solution = load_from_chapter_name(\"04_esn_fine_tuning\")\n",
    "\n",
    "# Uncomment it to see the implementations of `Linear` and `ESN`.\n",
    "# import inspect\n",
    "# print(inspect.getsource(Linear))\n",
    "# print(inspect.getsource(ESN))\n",
    "# print(inspect.getsource(narma_func))\n",
    "\n",
    "# Or just use ??.../...?? (uncomment the following lines).\n",
    "# Linear??\n",
    "# ESN??\n",
    "# narma_func??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparation and batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we will repeat similar operations over many related datasets.\n",
    "As preparation, we extend `LRReadout` and `calc_nrmse` so they can perform linear regression and calculation of NRMSE on multiple datasets at once, giving `BatchLRReadout` and `calc_batch_nrmse`.\n",
    "This kind of batch processing is straightforward in NumPy once you get the broadcasting patterns right.\n",
    "It often outperforms explicit Python loops due to vectorized operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.1.\n",
    "\n",
    "As learned previously, for the predictor matrix $\\tilde{X}=[1 : X]\\in\\mathbb{R}^{T\\times (N+1)}$ (with an added bias column) and the target sequence $Y\\in \\mathbb{R}^{T \\times D}$, the weights $\\hat{w}\\in\\mathbb{R}^{(N+1)\\times D}$ that minimize $\\|Xw - Y\\|^2$ are given by\n",
    "\n",
    "$$\n",
    "\\renewcommand{\\Rho}{\\mathrm{P}}\n",
    "\\begin{align*}\n",
    "\\hat{w} &= (\\tilde{X}^\\top \\tilde{X})^{-1}{\\tilde{X}}^\\top Y \\\\\n",
    "&=\\tilde{X}^+ Y\n",
    ".\\end{align*}\n",
    "$$\n",
    "\n",
    "However, due to the specifications of [`np.linalg.lstsq`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html), the previously implemented `LRReadout` cannot take multidimensional arrays $X\\in\\mathbb{R}^{... \\times T \\times (N+1)},~Y\\in \\mathbb{R}^{... \\times T \\times D}$ as arguments to obtain $\\hat{w}^\\mathrm{out} \\in \\mathbb{R}^{...\\times (N+1) \\times D}$ in batch processing.\n",
    "On the other hand, [`np.linalg.pinv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html) allows for input of multidimensional arrays $A\\in\\mathbb{R}^{...\\times M \\times N}$, returning the pseudo-inverse $A^+\\in\\mathbb{R}^{...\\times N \\times M}$.\n",
    "Based on the above equation and the answers from the previous chapter, fill in the blanks in the following code to complete `BatchLRReadout`, which enables batch processing for linear regression.\n",
    "\n",
    "- `BatchLRReadout.train`\n",
    "  - Argument(s):\n",
    "    - `x`: `np.ndarray`\n",
    "      - `shape`: `(..., time_steps, input_dim)`\n",
    "    - `y`: `np.ndarray`\n",
    "      - `shape`: `(..., time_steps, output_dim)`\n",
    "  - Return(s):\n",
    "    - `self.weight`: `np.ndarray`\n",
    "      - `shape`: `(..., output_dim, input_dim)`\n",
    "    - `self.bias`: `np.ndarray`\n",
    "      - `shape`: `(..., 1, output_dim)`\n",
    "\n",
    "  - Operation(s):\n",
    "      - Update `self.weight` with the obtained weight.\n",
    "      - Update `self.bias` with the obtained bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLRReadout(Linear):\n",
    "    def train(self, x: np.ndarray, y: np.ndarray):\n",
    "        assert (x.ndim > 1) and (x.shape[-1] == self.input_dim)\n",
    "        assert (y.ndim > 1) and (y.shape[-1] == self.output_dim)\n",
    "        x_biased = np.ones((*x.shape[:-1], x.shape[-1] + 1), dtype=self.dtype)\n",
    "        x_biased[..., 1:] = x\n",
    "        # TODO\n",
    "        sol = ...\n",
    "        self.weight = ...\n",
    "        self.bias = ...\n",
    "        # end of TODO\n",
    "        return self.weight, self.bias\n",
    "\n",
    "\n",
    "def solution(dim_in, dim_out, x_train, y_train, x_eval):\n",
    "    # DO NOT CHANGE HERE.\n",
    "    readout = BatchLRReadout(dim_in, dim_out)\n",
    "    readout.train(x_train, y_train)\n",
    "    return readout(x_eval)\n",
    "\n",
    "\n",
    "test_func(solution, \"01_01\")\n",
    "# show_solution(\"01_01\", \"BatchLRReadout\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.2.\n",
    "\n",
    "Similarly, implement batch processing of NRMSE calculation as `calc_batch_nrmse`, taking multidimensional arrays $Y, \\hat{Y}\\in\\mathbb{R}^{...\\times T \\times d}$ of length $T$ as arguments.\n",
    "The NRMSE is given by the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{NRMSE}(y, \\hat{y}) :&= \\dfrac{\\mathrm{RMSE}(y, \\hat{y})}{\\sigma(y)} \\\\\n",
    "\\mathrm{RMSE}(y, \\hat{y}) :&=  \\sqrt{\\dfrac{\\mathrm{RSS}(y, \\hat{y}) }{T} }\n",
    ".\\end{align*}\n",
    "$$\n",
    "\n",
    "- `calc_batch_nrmse`\n",
    "  - Argument(s):\n",
    "    - `y`: `np.ndarray`\n",
    "      - `shape`: `(..., t, d)`\n",
    "    - `yhat`: `np.ndarray`\n",
    "      - `shape`: `(..., t, d)`\n",
    "  - Return(s):\n",
    "    - `nrmse`: `np.ndarray`\n",
    "      - `shape`: `(..., d)`\n",
    "\n",
    "<details><summary>tips</summary>\n\n",
    "- [`np.mean`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)\n",
    "- [`np.var`](https://numpy.org/doc/stable/reference/generated/numpy.var.html)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_batch_nrmse(y, yhat):\n",
    "    # TODO\n",
    "    ...\n",
    "\n",
    "\n",
    "test_func(calc_batch_nrmse, \"01_02\")\n",
    "# show_solution(\"01_02\", \"calc_batch_nrmse\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.3.\n",
    "\n",
    "Implement the `train_and_eval` function that performs sampling, batch linear regression, and batch calculation of NRMSE for ESN state sequences $x[k] \\in \\mathbb{R}^{... T_\\mathrm{train}\\times N} $.\n",
    "Refer to the code provided below, which originally appeared at the end of the previous chapter.\n",
    "\n",
    "```python\n",
    "x = x_init\n",
    "xs = np.zeros((t_total, dim_esn))\n",
    "for idx in range(t_total):\n",
    "    x = net(x, w_in(us[idx]))\n",
    "    xs[idx] = x\n",
    "x_train, y_train = xs[t_washout:-t_eval], ys[t_washout:-t_eval]\n",
    "x_eval, y_eval = xs[-t_eval:], ys[-t_eval:]\n",
    "w_out.train(x_train, y_train)\n",
    "```\n",
    "\n",
    "- `train_and_eval`\n",
    "  - Argument(s):\n",
    "    - `x0`: `np.ndarray`\n",
    "      - `shape`: `(..., n)`\n",
    "    - `ys`: `np.ndarray`\n",
    "      - `shape`: `(...., t_washout + t_train + t_eval, d)`\n",
    "  - Return(s):\n",
    "    - `nrmse`: `np.ndarray`\n",
    "      - `shape`: `(..., d)`\n",
    "    - `xs`: `np.ndarray`\n",
    "      - `shape`: `(..., t_washout + t_train + t_eval, n)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_setup(seed, dim, rho, a=None, f=np.tanh, bound=1.0, bias=0.0, cls=BatchLRReadout):\n",
    "    rnd = np.random.default_rng(seed)\n",
    "    w_in = Linear(1, dim, bound=bound, bias=bias, rnd=rnd)\n",
    "    net = ESN(dim, sr=rho, f=f, a=a, rnd=rnd)\n",
    "    w_out = cls(dim, 1)\n",
    "    return w_in, net, w_out\n",
    "\n",
    "\n",
    "def sample_dataset(\n",
    "    seed,\n",
    "    t_washout=1000,\n",
    "    t_train=2000,\n",
    "    t_eval=1000,\n",
    "    narma_parameters=None,\n",
    "):\n",
    "    narma_parameters = (\n",
    "        narma_parameters\n",
    "        if narma_parameters is not None\n",
    "        else dict(alpha=0.3, beta=0.05, gamma=1.5, delta=0.1, mu=0.25, kappa=0.25)\n",
    "    )\n",
    "    rnd = np.random.default_rng(seed)\n",
    "    t_total = t_washout + t_train + t_eval\n",
    "    ts = np.arange(-t_washout, t_train + t_eval)\n",
    "    us = rnd.uniform(-1, 1, (t_total, 1))\n",
    "    ys = narma_func(us, np.zeros((10, 1)), **narma_parameters)\n",
    "    time_info = dict(t_washout=t_washout, t_train=t_train, t_eval=t_eval)\n",
    "    return ts, us, ys, time_info\n",
    "\n",
    "\n",
    "def sample_dynamics(x0, w_in, net, ts, vs, display=False):\n",
    "    assert vs.shape[-2] == ts.shape[0]\n",
    "    x = x0\n",
    "    xs = np.zeros((*x.shape[:-1], ts.shape[0], x.shape[-1]))\n",
    "    for idx in trange(ts.shape[0], display=display):\n",
    "        x = ...  # TODO Iterate over `ts` to sample the dynamics.\n",
    "        xs[..., idx, :] = ...  # TODO Store the state `x` at each time step.\n",
    "    return xs\n",
    "\n",
    "\n",
    "def eval_nrmse(xs, ys, w_out, time_info, return_out=False, **kwargs):\n",
    "    t_washout, t_eval = time_info[\"t_washout\"], time_info[\"t_eval\"]\n",
    "    x_train, y_train = ...  # TODO Specify training range.\n",
    "    x_eval, y_eval = ...  # TODO Specify evaluation range.\n",
    "    out = w_out.train(x_train, y_train, **kwargs)\n",
    "    y_out = w_out(x_eval)\n",
    "    nrmse = calc_batch_nrmse(y_eval, y_out)\n",
    "    if return_out:\n",
    "        return nrmse, *out\n",
    "    else:\n",
    "        return nrmse\n",
    "\n",
    "\n",
    "def train_and_eval(x0, w_in, net, w_out, ts, vs, ys, time_info, display=False):\n",
    "    assert vs.shape[-2] == ts.shape[0]\n",
    "    assert ys.shape[-2] == ts.shape[0]\n",
    "    xs = sample_dynamics(x0, w_in, net, ts, vs, display=display)\n",
    "    nrmse = eval_nrmse(xs, ys, w_out, time_info)\n",
    "    return nrmse, xs\n",
    "\n",
    "\n",
    "test_func(train_and_eval, \"01_03\", multiple_output=True)\n",
    "# show_solution(\"01_03\", \"sample_dynamics\")  # Uncomment it to see the solution.\n",
    "# show_solution(\"01_03\", \"eval_nrmse\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Input scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the effect of the input scaling parameters $(\\sigma, \\phi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we explain the role of these parameters.\n",
    "As stated earlier, the input $u[k]$ is transformed using $(\\sigma,\\phi)$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "v[k]= \\sigma u[k] + \\phi\n",
    ".\\end{align*}\n",
    "$$\n",
    "\n",
    "Because $u[k]\\sim\\mathcal{U}([-1, 1])$, the transformed input $v[k]$ follows $v[k]\\sim\\mathcal{U}([-\\sigma + \\phi, \\sigma + \\phi])$.\n",
    "Therefore, $\\sigma$ adjusts the variance of the input, and $\\phi$ adjusts the mean (variance $\\mathrm{Var}[v]=\\frac{1}{3}\\sigma^2$, mean $\\mathrm{E}[v]=\\phi$).\n",
    "The effect of these parameters can be discussed in relation to the \"shape\" of the activation function.\n",
    "For example, $\\tanh$ is an odd function (i.e., $\\tanh\\left(-y\\right)=-\\tanh\\left(y\\right)$), as shown in Figure 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; width: 750px; margin: auto; background-color: #f8f9fa; padding: 10px; border-radius: 10px;\">\n",
    "\n",
    "<img src=\"data:image/webp;base64,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\" alt=\"fig_4_1.webp\">",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figcaption align = \"center\">\n",
    "\n",
    "Figure 1 \"Shape\" of $f(x)=\\tanh(x)$ and its derivative $\\dfrac{df}{dx}(x)=1-\\tanh^2(x)$\n",
    "\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By expanding $v[k]$, the temporal evolution of $x[k]$ can be expressed as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x[k+1] = \\tanh\\left(\\rho W^\\mathrm{rec} x[k] + \\sigma W^\\mathrm{in} u[k+1] + \\phi W^\\mathrm{in}\\right)\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by focusing on the characteristics of $\\sigma$, by fixing $\\phi=0$.\n",
    "As can be seen from the equation and the figure, the larger $\\sigma$ is, the more the domain of $\\tanh$ extends into its \"flat\" range.\n",
    "Conversely, the smaller $\\sigma$ is, the more $\\tanh$ argument stays in its near-linear region, where $\\tanh\\approx x$.\n",
    "Therefore, $\\sigma$ has a significant impact on the nonlinearity of the ESN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's examine the effect of $\\phi$.\n",
    "This will be explained in more detail in a later chapter on information processing capacity (IPC), but in short, $\\phi$ influences the order of components included in the transformation.\n",
    "Given that the input is symmetric (mean 0), if we polynomially expand $x[k]$ using $\\{ u[k],~u[k-1].~\\ldots\\}$, only odd-order elements (such as $u^3[\\cdot], u^5[\\cdot], u[\\cdot]u^2[\\cdot]$) will appear in the case of $\\phi=0$.\n",
    "In contrast, the input will no longer have a zero mean when $\\phi \\neq 0$, and so even-order elements (such as $u^2[\\cdot], u^4[\\cdot]$) will appear as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such distributions of components have a significant impact on task performance.\n",
    "For example, analysis has shown that the NARMA10 task with the current setting of parameters is largely composed of the following components<sup>[1]</sup>:\n",
    "- 1st-order: $u[k-1],~u[k-2],~u[k-3],~u[k-10],~u[k-11],~u[k-12]$\n",
    "- 2nd-order: $u[k-1]u[k-10],~u[k-2]u[k-11],~u[k-3]u[k-12]$\n",
    "\n",
    "Therefore, there is a significant performance difference between the cases of $\\phi=0$ and $\\phi\\neq0$.\n",
    "In the following exercise, let's verify the effects of $(\\sigma, \\phi)$ that we have discussed so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.1.\n",
    "\n",
    "Parameters $\\Sigma = (\\sigma_0$, $\\sigma_1,~\\ldots,~\\sigma_{k-1})$ and $\\Phi = (\\phi_0$, $\\phi_1,~\\ldots,~\\phi_{k-1})$ are given as arrays of length $k$.\n",
    "Fill in the blanks in the following code to complete the function `convert_us_into_vs`, which outputs $v_i[k] = \\sigma_i u[k] + \\phi_i$ for each $(\\sigma_i, \\phi_i)$ at once.\n",
    "Note that you should pay attention to the `shape` of the given input `us`, which is $U=\\{u[0],~\\ldots,~u[k-1]\\} \\in \\mathbb{R}^{T \\times 1}$.\n",
    "\n",
    "- `convert_us_into_vs`\n",
    "  - Argument(s):\n",
    "    - `us`: `np.ndarray`\n",
    "      - `shape`: `(t, 1)`\n",
    "    - `sigma`: `np.ndarray`\n",
    "      - `shape`: `(k,)`\n",
    "    - `phi`: `np.ndarray`\n",
    "      - `shape`: `(k,)`\n",
    "  - Return(s):\n",
    "    - `vs`: `np.ndarray`\n",
    "      - `shape`: `(k, t, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_us_into_vs(us, sigma, phi):\n",
    "    assert len(sigma) == len(phi)\n",
    "    vs = ...  # TODO Use broadcasting to convert `us` into `vs`.\n",
    "    return vs\n",
    "\n",
    "\n",
    "test_func(convert_us_into_vs, \"02_01\")\n",
    "# show_solution(\"02_01\", \"convert_us_into_vs\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the implemented `convert_us_into_vs`, let's compare the symmetric case ($v[k]\\sim \\mathcal{U}([-\\sigma, \\sigma])$) with the asymmetric case ($v[k]\\sim \\mathcal{U}([0, \\sigma])$).\n",
    "When $\\sigma$ is small, the asymmetric case outperforms the symmetric case in terms of task accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_setup, seed_dataset = 1234, 5678  # you can freely change here\n",
    "dim, rho = 100, 0.9\n",
    "dataset_info = dict(t_washout=100, t_train=2000, t_eval=1000)\n",
    "\n",
    "w_in, net, w_out = create_setup(seed_setup, dim, rho, f=np.tanh)\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "\n",
    "\n",
    "sigmas = np.logspace(-2, 0, 21)  # 10^{-2.0}, 10^{-1.9}, ... 10^{0.0}\n",
    "x0 = np.zeros((sigmas.shape[0], net.dim))\n",
    "\n",
    "# Symmetrical case (phi=0).\n",
    "vs_sym = convert_us_into_vs(us, sigmas, np.zeros_like(sigmas))\n",
    "nrmse_sym, _xs_sym = train_and_eval(x0, w_in, net, w_out, ts, vs_sym, ys, time_info)\n",
    "best_sym = np.argmin(nrmse_sym[:, 0])\n",
    "\n",
    "# Asymmetrical case (phi=sigma).\n",
    "vs_asym = convert_us_into_vs(us, 0.5 * sigmas, 0.5 * sigmas)\n",
    "nrmse_asym, _xs_asym = train_and_eval(x0, w_in, net, w_out, ts, vs_asym, ys, time_info)\n",
    "best_asym = np.argmin(nrmse_asym[:, 0])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.plot(sigmas, nrmse_sym[:, 0], label=r\"sym: $[-\\sigma, \\sigma]$\")\n",
    "ax.plot(sigmas, nrmse_asym[:, 0], label=r\"asym: $[0, \\sigma]$\")\n",
    "ax.scatter(sigmas[best_sym], nrmse_sym[best_sym, 0], s=100.0, marker=\"*\")\n",
    "ax.scatter(sigmas[best_asym], nrmse_asym[best_asym, 0], s=100.0, marker=\"*\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(r\"$\\sigma$\")\n",
    "ax.set_ylabel(r\"NRMSE (best: $\\bigstar$)\")\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.2. (Advanced)\n",
    "\n",
    "- Try the same experiment for other nonlinear activation functions. In particular, try an even activation function and observe the effect.\n",
    "- The accuracy deteriorates quickly when $\\sigma$ is large. Discuss why this happens, by comparing the time series of $x[k]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ESN parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's examine the role of the ESN's internal parameters $(N, \\rho, a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of ESN nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the performance of RC improves as the **dimensionality** $N$ increases.\n",
    "As will be discussed in the IPC chapter later, this is because the upper limit of IPC is determined by the dimensionality $N$ of the reservoir.\n",
    "Therefore, the size of $N$ is a significant parameter that determines the performance of the reservoir.\n",
    "\n",
    "Executing the next cell will plot the change in NRMSE according to the change in $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_setup, seed_dataset = 1234, 5678\n",
    "dims = [10, 20, 50, 100, 200, 500, 1000]\n",
    "rho = 0.9\n",
    "dataset_info = dict(t_washout=100, t_train=2000, t_eval=1000)\n",
    "\n",
    "w_in, net, w_out = create_setup(seed_setup, dim, rho, f=np.tanh)\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "vs = 0.05 * us + 0.05  # vs \\in [0.0, 0.1]\n",
    "\n",
    "nrmses = []\n",
    "for dim in tqdm(dims):\n",
    "    x0 = np.zeros(dim)\n",
    "    w_in, net, w_out = create_setup(seed_setup, dim, rho, f=np.tanh)\n",
    "    nrmse = train_and_eval(x0, w_in, net, w_out, ts, vs, ys, time_info)\n",
    "    nrmses.append(nrmse[0])\n",
    "nrmses = np.asarray(nrmses)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.plot(dims, nrmses, marker=\".\", markersize=10.0)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"# of ESN nodes\")\n",
    "ax.set_ylabel(\"NRMSE\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More precisely, the upper limit of a reservoir's IPC is determined by its rank $r (\\leq N)$.\n",
    "This represents the number of linearly independent states and is usually calculated as the rank of $X$ (when we include the constant bias component) or its covariance matrix $C(X):=\\mathrm{E}[(X-\\mathrm{E}[X])^\\top (X-\\mathrm{E}[X])]$ (when we exclude the constant bias component).\n",
    "For example, consider the following state matrix $X=[{x}_0; {x}_1; {x}_2]^{}\\in\\mathbb{R}^{T\\times 3}$ with dimensionality $N=3$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{x}_0 &= [u[0],~u[1],~\\ldots,~u[T-1]]^\\top, \\\\\n",
    "{x}_1 &= [u^2[0],~u^2[1],~\\ldots,~u^2[T-1]]^\\top, \\\\\n",
    "{x}_2 &= [3u^2[0]-4u[0],~3u^2[1]-4u[1],~\\ldots,~3u^2[T-1]-4u[T-1]] ^\\top\n",
    ".\\end{align}\n",
    "$$\n",
    "\n",
    "Since $x_2=3x_1 - 4x_0$, $x_2$ is linearly dependent and therefore the rank of $X$ is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "In [*]: rnd = np.random.default_rng(1234)\n",
    "   ...: us = rnd.uniform(-1, 1, (100, 1))\n",
    "   ...: xs = np.concatenate([us, us**2, 3 * us**2 - 4 * us], axis=1)\n",
    "   ...: xs_m = xs - xs.mean(axis=0)\n",
    "   ...: print('rank: {}'.format(np.linalg.matrix_rank(xs_m.T @ xs_m)))\n",
    "rank: 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of linear regression, $\\hat{Y}$, can be interpreted as the projection of $Y$ onto the subspace spanned by $X$, expressed as $\\hat{Y}=P_X Y$ using the projection matrix $P_X=X X^+$.\n",
    "The dimension of this subspace is determined by the rank $r$ of $X$.\n",
    "In the $N=3$ example above, the linearly dependent component $x_2$ does not increase the dimension of the subspace and thus does not contribute to the reduction of the residual error $\\|P_X Y - Y \\|^2$.\n",
    "In terms of IPC, $x_2$ is an extra component that does not contribute to an increase in IPC.\n",
    "In this way, the rank $r$ is an indicator for quantitatively evaluating redundant components and appears frequently in the context of RC.\n",
    "\n",
    "A case where the rank is not maximal can be easily constructed using a linear ESN with the identity map as the activation function.\n",
    "The code below compares the rank of a linear ESN with that of a nonlinear ESN using $\\tanh$ as its activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_setup, seed_dataset = 1234, 5678\n",
    "sigma, phi = 0.05, 0.05  # vs \\in [0.0, 0.1]\n",
    "dims = np.arange(1, 11) * 10\n",
    "rho = 0.9\n",
    "dataset_info = dict(t_washout=100, t_train=2000, t_eval=0)\n",
    "\n",
    "ts, us, _ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "vs = sigma * us + phi\n",
    "\n",
    "rank_nlin, rank_lin = [], []\n",
    "for dim in tqdm(dims):\n",
    "    x0 = np.zeros(dim)\n",
    "    t_washout = dataset_info[\"t_washout\"]\n",
    "    w_in, net, _w_out = create_setup(seed_setup, dim, rho, f=np.tanh)\n",
    "    # Non-linear case\n",
    "    xs_nlin = sample_dynamics(x0, w_in, net, ts, vs)[t_washout:]\n",
    "    xs_nlin -= xs_nlin.mean(axis=0)\n",
    "    rank_nlin.append(np.linalg.matrix_rank(xs_nlin.T @ xs_nlin))\n",
    "    # Linear case\n",
    "    net.f = lambda val: val  # Identity function\n",
    "    xs_lin = sample_dynamics(x0, w_in, net, ts, vs)[t_washout:]\n",
    "    xs_lin -= xs_lin.mean(axis=0)\n",
    "    rank_lin.append(np.linalg.matrix_rank(xs_lin.T @ xs_lin))\n",
    "rank_nlin = np.asarray(rank_nlin)\n",
    "rank_lin = np.asarray(rank_lin)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.plot(dims, dims, color=\"k\", label=r\"$r=N$\")\n",
    "ax.plot(dims, rank_nlin, marker=\".\", markersize=10.0, color=\"red\", ls=\":\", label=r\"$f=\\tanh$\")\n",
    "ax.plot(\n",
    "    dims,\n",
    "    rank_lin,\n",
    "    marker=\".\",\n",
    "    markersize=10.0,\n",
    "    color=\"blue\",\n",
    "    ls=\"--\",\n",
    "    label=r\"$f=\\mathrm{id}_\\mathbb{R}$\",\n",
    ")\n",
    "ax.set_xlabel(\"# of ESN nodes\")\n",
    "ax.set_ylabel(r\"rank $r$\")\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.1. (Advanced)\n",
    "\n",
    "- In the linear case, the rank does not become maximal when $N$ is large, as shown in the above example. To understand this cause intuitively, plot the absolute values $|\\lambda_{i}|$ of the eigenvalues $\\{\\lambda_i \\}_{i=0}^{N-1}$ of the covariance matrix $C(X)$ in ascending order on a logarithmic graph. Compare this between the linear and nonlinear ESNs.\n",
    "- Based on the distribution of the absolute values of the eigenvalues, devise a method of initializing $W^\\mathrm{rec}$ that maximizes the rank even in a linear ESN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will examine the effect of the spectral radius $\\rho$.\n",
    "As introduced in the previous chapter, the spectral radius $\\rho$ is defined as the maximum absolute value among the eigenvalues $\\{\\lambda_i \\}_{i=0}^{N-1}$ of $W^\\mathrm{rec}$ using the following formula:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\rho(W^\\mathrm{rec}):=\\max_{i} |\\lambda_{i}|\n",
    ".\\end{align*}\n",
    "$$\n",
    "\n",
    "You can visually understand why it is called the spectral \"radius\" by arranging the eigenvalues on a complex plane.\n",
    "Especially when each element of $W^\\mathrm{rec} \\in \\mathbb{R}^{N\\times N}$ is sampled from the normal distribution $\\mathcal{N}\\left(0, \\frac{1}{N}\\right)$, the eigenvalues are approximately uniformly distributed on a unit disk of radius 1 (more precisely, as $N\\to \\infty$, they almost surely converge to a uniform distribution on the unit disk, a phenomenon known as the [*circular law*](https://en.wikipedia.org/wiki/Circular_law) in random matrix theory).\n",
    "\n",
    "In the class `ESN`, the spectral radius $\\rho$ can be separated and treated as a hyperparameter by setting the option `normalize = True`.\n",
    "To begin with, let's implement the function `reshape_rho` that allows the simultaneous sampling of $x[t]$ for multiple values of $\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.2.\n",
    "\n",
    "An array of spectral radii $\\Rho = \\left(\\rho_0, \\rho_1,~\\ldots,~\\rho_{k-1} \\right)$ with length $k$ is given.\n",
    "Fill in the blanks in the following code to complete `reshape_rho`, which enables the simultaneous sampling of $x_{i}[k]$ when only the spectral radius $\\rho_i$ is changed for the same input $u[k]$ and $W^\\mathrm{rec}$.\n",
    "\n",
    "- `reshape_rho`\n",
    "  - Argument(s):\n",
    "    - `rho`: `np.ndarray`\n",
    "      - `shape`: `(k,)`\n",
    "  - Return(s):\n",
    "    - `rho_new`: `np.ndarray`\n",
    "      - `shape`: `(k, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_rho(rho):\n",
    "    rho_new = ...  # TODO\n",
    "    return rho_new\n",
    "\n",
    "\n",
    "test_func(reshape_rho, \"03_02\")\n",
    "# show_solution(\"03_02\", \"reshape_rho\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To intuitively understand the effect of the spectral radius $\\rho$, let's consider the case of a linear ESN with no input (i.e., $u[k]=0, f=\\mathrm{id}_\\mathbb{R}$).\n",
    "In this case, the temporal evolution of the ESN can be represented by the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x[t+1] = \\rho W^\\mathrm{rec} x[t]\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the analytical solution for $x[k]$ is given as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x[k] = \\rho^k \\left({W^\\mathrm{rec}}\\right)^k x[0]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $W^\\mathrm{rec}$ is normalized, $\\left({W^\\mathrm{rec}}\\right)^k$ itself does not diverge.\n",
    "However, $\\rho^k$ diverges to infinity when $\\rho > 1$ and converges when $\\rho < 1$.\n",
    "The following cell visualizes the response of $x[k]$ according to differences in $\\rho$.\n",
    "For comparison, the case of nonlinear ESN with $\\tanh$ is shown on the right side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_setup, seed_dataset = 1234, 5678\n",
    "dim, rhos = 50, np.array([0.99, 1.0, 1.01])\n",
    "dataset_info = dict(t_washout=300, t_train=0, t_eval=0)\n",
    "\n",
    "w_in, net, _w_out = create_setup(seed_setup, dim, reshape_rho(rhos))  # Use `reshape_rho`.\n",
    "ts, us, _ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "vs = np.zeros_like(us)  # Zero input\n",
    "x0 = net.rnd.uniform(low=-0.5, high=0.5, size=(1, dim))\n",
    "x0 = np.broadcast_to(x0, (len(rhos), dim))  # Using the same initial conditions\n",
    "\n",
    "xs_nlin = sample_dynamics(x0, w_in, net, ts, vs)\n",
    "net.f = lambda val: val  # Identity function\n",
    "xs_lin = sample_dynamics(x0, w_in, net, ts, vs)\n",
    "\n",
    "fig, ax = plt.subplots(len(rhos), 2, figsize=(12, 10), gridspec_kw=dict(hspace=0.05, wspace=0.2))\n",
    "for idx, rho in enumerate(rhos):\n",
    "    ax[idx, 0].set_ylabel(r\"$\\rho = {:.2f}$\".format(rho))\n",
    "    for idy, xs in enumerate([xs_lin, xs_nlin]):\n",
    "        ax[idx, idy].plot(xs[idx], lw=1.0)\n",
    "        ax[idx, idy].axhline(1.0, ls=\"--\", color=\"k\", lw=1.0)\n",
    "        ax[idx, idy].axhline(-1.0, ls=\"--\", color=\"k\", lw=1.0)\n",
    "        ax[idx, idy].set_yticks([-1.0, 1.0])\n",
    "        if idx < len(rhos) - 1:\n",
    "            ax[idx, idy].set_xticklabels([])\n",
    "ax[-1, 0].set_xlabel(\"time steps\")\n",
    "ax[-1, 1].set_xlabel(\"time steps\")\n",
    "ax[0, 0].set_title(r\"$f=\\mathrm{id}_\\mathbb{R}$\")\n",
    "ax[0, 1].set_title(r\"$f=\\tanh$\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, despite having the same initial value $x[0]$, the range of $x[k]$ for $\\rho = 1.01$ deviates significantly from $[-1, 1]$.\n",
    "In the case of a nonlinear ESN, $x[k]$ remains within the range $[-1, 1]$ even when $\\rho \\geq 1$ because the $\\tanh$ function prevents it from diverging to infinity.\n",
    "However, it does not converge to zero as it does when $\\rho < 1$.\n",
    "Here, the ESP does not hold, and the influence of $x[0]$ continues to persist.\n",
    "As will be discussed in later sections, increasing $\\rho$ can lead to *chaos*, a phenomenon characterized by aperiodicity and sensitivity to initial conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, for typical use cases such as NARMA10, the parameter search for $\\rho$ is often restricted to $\\rho < 1$.\n",
    "Now, let's return to considering the case with input.\n",
    "For simplicity, consider a linear ESN with input scaling $(\\sigma, \\phi)=(1, 0)$ and other settings $(f, W^\\mathrm{rec})=(\\mathrm{id}_\\mathrm{R}, I)$.\n",
    "In this case, the temporal evolution of the state is expressed by the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x[k+1] = \\rho x[k] + W^\\mathrm{in} u[k+1]\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the analytical solution for $x[k]$ can be expressed as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x[k] = W^\\mathrm{in} \\sum_{j=0}^\\infty \\rho^{j}  u[k-j]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As inferred from the equation, $\\rho$ is a parameter that controls how long the reservoir holds internal states $x[k]$.\n",
    "The closer $\\rho$ is to 1, the more it retains past inputs.\n",
    "To check this effect, let's now investigate the dependency of NARMA10 accuracy on $\\rho$.\n",
    "The following code plots the NRMSE of the NARMA10 task and the rank of $x[k]$ as $\\rho$ changes from 0 to 1 in increments of 0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_setup, seed_dataset = 1234, 5678\n",
    "sigma, phi = 0.05, 0.05  # vs \\in [0.0, 0.1]\n",
    "dims = [25, 50, 100]\n",
    "rhos = np.linspace(0.0, 1.0, 51)[1:-1]  # 0.02, ..., 0.98\n",
    "rhos_batch = reshape_rho(rhos)  # use `reshape_rhos`.\n",
    "dataset_info = dict(t_washout=100, t_train=2000, t_eval=1000)\n",
    "\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "vs = sigma * us + phi\n",
    "\n",
    "nrmses, best_ids, ranks = [], [], []\n",
    "for dim in tqdm(dims):\n",
    "    w_in, net, w_out = create_setup(seed_setup, dim, rhos_batch, f=np.tanh)\n",
    "    x0 = np.zeros((rhos.shape[0], net.dim))\n",
    "    nrmse, xs = train_and_eval(x0, w_in, net, w_out, ts, vs, ys, time_info)\n",
    "    nrmses.append(nrmse[:, 0])\n",
    "    best_ids.append(np.argmin(nrmse[:, 0]))\n",
    "    xs_m = xs - xs.mean(axis=-2, keepdims=True)\n",
    "    ranks.append(np.linalg.matrix_rank(xs_m.swapaxes(-2, -1) @ xs_m))\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 8), gridspec_kw=dict(hspace=0.05))\n",
    "for dim, nrmse, best_id, rank in zip(dims, nrmses, best_ids, ranks, strict=False):\n",
    "    ax[0].plot(rhos, rank / dim, label=r\"$N={}$\".format(dim))\n",
    "    ax[1].plot(rhos, nrmse, label=r\"$N={}$\".format(dim))\n",
    "    ax[1].scatter(rhos[best_id], nrmse[best_id], s=100.0, marker=\"*\")\n",
    "ax[0].set_xticklabels([])\n",
    "ax[0].set_ylabel(r\"$r/N$\")\n",
    "ax[1].set_xlabel(r\"$\\rho$\")\n",
    "ax[1].set_ylabel(r\"NRMSE (best: $\\bigstar$)\")\n",
    "ax[1].legend(frameon=False)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because NARMA10 requires past input components with order of about 10, the optimal range of $\\rho$ is where it is relatively close to 1, allowing the reservoir to keep past inputs for a longer period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaky rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important parameter in ESNs is $a$, a parameter that controls the extent to which past states are retained (similar to the spectral radius).\n",
    "It takes values in the range $[0, 1]$.\n",
    "In particular, the case of $a=1$ is equivalent to a discrete ESN without the linear term, and the case of $a=0$ is equivalent to a constant $x[k]=\\mathrm{const.}$\n",
    "\n",
    "The leaky rate $a$ often appears in the context of continuous ESNs defined in continuous time and their approximations, as shown below:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tau \\dfrac{dx}{dt}(t) = -x(t) + \\tanh\\left(\\rho W^\\mathrm{rec} x(t) + W^\\mathrm{in} v(t)\\right)\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\tau \\in \\mathbb{R}$ is a time constant.\n",
    "By approximating this equation using the Euler method, it can now be expressed as the following discrete equation.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x(t+\\Delta t) = \\left(1-\\dfrac{\\Delta t}{\\tau}\\right) x(t) + \\dfrac{\\Delta t}{\\tau}\\tanh\\left(\\rho W^\\mathrm{rec} x(t) + W^\\mathrm{in} v(t + \\Delta t)\\right)\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $t := k \\Delta t, a=\\Delta t / \\tau$, this equation becomes equivalent to the original equation.\n",
    "In the context of continuous ESNs, $a=0.1$ is commonly seen, but is not necessarily limited to this value.\n",
    "Similar to $\\rho$, let's implement the function `reshape_lr` that allows the simultaneous sampling of $x[k]$ for multiple values of $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.3.\n",
    "\n",
    "An array of leaky rates $A = \\left(a_0, a_1,~\\ldots,~a_{k-1} \\right) $ with length $k$ is given.\n",
    "Fill in the blanks in the following code to complete `reshape_lr`, which enables the simultaneous sampling of $x_{i}[k]$ when only the leaky rate $a_{i}$ is changed for the same input $u[k]$ and $W^\\mathrm{rec}$.\n",
    "\n",
    "- `reshape_lr`\n",
    "  - Argument(s):\n",
    "    - `lr`: `np.ndarray`\n",
    "      - `shape`: `(k,)`\n",
    "  - Return(s):\n",
    "    - `lr_new`: `np.ndarray`\n",
    "      - `shape`: `(k, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_lr(lr):\n",
    "    lr_new = ...  # TODO\n",
    "    return lr_new\n",
    "\n",
    "\n",
    "test_func(reshape_lr, \"03_03\")\n",
    "# show_solution(\"03_03\", \"reshape_lr\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code plots the NRMSE of NARMA10 tasks when $a$ is changed from $10^{-2}$ to $1$ (discrete ESN).\n",
    "Note that the spectral radius is set to $\\rho=0.5$, to reduce the contribution of internal connections on the reservoir's memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_setup, seed_dataset = 1234, 5678\n",
    "sigma, phi = 0.05, 0.05  # vs \\in [0.0, 0.1]\n",
    "dims = [25, 50, 100]\n",
    "rho = 0.5\n",
    "leaky_rates = np.logspace(-1, 0, 21)\n",
    "lr_batch = reshape_lr(leaky_rates)  # Use `reshape_lr`.\n",
    "dataset_info = dict(t_washout=100, t_train=2000, t_eval=1000)\n",
    "\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "vs = sigma * us + phi\n",
    "\n",
    "nrmses, best_ids = [], []\n",
    "for dim in tqdm(dims):\n",
    "    w_in, net, w_out = create_setup(seed_setup, dim, rho, f=np.tanh, a=lr_batch)\n",
    "    x0 = np.zeros((leaky_rates.shape[0], net.dim))\n",
    "    nrmse, xs = train_and_eval(x0, w_in, net, w_out, ts, vs, ys, time_info)\n",
    "    nrmses.append(nrmse[:, 0])\n",
    "    best_ids.append(np.argmin(nrmse[:, 0]))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "for dim, nrmse, best_id in zip(dims, nrmses, best_ids, strict=False):\n",
    "    ax.plot(leaky_rates, nrmse, label=r\"$N={}$\".format(dim))\n",
    "    ax.scatter(leaky_rates[best_id], nrmse[best_id], s=100.0, marker=\"*\")\n",
    "ax.set_xlabel(r\"$a$\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_ylabel(r\"NRMSE (best: $\\bigstar$)\")\n",
    "ax.legend(frameon=False)\n",
    "ax.grid(True, which=\"minor\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.4. (Advanced)\n",
    "\n",
    "- Try experiments with tasks requiring shorter time constants (such as NARMA10), and observe how the dependency on $\\rho$ and $a$ changes.\n",
    "- Euler's method can be generalized as a [1st-order Runge-Kutta (RK) method](https://en.wikipedia.org/wiki/List_of_Runge%E2%80%93Kutta_methods). Apply higher-order RK methods (e.g., RK4) to the differential equation of ESN in continuous time to implement a discretized ESN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the effect of the dataset size $T_\\textrm{train}$.\n",
    "In general, more accurate evaluations become possible as the time series for training data gets longer.\n",
    "The following code plots the NRMSE of NARMA10 tasks for different lengths of training data, $T_\\textrm{train}= (100, 200, 400, 800, 1600, 3200)$.\n",
    "Other parameters are set to $(\\sigma, \\phi, N, \\rho) = (0.05, 0.05, 50, 0.9)$, and 100 samples of $(W^\\mathrm{in}, W^\\mathrm{rec})$ are generated randomly and evaluated by default.\n",
    "It may take a few minutes to complete.\n",
    "If it takes too long, try reducing the value of `sample_num` or `dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 100\n",
    "seed_setup, seed_dataset = 1234, 5678\n",
    "t_trains = [100, 200, 400, 800, 1600, 3200]\n",
    "sigma, phi = 0.05, 0.05  # vs \\in [0.0, 0.1]\n",
    "dim, rho = 50, 0.9\n",
    "dataset_info = dict(t_washout=100, t_train=max(t_trains), t_eval=1000)\n",
    "\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "w_in_ws, net_ws = [], []\n",
    "for seed in trange(sample_num):  # Create setups for `sample_num` times.\n",
    "    w_in, net, _w_out = create_setup(seed_setup + seed, dim, rho, f=np.tanh)\n",
    "    w_in_ws.append(w_in.weight)\n",
    "    net_ws.append(net.weight)\n",
    "\n",
    "w_in_batch = Linear(1, dim)\n",
    "w_in_batch.weight = np.array(w_in_ws)[:, None, :, :]  # [*bs, 1, N, 1] ([1] -> [*bs, 1, N])\n",
    "net_batch = ESN(dim, sr=rho)\n",
    "net_batch.weight = np.array(net_ws)  # [bs, N, N] ([*bs, 1, N] -> [*bs, 1, N])\n",
    "w_out = BatchLRReadout(dim, 1)\n",
    "\n",
    "vs = sigma * us + phi\n",
    "x0 = np.zeros((sample_num, 1, net.dim))\n",
    "xs = sample_dynamics(x0, w_in_batch, net_batch, ts, vs, display=True)\n",
    "nrmse_dict = {}\n",
    "for t_train in t_trains:\n",
    "    time_info[\"t_washout\"] = dataset_info[\"t_washout\"] + max(t_trains) - t_train\n",
    "    nrmse = eval_nrmse(xs, ys, w_out, time_info=time_info)[:, 0, 0]\n",
    "    nrmse_dict[t_train] = nrmse\n",
    "    print(\"t_train={}: {:.3e}{:.3e} (#sample={})\".format(t_train, nrmse.mean(), nrmse.std(), nrmse.size))\n",
    "\n",
    "nrmse_aves = [nrmse.mean() for nrmse in nrmse_dict.values()]\n",
    "nrmse_stds = [nrmse.std() for nrmse in nrmse_dict.values()]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.errorbar(t_trains, nrmse_aves, nrmse_stds)\n",
    "ax.set_xlabel(r\"$T_\\mathrm{train}$\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_ylabel(\"NRMSE\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.grid(True, which=\"both\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $T_\\mathrm{train}$ is too small, the performance is poor and unstable, with a large variance.\n",
    "This suggests that the condition $N \\geq T_\\mathrm{train}$ for achieving the maximum rank is not sufficient, and that longer time samples are **also** needed.\n",
    "In practice, it is desirable for $T_\\mathrm{train}$ to be much larger than $N$.\n",
    "Empirically, a length more than 10 times $N$ is often used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4.1. (Advanced)\n",
    "\n",
    "- Increase $N$, and investigate the relationship between $T_\\mathrm{train}$ and the NRMSE in a similar way.\n",
    "- Conduct similar investigations for other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Addressing short time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *\"I remember my friend Johnny von Neumann used to say, with four parameters I can fit an elephant, and with five I can make him wiggle his trunk.\"*\n",
    ">  [Enrico Fermi, 1953](https://en.wikipedia.org/wiki/Von_Neumann%27s_elephant)\n",
    "\n",
    "In the previous section, we highlighted the importance of large $T_\\mathrm{train}$.\n",
    "However, in reality, it can be difficult to obtain training data with sufficient length relative to the reservoir dimensionality $N$.\n",
    "This typically occurs in research on physical reservoirs, where the intrinsic time constants of physical systems or the sampling frequency of sensors limit the size of $T_\\mathrm{train}$.\n",
    "Conversely, it is also possible to encounter very high-dimensional settings where $N$ is so large that the required $T_\\mathrm{train}$ becomes impractically large, making computation or data storage infeasible.\n",
    "\n",
    "In such situations, it is crucial to avoid the phenomenon called **overfitting**.\n",
    "This refers to the situation where the model fits the training data too closely, resulting in poor generalization performance on evaluation (validation) data.\n",
    "Here, we introduce some techniques to avoid overfitting: **ridge regression** and the automatic adjustment of the ridge parameter using **Akaike information criteria (AIC)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression is a method that adds a regularization term to the cost function $\\mathrm{RSS}(X, Y, w):=\\|Xw - Y\\|^2$ of the least squares problem, imposing a constraint on the size ($L^2$ norm) of the weight parameter $w$.\n",
    "With the ridge parameter $\\lambda(>0)$, the new cost function $\\mathcal{L}^\\mathrm{ridge}$ is defined by the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}^\\mathrm{ridge}(X, Y, w, \\lambda) & := \\mathrm{RSS}(X, Y, w) + \\lambda \\|w\\|^2\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the optimal $\\hat{w}^\\mathrm{ridge}$ that minimizes this cost function can be obtained in one-shot (similar to linear regression) using the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{w}^\\mathrm{ridge}(X, Y, \\lambda):&=\\mathrm{arg}\\min_{w} \\mathcal{L}^\\mathrm{ridge}(X, Y, w, \\lambda) \\\\\n",
    "&=(X^\\top X + \\lambda I)^{-1}X^\\top Y\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gram matrix $X^\\top X$ is a positive semi-definite matrix (all eigenvalues are non-negative), so $X^\\top X + \\lambda I$ is a positive definite matrix and always has an inverse.\n",
    "Therefore, ridge regression is numerically more stable than ordinary linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5.1.\n",
    "\n",
    "Fill in the blanks of the following code to complete `RidgeReadout`, inheriting from `Linear`.\n",
    "This class takes the predictor sequence $\\tilde{X}=[1 : X]\\in\\mathbb{R}^{...\\times T\\times (N+1)}$ (with additional bias term), target sequence $Y\\in \\mathbb{R}^{... \\times T \\times D}$, and ridge parameter $\\lambda \\in \\mathbb{R}^{+}$ as arguments.\n",
    "It calculates $\\hat{w}^\\mathrm{ridge}\\in\\mathbb{R}^{...\\times(N+1)\\times D}$ that minimizes  $\\mathcal{L}^\\mathrm{ridge}(\\tilde{X}, Y, w)$, and then updates its weights and biases.\n",
    "Refer to the implementation of `BatchLRReadout` as well.\n",
    "\n",
    "- `RidgeReadout.train`\n",
    "  - Argument(s):\n",
    "    - `x`: `np.ndarray`\n",
    "      - `shape`: `(..., time_steps, input_dim)`\n",
    "    - `y`: `np.ndarray`\n",
    "      - `shape`: `(..., time_steps, output_dim)`\n",
    "  - Returns(s):\n",
    "    - `self.weight`: `np.ndarray`\n",
    "      - `shape`: `(..., output_dim, input_dim)`\n",
    "    - `self.bias`: `np.ndarray`\n",
    "      - `shape`: `(..., 1, output_dim)`\n",
    "\n",
    "  - Operation(s):\n",
    "      - Update `self.weight` with the obtained weight.\n",
    "      - Update `self.bias` with the obtained bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeReadout(Linear):\n",
    "    def __init__(self, *args, lmbd: float = 0.0, **kwargs):\n",
    "        super(RidgeReadout, self).__init__(*args, **kwargs)\n",
    "        self.lmbd = lmbd\n",
    "\n",
    "    def train(self, x: np.ndarray, y: np.ndarray):\n",
    "        assert (x.ndim > 1) and (x.shape[-1] == self.input_dim)\n",
    "        assert (y.ndim > 1) and (y.shape[-1] == self.output_dim)\n",
    "        x_biased = np.ones((*x.shape[:-1], x.shape[-1] + 1), dtype=self.dtype)\n",
    "        x_biased[..., 1:] = x\n",
    "        # TODO Implement ridge regression to obtain `self.weight` and `self.bias`.\n",
    "        xtx = ...\n",
    "        xty = ...\n",
    "        sol = ...\n",
    "        self.weight = ...\n",
    "        self.bias = ...\n",
    "        # end of TODO\n",
    "        return self.weight, self.bias\n",
    "\n",
    "\n",
    "def solution(dim_in, dim_out, x_train, y_train, x_eval, lmbd):\n",
    "    # DO NOT CHANGE HERE.\n",
    "    readout = RidgeReadout(dim_in, dim_out, lmbd=lmbd)\n",
    "    readout.train(x_train, y_train)\n",
    "    return readout(x_eval)\n",
    "\n",
    "\n",
    "test_func(solution, \"05_01\")\n",
    "# show_solution(\"05_01\", \"RidgeReadout\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell includes the previous code that plots NRMSE for various values of $T_\\mathrm{train}$, but here, `BatchLRReadout` is replaced with `RidgeReadout`.\n",
    "Note that substituting $\\lambda = 0$ makes the ridge regression equivalent to linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 100\n",
    "seed_setup, seed_dataset = 1234, 5678\n",
    "t_trains = [50, 100, 200, 400, 800, 1600, 3200]\n",
    "ridge_parameters = [0.0, 1.0, 1e-2, 1e-4, 1e-6, 1e-8]\n",
    "sigma, phi = 0.05, 0.05  # vs \\in [0.0, 0.1]\n",
    "dim, rho = 50, 0.9\n",
    "dataset_info = dict(t_washout=100, t_train=max(t_trains), t_eval=1000)\n",
    "\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "w_in_ws, net_ws = [], []\n",
    "for seed in trange(sample_num):  # Create setups for `sample_num` times.\n",
    "    w_in, net, _w_out = create_setup(seed_setup + seed, dim, rho, f=np.tanh)\n",
    "    w_in_ws.append(w_in.weight)\n",
    "    net_ws.append(net.weight)\n",
    "\n",
    "w_in_batch = Linear(1, dim)\n",
    "w_in_batch.weight = np.array(w_in_ws)[:, None, :, :]  # [bs, 1, N, 1] (-> [bs, 1, N])\n",
    "net_batch = ESN(dim, sr=rho)\n",
    "net_batch.weight = np.array(net_ws)  # [bs, N, N] ([bs, 1, N] -> [bs, 1, N])\n",
    "w_out = RidgeReadout(dim, 1)  # Use ridge regression.\n",
    "\n",
    "vs = sigma * us + phi\n",
    "x0 = np.zeros((sample_num, 1, net.dim))\n",
    "xs = sample_dynamics(x0, w_in_batch, net_batch, ts, vs, display=True)\n",
    "nrmse_dict = {lmbd: {} for lmbd in ridge_parameters}\n",
    "for lmbd, t_train in tqdm(list(itertools.product(ridge_parameters, t_trains))):\n",
    "    time_info[\"t_washout\"] = dataset_info[\"t_washout\"] + max(t_trains) - t_train\n",
    "    w_out.lmbd = lmbd\n",
    "    nrmse = eval_nrmse(xs, ys, w_out, time_info=time_info)[:, 0, 0]\n",
    "    nrmse_dict[lmbd][t_train] = nrmse\n",
    "    # print('={:.2e}, t_train={}: {:.3e}{:.3e} (#sample={})'.format(\n",
    "    #     lmbd, t_train, nrmse.mean(), nrmse.std(), nrmse.size))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "for lmbd in ridge_parameters:\n",
    "    nrmse_aves = [nrmse.mean() for nrmse in nrmse_dict[lmbd].values()]\n",
    "    nrmse_stds = [nrmse.std() for nrmse in nrmse_dict[lmbd].values()]\n",
    "    if lmbd == 0.0:\n",
    "        plot_kws = dict(label=r\"LR ($\\lambda=0$)\", color=\"k\", ls=\":\")\n",
    "    else:\n",
    "        plot_kws = dict(label=r\"Ridge ($\\lambda=10^{{{:.0f}}}$)\".format(np.log10(lmbd)))\n",
    "    ax.errorbar(t_trains, nrmse_aves, nrmse_stds, **plot_kws)\n",
    "ax.set_xlabel(r\"$T_\\mathrm{train}$\")\n",
    "ax.set_ylabel(\"NRMSE\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend(loc=\"upper right\", borderaxespad=0, ncol=1, frameon=False)\n",
    "ax.grid(True, which=\"both\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression becomes unstable around $T_\\mathrm{train} \\approx N$, but the accuracy of ridge regression ($\\lambda=10^{-4}, 10^{-6}$) remains stable.\n",
    "Furthermore, even when $T_\\mathrm{train} \\gg N$, ridge regression achieves almost the same accuracy and stability as linear regression.\n",
    "On the other hand, when $\\lambda$ is large ($\\lambda=10^{0}, 10^{-2}$), the accuracy remains stable but is significantly worse than linear regression.\n",
    "This phenomenon is called **underfitting**, the opposite of overfitting.\n",
    "Additionally, choosing values of $\\lambda$ that are too small ($\\lambda=10^{-8}$) causes instability to occur around $T_\\mathrm{train} \\approx N$, i.e., overfitting, similar to linear regression.\n",
    "\n",
    "Thus, the appropriate selection of the ridge parameter $\\lambda$ is very important for learning accuracy and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic adjustment of ridge parameter $\\lambda$ based on AIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ridge regression, introduction of the ridge parameter allows for more stable learning even in cases where $T_\\mathrm{train}$ is small.\n",
    "However, this ridge parameter $\\lambda$ is a hyperparameter and requires separate tuning.\n",
    "Specifically, since $\\lambda$ can take any positive real number, its search range is infinite, making its tuning quite troublesome.\n",
    "As a solution, we introduce an AIC to automatically determine $\\lambda$.\n",
    "AIC is a measure that balances model size and residual error, and is widely used in the field of machine learning to construct reasonable models.\n",
    "In the case of ridge regression, AIC can be calculated using the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{AIC}(X, Y, \\lambda) :&= T \\ln\\left(\\mathrm{RSS}(X, Y, \\hat{w}^\\mathrm{ridge}(X, Y, \\lambda))\\right) + \\mathrm{df}(X, \\lambda) \\\\\n",
    "&= T\\ln\\left(\\|X\\hat{w}^\\mathrm{ridge}(X, Y, \\lambda) - Y\\|^2\\right) + \\mathrm{df}(X, \\lambda)\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathrm{df}(X, \\lambda)$ is the effective degrees of freedom of the model, given by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{df}(X, \\lambda) :&= \\mathrm{tr}[X(X^\\top X + \\lambda I)^{-1}X^\\top] \\\\\n",
    "&= \\sum_{i=0}^{N-1}\\frac{\\sigma_{i}^2}{\\sigma_{i}^2+\\lambda}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\{\\sigma_i \\}_{i=0}^{N-1}$ are the singular values when singular value decomposition is applied to $X \\in \\mathbb{R}^{T\\times N}$.\n",
    "By minimizing AIC, an appropriate $\\lambda$ can be obtained.\n",
    "Also, $0<\\mathrm{df} \\leq N$ can be inferred from the equation.\n",
    "\n",
    "Therefore, instead of directly searching for $\\lambda$, we can prepare a set of candidate degrees of freedom $\\{\\mathrm{df}_k \\}$, calculate $\\lambda_k$ such that $\\mathrm{df}_k = \\mathrm{df}(X, \\lambda_k)$ for each candidate of $\\mathrm{df}$, and select $\\lambda_k$ that minimizes $\\mathrm{AIC}(X, Y, \\lambda_k)$.\n",
    "This approach allows us to search within a limited range of $\\mathrm{df}$ candidates, instead of within the whole infinite range of $\\lambda$.\n",
    "\n",
    "Below, we write out the detailed algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a list of candidates for $\\{\\mathrm{df}_k\\}$. Typically, only integer values $\\{1,~\\ldots,~N\\}$ are selected as candidates.\n",
    "2. Use Newton's method to solve for $\\lambda_k$ such that $f(\\lambda_k)=0$.\n",
    "3. Compute  $\\mathrm{AIC}(X, Y, \\lambda_k)$ using $\\lambda_k$.\n",
    "4. Find the $k$ and $\\lambda_k$ that minimizes $\\mathrm{AIC}(X, Y, \\lambda_k)$.\n",
    "\n",
    "Here, the function $f(\\lambda_k)$ is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(\\lambda_k) = \\mathrm{df}_k - \\sum_{i=0}^{N-1}\\frac{\\sigma_{i}^2}{\\sigma_{i}^2+\\lambda_k}\n",
    ".\\end{align*}\n",
    "$$\n",
    "\n",
    "Also, the derivative $\\dfrac{df}{d\\lambda_k}$ used in Newton's method is given by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dfrac{df}{d\\lambda_k}(\\lambda_k) = \\sum_{i=0}^{N-1}\\frac{\\sigma_{i}^2}{\\left(\\sigma_{i}^2+\\lambda_k\\right)^2}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5.2.\n",
    "\n",
    "Fill in the blanks of the following code to complete `calc_df_and_lambda`, `calc_aic`, and `AutoRidgeReadout`, implementing the algorithm described above.\n",
    "\n",
    "- `AutoRidgeReadout.train`\n",
    "  - Argument(s):\n",
    "    - `x`: `np.ndarray`\n",
    "      - `shape`: `(..., time_steps, input_dim)`\n",
    "    - `y`: `np.ndarray`\n",
    "      - `shape`: `(..., time_steps, output_dim)`\n",
    "  - Return(s):\n",
    "    - `self.weight`: `np.ndarray`\n",
    "      - `shape`: `(..., output_dim, input_dim)`\n",
    "    - `self.bias`: `np.ndarray`\n",
    "      - `shape`: `(..., 1, output_dim)`\n",
    "    - `*misc`\n",
    "\n",
    "  - Operation(s):\n",
    "      - Update `self.weight` with the obtained weight.\n",
    "      - Update `self.bias` with the obtained bias.\n",
    "      - Update `self.lmbd` with the obtained $\\lambda$ minimizing $\\mathrm{AIC}$.\n",
    "\n",
    "<details><summary>tips</summary>\n",
    "\n",
    "- [`scipy.optimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.newton.html)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_df_and_lambda(xs: np.array, df_max: int = None, num_cand: int = None, lmbd0=1e-12):\n",
    "    batch_size, dim = xs.shape[:-2], xs.shape[-1]\n",
    "    if df_max is None:\n",
    "        df_max = dim\n",
    "    if num_cand is None:\n",
    "        num_cand = df_max\n",
    "\n",
    "    _left, sigma, _right = np.linalg.svd(xs)\n",
    "    sigma2 = (sigma**2)[..., None, :]  # [*bs, 1, dim]\n",
    "    dfs = np.linspace(0, df_max, num_cand + 1)[1:]  # Candidates for degrees of freedom -> [num_cand]\n",
    "    init_cond = np.full((*batch_size, num_cand), lmbd0)  # Initial condition for  -> [*bs, num_cand]\n",
    "\n",
    "    def func(lmbd):\n",
    "        # TODO `lmbd`: [*bs, num_cand], `sigma2`: [*bs, 1, dim]\n",
    "        ...\n",
    "\n",
    "    def fprime(lmbd):\n",
    "        # TODO `lmbd`: [*bs, num_cand], `sigma2`: [*bs, 1, dim]\n",
    "        ...\n",
    "\n",
    "    # TODO Solve f() = 0 for using Newton's method. The output `lmbds` should be of shape [*bs, num_cand].\n",
    "    lmbds = ...\n",
    "    # end of TODO\n",
    "    lmbds[lmbds < 0] = 0  # Remove negative  due to numerical errors.\n",
    "    return dfs, lmbds\n",
    "\n",
    "\n",
    "def calc_aic(xs, ys, **kwargs):\n",
    "    assert xs.shape[-2] == ys.shape[-2]\n",
    "    *batch_size, length, dim_in = xs.shape\n",
    "    dfs, lmbds = calc_df_and_lambda(xs, **kwargs)  # dfs: [num_cand], lmbds: [*bs, num_cand]\n",
    "    xs = xs[..., None, :, :]  # [*bs, 1, length, dim_in]\n",
    "    ys = ys[..., None, :, :]  # [*bs, 1, length, dim_in]\n",
    "    xtx = xs.swapaxes(-2, -1) @ xs  # X^T X: [*bs, 1, dim_in, dim_in]\n",
    "    xty = xs.swapaxes(-2, -1) @ ys  # X^T Y: [*bs, 1, dim_in, dim_out]\n",
    "    # TODO Ridge regression with  -> [*bs, num_cand, dim_in, dim_out]\n",
    "    sol = ...\n",
    "    # end of TODO\n",
    "    rss = ...  # TODO RSS: [*bs, num_cand]\n",
    "    aics = ...  # TODO AIC: [*bs, num_cand]\n",
    "    return dfs, lmbds, sol, rss, aics\n",
    "\n",
    "\n",
    "class AutoRidgeReadout(Linear):\n",
    "    def __init__(self, *args, lmbd: float = 0.0, **kwargs):\n",
    "        super(AutoRidgeReadout, self).__init__(*args, **kwargs)\n",
    "        self.lmbd = lmbd\n",
    "\n",
    "    def train(self, x: np.ndarray, y: np.ndarray, **kwargs):\n",
    "        assert (x.ndim > 1) and (x.shape[-1] == self.input_dim)\n",
    "        assert (y.ndim > 1) and (y.shape[-1] == self.output_dim)\n",
    "        x_biased = np.ones((*x.shape[:-1], x.shape[-1] + 1), dtype=self.dtype)\n",
    "        x_biased[..., 1:] = x\n",
    "        dfs, lmbds, sol, rss, aics = calc_aic(x_biased, y, **kwargs)\n",
    "        # dfs: [num_cand], lmbds: [*bs, num_cand]\n",
    "        # sol: [*bs, num_cand, dim_in, dim_out]\n",
    "        # rss: [*bs, num_cand], aics: [*bs, num_cand]\n",
    "        best_idx = ...  # TODO Extract the index of the best solution minimizing AIC.\n",
    "        sol_best = ...  # TODO Choose the best solution minimizing AIC.\n",
    "        self.lmbd = ...  # TODO Save the best  minimizing AIC.\n",
    "        self.weight = ...  # TODO Update weight based on `sol_best`.\n",
    "        self.bias = ...  # TODO Update bias based on `sol_best`.\n",
    "        return self.weight, self.bias, dfs, lmbds, sol, rss, aics\n",
    "\n",
    "\n",
    "def solution(dim_in, dim_out, x_train, y_train, x_eval):\n",
    "    # DO NOT CHANGE HERE.\n",
    "    readout = AutoRidgeReadout(dim_in, dim_out)\n",
    "    readout.train(x_train, y_train)\n",
    "    return readout(x_eval)\n",
    "\n",
    "\n",
    "test_func(solution, \"05_02\")\n",
    "# show_solution(\"05_02\", \"calc_df_and_lambda\")  # Uncomment it to see the solution.\n",
    "# show_solution(\"05_02\", \"calc_aic\")  # Uncomment it to see the solution.\n",
    "# show_solution(\"05_02\", \"AutoRidgeReadout\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the effectiveness of `AutoRidgeReadout` with the case of $(\\sigma,\\phi,N,\\rho,T_\\mathrm{train})=(0.05,0.05,500,0.9,2000)$.\n",
    "To reduce computation time, we set `num_cand = dim // 10` and $\\{\\mathrm{df}_k\\}=\\{10, 20,~\\ldots,~500\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_setup, seed_dataset = 1234, 5678  # Please choose your favorite seeds.\n",
    "sigma, phi = 0.05, 0.05  # vs \\in [0.0, 0.1]\n",
    "dim, rho = 500, 0.9\n",
    "dataset_info = dict(t_washout=100, t_train=2000, t_eval=1000)\n",
    "\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "vs = sigma * us + phi\n",
    "\n",
    "w_in, net, w_out = create_setup(seed_setup, dim, rho, f=np.tanh, cls=AutoRidgeReadout)\n",
    "\n",
    "x0 = np.zeros(net.dim)\n",
    "xs = sample_dynamics(x0, w_in, net, ts, vs, display=True)\n",
    "nrmse, _weight, _bias, dfs, lmbds, sol, rss, aics = eval_nrmse(\n",
    "    xs, ys, w_out, time_info, return_out=True, df_max=dim, num_cand=dim // 10\n",
    ")\n",
    "\n",
    "best_idx = np.argmin(aics)\n",
    "rel_aics = aics - aics.min() + 1e0  # Normalize AICs for better visualization.\n",
    "y_out = w_out(xs)\n",
    "\n",
    "plot_length = 200\n",
    "fig = Figure(figsize=(8, 8))\n",
    "fig.create_grid(2, 1, height_ratios=(2, 1), hspace=0.4)\n",
    "ax0, ax1 = fig[0], fig[1]\n",
    "ax0.create_grid(2, 1, wspace=0.4)\n",
    "ax0[0].plot(dfs, lmbds)\n",
    "ax0[0].scatter(dfs[best_idx], lmbds[best_idx], s=100.0, marker=\"*\")\n",
    "ax0[0].set_yscale(\"log\")\n",
    "ax0[0].set_ylabel(r\"$\\lambda$\")\n",
    "ax0[0].set_xticklabels([])\n",
    "ax0[1].plot(dfs, rel_aics)\n",
    "ax0[1].scatter(dfs[best_idx], rel_aics[best_idx], s=100.0, marker=\"*\")\n",
    "ax0[1].set_ylabel(r\"relative $\\mathrm{AIC}$\")\n",
    "ax0[1].set_yscale(\"log\")\n",
    "info_str = r\"best: $\\mathrm{{df}}={:.0f},~\\lambda=10^{{{:.2f}}}$\".format(dfs[best_idx], np.log10(lmbds[best_idx]))\n",
    "ax0.set_title(info_str)\n",
    "ax0[1].set_xlabel(\"df\")\n",
    "\n",
    "ax1.set_xlabel(r\"$\\mathrm{{df}}$\")\n",
    "ax1.plot(ts[-plot_length:], ys[-plot_length:], color=\"k\", ls=\":\", lw=1.5)\n",
    "ax1.plot(ts[-plot_length:], y_out[-plot_length:], lw=1.5, color=\"red\")\n",
    "ax1.set_title(\"NRMSE={:.3e}\".format(nrmse[0]))\n",
    "ax1.set_ylabel(r\"$y[k]$ & $\\hat{y}[k]$\")\n",
    "ax1.set_xlabel(\"time steps\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try comparing the results of `AutoRidgeReadout` with linear regression ( $\\lambda=0$ ) and the underfitting case ( $\\lambda=10^{-2}$ )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 20\n",
    "seed_setup, seed_dataset = 1234, 5678\n",
    "t_trains = [100, 200, 400, 800, 1600, 3200]\n",
    "sigma, phi = 0.05, 0.05  # vs \\in [0.0, 0.1]\n",
    "dim, rho = 50, 0.9\n",
    "dataset_info = dict(t_washout=100, t_train=max(t_trains), t_eval=1000)\n",
    "ridge_parameters = [0.0, 1e-2, None]  # None: `AutoRidgeReadout`\n",
    "\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "w_in_ws, net_ws = [], []\n",
    "for seed in trange(sample_num):  # Create setups for `sample_num` times.\n",
    "    w_in, net, _w_out = create_setup(seed_setup + seed, dim, rho, f=np.tanh)\n",
    "    w_in_ws.append(w_in.weight)\n",
    "    net_ws.append(net.weight)\n",
    "\n",
    "w_in_batch = Linear(1, dim)\n",
    "w_in_batch.weight = np.array(w_in_ws)[:, None, :, :]  # [bs, 1, N, 1] (-> [bs, 1, N])\n",
    "net_batch = ESN(dim, sr=rho)\n",
    "net_batch.weight = np.array(net_ws)  # [bs, N, N] ([bs, 1, N] -> [bs, 1, N])\n",
    "x0 = np.zeros((sample_num, 1, net.dim))\n",
    "vs = sigma * us + phi\n",
    "xs = sample_dynamics(x0, w_in_batch, net_batch, ts, vs, display=True)\n",
    "\n",
    "nrmse_dict = {lmbd: {} for lmbd in ridge_parameters}\n",
    "lbmd_dict = {lmbd: {} for lmbd in ridge_parameters}\n",
    "for lmbd, t_train in tqdm(list(itertools.product(ridge_parameters, t_trains))):\n",
    "    time_info[\"t_washout\"] = dataset_info[\"t_washout\"] + max(t_trains) - t_train\n",
    "    if lmbd is None:\n",
    "        w_out = AutoRidgeReadout(dim, 1)\n",
    "        train_kws = dict(df_max=dim)\n",
    "    else:\n",
    "        w_out = RidgeReadout(dim, 1, lmbd=np.array(lmbd))\n",
    "        train_kws = {}\n",
    "    nrmse = eval_nrmse(xs, ys, w_out, time_info=time_info, **train_kws)\n",
    "    nrmse_dict[lmbd][t_train] = np.array(nrmse)\n",
    "    lbmd_dict[lmbd][t_train] = np.array(w_out.lmbd)\n",
    "    # print('t_train={}: : {:.3e}{:.3e}, NRMSE: {:.3e}{:.3e}'.format(\n",
    "    #     t_train, w_out.lmbd.mean(), w_out.lmbd.std(), nrmse.mean(), nrmse.std()))\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 8), gridspec_kw=dict(height_ratios=(1, 2), hspace=0.05))\n",
    "for lmbd in ridge_parameters:\n",
    "    nrmse_aves = [nrmse.mean() for nrmse in nrmse_dict[lmbd].values()]\n",
    "    nrmse_stds = [nrmse.std() for nrmse in nrmse_dict[lmbd].values()]\n",
    "    lmbd_aves = [lmbd.mean() for lmbd in lbmd_dict[lmbd].values()]\n",
    "    lmbd_stds = [lmbd.std() for lmbd in lbmd_dict[lmbd].values()]\n",
    "    if lmbd is None:\n",
    "        plot_kws = dict(label=r\"AutoRidge\", color=\"red\", ls=\"--\", lw=2.0)\n",
    "        ax[0].errorbar(t_trains, lmbd_aves, lmbd_stds, **plot_kws)\n",
    "    elif lmbd == 0.0:\n",
    "        plot_kws = dict(label=r\"LR ($\\lambda=0$)\", color=\"k\", ls=\":\")\n",
    "    else:\n",
    "        plot_kws = dict(label=r\"Ridge ($\\lambda=10^{{{:.0f}}}$)\".format(np.log10(lmbd)))\n",
    "    ax[1].errorbar(t_trains, nrmse_aves, nrmse_stds, **plot_kws)\n",
    "ax[0].set_xscale(\"log\")\n",
    "ax[0].set_xticklabels([])\n",
    "ax[0].set_ylabel(r\"$\\lambda$\")\n",
    "ax[0].grid(True, which=\"both\")\n",
    "ax[1].set_xlabel(r\"$T_\\mathrm{train}$\")\n",
    "ax[1].set_ylabel(\"NRMSE\")\n",
    "ax[1].set_xscale(\"log\")\n",
    "ax[1].set_yscale(\"log\")\n",
    "ax[1].legend(loc=\"upper right\", borderaxespad=0, ncol=1, frameon=False)\n",
    "ax[1].grid(True, which=\"both\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5.3. (Advanced)\n",
    "\n",
    "- Test the robustness of `AutoRidgeReadout`, by changing values of $\\sigma$ or $\\rho$.\n",
    "- Validate the performance for various values of $\\lambda$ using `RidgeReadout`, and compare it with the performance of `AutoRidgeReadout`.\n",
    "- `AutoRidgeReadout` does not necessarily choose $\\lambda$ that minimizes the cost (NRMSE). Explain why.\n",
    "- There exist other information criteria, such as the Bayesian Information Criterion (BIC). Implement models that minimize these other critieria, and compare their performance with that of AIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn the method of **grid search**, which explores multiple parameters simultaneously to find the best combination.\n",
    "In grid search, **all** combinations of hyperparameter candidate values are exhaustively searched.\n",
    "For example, let's suppose we want to find the best combination of three parameters $(\\sigma,\\phi,\\rho)$: input scale, bias, and spectral radius.\n",
    "The following parameter values are set as candidates:\n",
    "\n",
    "- $\\sigma$ : $(0.05,0.10,0.15,~\\ldots~,1.00)$\n",
    "- $\\phi$ : $(0.00,0.05,0.10,~\\ldots~,1.00)$\n",
    "- $\\rho$ : $(0.6,0.7,0.8,0.9)$\n",
    "\n",
    "In this case, there are $20\\times 21\\times 4=840$ total combinations, and all combinations are validated exhaustively in grid search.\n",
    "Here, the previously introduced batch processing is very effective.\n",
    "In the exercise below, let's learn how to specify the input $u[k]$ and the initial values of the ESN to conduct grid search efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6.1.\n",
    "\n",
    "The following arguments are given: input sequence $U=\\{u[k]\\} \\in \\mathbb{R}^{T\\times 1}$, an array of length $k$ containing input scaling $\\Sigma = (\\sigma_0$, $\\sigma_1,~\\ldots,~\\sigma_{k-1})$, an array of length $l$ containing biases $\\Phi = (\\phi_0$, $\\phi_1,~\\ldots,~\\phi_{l-1})$, an array of length $m$ containing spectral radii $\\Rho = (\\rho_0$, $\\rho_1,~\\ldots,~\\rho_{m-1})$.\n",
    "Fill in the blanks in the following code to complete `create_grid_search_setup`, a function that generates $v[k]$ and reshapes $\\Rho$ to collectively sample all $k\\times l\\times m$ conditions as $x[k] \\in \\mathbb{R}^{k\\times l\\times m\\times N}$, for the same weight intializations of $W^\\mathrm{in}, W^\\mathrm{rec}$ in each experiment.\n",
    "\n",
    "- `create_grid_search_setup`\n",
    "  - Argument(s):\n",
    "    - `us`: `np.ndarray`\n",
    "      - `shape`: `(t, 1)`\n",
    "    - `sigma`: `np.ndarray`\n",
    "      - `shape`: `(k,)`\n",
    "    - `phi`: `np.ndarray`\n",
    "      - `shape`: `(l,)`\n",
    "    - `rho`: `np.ndarray`\n",
    "      - `shape`: `(m,)`\n",
    "  - Return(s):\n",
    "    - `vs`: `np.ndarray`\n",
    "      - `shape`: `(k, l, 1, t, 1)`\n",
    "    - `rho_new`: `np.ndarray`\n",
    "      - `shape`: `(m, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_search_setup(us, sigma, phi, rho):\n",
    "    vs = ...  # TODO\n",
    "    rho_new = ...  # TODO\n",
    "    return vs, rho_new\n",
    "\n",
    "\n",
    "test_func(create_grid_search_setup, \"06_01\", multiple_output=True)\n",
    "# show_solution(\"06_01\", \"create_grid_search_setup\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code evaluates the NRMSE for all $840$ parameter combinations.\n",
    "Depending on your environment, it may take around 2 to 3 minutes to complete the plot.\n",
    "If it takes too long, consider reducing `dim` and `t_train`, or increasing the spacing between parameter values in `sigmas`, `phis`, and `rhos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_setup, seed_dataset = 1234, 5678\n",
    "dim = 50\n",
    "sigmas = np.linspace(0.0, 1.0, 21)[1:]  # [0.05, 0.10, ...., 1.00]\n",
    "phis = np.linspace(0.0, 1.0, 21)  # [0.0, 0.05,, ...., 1.00]\n",
    "rhos = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "dataset_info = dict(t_washout=100, t_train=2000, t_eval=1000)\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "vs_batch, rhos_batch = create_grid_search_setup(us, sigmas, phis, rhos)  # Use `create_grid_search_setup`.\n",
    "\n",
    "w_in, net, w_out = create_setup(seed_setup, dim, rhos_batch, f=np.tanh)\n",
    "x0 = np.zeros((sigmas.shape[0], phis.shape[0], rhos.shape[0], net.dim))\n",
    "xs = sample_dynamics(x0, w_in, net, ts, vs_batch, display=True)\n",
    "nrmse = eval_nrmse(xs, ys, w_out, time_info)[..., 0]  # [k, l, m]\n",
    "\n",
    "top_num = 5\n",
    "best_ids = np.array(np.unravel_index(np.argsort(nrmse, axis=None)[:top_num], nrmse.shape)).T\n",
    "for idx, pos in enumerate(best_ids):\n",
    "    print(\n",
    "        \"#{}: NRMSE={:.2e}, ={:.2e}, ={:.2e}, ={:.2e}\".format(\n",
    "            idx + 1, nrmse[(*pos,)], sigmas[pos[0]], phis[pos[1]], rhos[pos[2]]\n",
    "        )\n",
    "    )\n",
    "\n",
    "grid_num = (len(rhos) ** 0.5).__ceil__()\n",
    "fig = Figure(figsize=(6 * grid_num, 5 * grid_num))\n",
    "fig.create_grid(grid_num, grid_num, hspace=0.5, wspace=0.5)\n",
    "\n",
    "vmax = min(1.0, np.max(nrmse))\n",
    "vmin = max(0.1, np.min(nrmse))\n",
    "for idx, rho in enumerate(rhos):\n",
    "    fig[idx].plot_matrix(\n",
    "        nrmse[..., idx],\n",
    "        index=sigmas,\n",
    "        column=phis,\n",
    "        vmax=vmax,\n",
    "        vmin=vmin,\n",
    "        cmap=\"viridis\",\n",
    "        zscale=\"log\",\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    fig[idx].set_xlabel(r\"$\\phi$\")\n",
    "    fig[idx].set_ylabel(r\"$\\sigma$\")\n",
    "    fig[idx].set_title(r\"$\\rho={:.2f}$\".format(rho))\n",
    "\n",
    "px, py, idx = best_ids[0]\n",
    "fig[idx].scatter(py, px, s=200.0, marker=\"*\", color=\"magenta\")\n",
    "fig.suptitle(\n",
    "    r\"best NRMSE={:.2e}, $(\\sigma,\\phi,\\rho)=$({:g},{:g},{:g})\".format(\n",
    "        nrmse[(*best_ids[0],)], sigmas[px], phis[py], rhos[idx]\n",
    "    )\n",
    ")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search requires computation proportional to the number of parameter combinations.\n",
    "Therefore, as the number of hyperparameters increases, the calculations may become impractical or result in memory shortages.\n",
    "A common strategy is to first set the parameters coarsely and then search more finely.\n",
    "\n",
    "There are various methods, but the following algorithm is one simple example:\n",
    "1. Decide the number of divisions $p$ for the parameters.\n",
    "2. Specify the minimum value $\\theta_\\mathrm{min}$ and maximum value $\\theta_\\mathrm{max}$ of the parameter $\\theta$ to be optimized.\n",
    "3. Prepare $p$ parameter values to be searched, $\\theta_{k} = \\theta_\\mathrm{min} + (k + 0.5)(\\theta_\\mathrm{max}-\\theta_\\mathrm{min})/p~(0 \\leq k < p)$.\n",
    "4. Calculate the objective function $\\mathcal{L}(\\theta_k)$ for each $\\theta_k$.\n",
    "5. Obtain $\\hat{k}:=\\mathrm{argmin}_k \\mathcal{L}(\\theta_k)$ that minimizes $\\mathcal{L}(\\theta_k)$.\n",
    "6. Update $\\theta_\\mathrm{min}\\leftarrow \\theta_\\mathrm{min}+\\hat{k}(\\theta_\\mathrm{max}-\\theta_\\mathrm{min})/p$.\n",
    "7. Update $\\theta_\\mathrm{max}\\leftarrow \\theta_\\mathrm{min}+(\\hat{k}+1)(\\theta_\\mathrm{max}-\\theta_\\mathrm{min})/p$.\n",
    "8. Return to step 3.\n",
    "\n",
    "This algorithm works only when the change of $\\mathcal{L}$ is sufficiently smooth relative to the parameter division intervals.\n",
    "However, it is simple to implement and useful for a rough search of good parameter combinations.\n",
    "In the exercise below, let's implement the key parts of this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6.2.\n",
    "\n",
    "Real numbers $\\theta_\\mathrm{min}$ and $\\theta_\\mathrm{max}$, and a positive integer $p$ representing the number of parameter divisions are given.\n",
    "Fill in the blanks in the following code to complete `create_parameter_set`, a function that generates an array $\\{\\theta_{k}\\}$ of length $p$ containing candidate parameter values.\n",
    "\n",
    "- `create_parameter_set`\n",
    "  - Argument(s):\n",
    "    - `th_min`: `float`\n",
    "    - `th_max`: `float`\n",
    "    - `num_split` : `int`\n",
    "  - Return(s):\n",
    "    - `th_new`: `np.ndarray`\n",
    "      - `shape`: `(num_split,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameter_set(th_min, th_max, num_split):\n",
    "    th_new = ...  # TODO\n",
    "    return th_new\n",
    "\n",
    "\n",
    "test_func(create_parameter_set, \"06_02\")\n",
    "# show_solution(\"06_02\", \"create_parameter_set\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code searches the parameter ranges with 5 divisions (`num_split`) over 10 iterations (`num_iteration`).\n",
    "\n",
    "- $\\sigma\\in (0.0, 1.0)$\n",
    "- $\\phi\\in (0.0, 1.0)$\n",
    "- $\\rho\\in (0.0, 2.0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_split = 5\n",
    "num_iteration = 10\n",
    "seed_setup, seed_dataset = 1234, 5678\n",
    "dim = 50\n",
    "parameter_range = dict(\n",
    "    sigma=(0.0, 1.0),\n",
    "    phi=(0.0, 1.0),\n",
    "    rho=(0.0, 2.0),\n",
    ")\n",
    "dataset_info = dict(t_washout=100, t_train=2000, t_eval=1000)\n",
    "ts, us, ys, time_info = sample_dataset(seed_dataset, **dataset_info)\n",
    "\n",
    "for idx in range(num_iteration):\n",
    "    # print(parameter_range)\n",
    "    sigmas, phis, rhos = map(lambda v: create_parameter_set(*v, num_split), parameter_range.values())\n",
    "    # print(sigmas, phis, rhos)\n",
    "    vs_batch, rhos_batch = create_grid_search_setup(us, sigmas, phis, rhos)\n",
    "    w_in, net, w_out = create_setup(seed_setup, dim, rhos_batch, f=np.tanh)\n",
    "    x0 = np.zeros((sigmas.shape[0], phis.shape[0], rhos.shape[0], net.dim))\n",
    "    xs = sample_dynamics(x0, w_in, net, ts, vs_batch, display=False)\n",
    "    nrmse = eval_nrmse(xs, ys, w_out, time_info)[..., 0]  # [k, l, m]\n",
    "    best_ids = np.unravel_index(np.argmin(nrmse, axis=None), nrmse.shape)\n",
    "    print(\n",
    "        \"#{:>2}: NRMSE={:.6e}, ={:g}, ={:g}, ={:g}\".format(\n",
    "            idx + 1, nrmse[(*best_ids,)], sigmas[best_ids[0]], phis[best_ids[1]], rhos[best_ids[2]]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    parameter_set_new = dict()\n",
    "    for pos, (key, param_range) in zip(best_ids, parameter_range.items(), strict=False):\n",
    "        vmin, vmax = param_range\n",
    "        vmin_new = vmin + (vmax - vmin) * pos / num_split\n",
    "        vmax_new = vmin + (vmax - vmin) * (pos + 1) / num_split\n",
    "        parameter_set_new[key] = (vmin_new, vmax_new)\n",
    "    parameter_range = parameter_set_new\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6.3. (Advanced)\n",
    "\n",
    "- Increasing $N$ or $p$ may cause memory shortage in some environments.\n",
    "Modify the code to avoid running out of memory.\n",
    "- Investigate the strengths and weaknesses of this algorithm by varying $N$ and $p$.\n",
    "- Explore more robust methods for hyperparameter search and incorporate them into the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6.4. (Advanced)\n",
    "\n",
    "- Apply the parameter search methods learned here to physical reservoir research.\n",
    "- For example, use the current setup to validate the open dataset from pneumatic artificial muscle (PAM)<sup>[3, 4]</sup> research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Kubota, T., Takahashi, H., & Nakajima, K. (2021). *Unifying framework for information processing in stochastically driven dynamical systems*. Physical Review Research, 3(4), 043135. https://doi.org/10.1103/PhysRevResearch.3.043135\n",
    "\n",
    "[2] Goto, K., Nakajima, K., & Notsu, H. (2021). *Twin vortex computer in fluid flow*. New Journal of Physics, 23(6), 063051. https://doi.org/10.1088/1367-2630/ac024d\n",
    "\n",
    "[3] Sakurai, R., Nishida, M., Sakurai, H., Wakao, Y., Akashi, N., Kuniyoshi, Y., Minami, Y., & Nakajima, K. (2020). *Emulating a sensor using soft material dynamics: A reservoir computing approach to pneumatic artificial muscle*. 2020 3rd IEEE International Conference on Soft Robotics (RoboSoft), 710717. https://doi.org/10.1109/RoboSoft48309.2020.9115974\n",
    "\n",
    "[4] Akashi, N., Yamaguchi, T., Tsunegi, S., Taniguchi, T., Nishida, M., Sakurai, R., Wakao, Y., & Nakajima, K. (2020). *Input-driven bifurcations and information processing capacity in spintronics reservoirs*. Physical Review Research, 2(4), 043303. https://doi.org/10.1103/PhysRevResearch.2.043303"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rc-bootcamp (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
