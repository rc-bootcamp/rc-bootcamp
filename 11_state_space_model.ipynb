{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb2995b",
   "metadata": {},
   "source": [
    "# Chapter 11. 状態空間モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b478c20",
   "metadata": {},
   "source": [
    "この章では、2020年代に長期記憶を扱うニューラルネットワークとして脚光を浴びた状態空間モデルを扱います。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cbd95",
   "metadata": {},
   "source": [
    "## 前書き"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94a032",
   "metadata": {},
   "source": [
    "### 長期記憶の重要性と状態空間モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016acca8",
   "metadata": {},
   "source": [
    "これまでESNを題材に、RNNの中で記憶がどのように解析されるかを説明してきました。\n",
    "特に記憶容量 (MC) 、記憶関数 (MF)、情報処理容量 (IPC) といったRCの指標を用いると、力学系において入力時系列の情報がその状態の上でどの程度保持されるかが定量化され、かつその上限や内訳も「解剖」して議論されます。\n",
    "MCはもともとH. Jaegerら<sup>[1]</sup>によって提案された指標ですが、その論文のタイトル\"Short-term memory in echo state networks\"が示すように「短期」記憶の評価を目的として導入されました。\n",
    "すなわち力学系の次元 $N$ に対して、同等から数倍程度のステップ数の過去入力情報がどの程度保持されるかを評価するのに一般的には用いられます。\n",
    "これまで学んだように入力時系列がi.i.d.な分布から生成されるとき、MCの上限は高々 $N$ になるので、多くのケースではそれで十分に有用な情報が得られます。\n",
    "一方でMCは普遍的な指標なので、システムのサイズより遥かに大きい時間スケールの「長期」記憶の評価に対しても適用できます。\n",
    "例えば有意な値が得られるかは別にして、数100ノードのESNに対して数万〜数十万ステップ前の情報に対してMFを計算することも可能です。\n",
    "ただし通常のESNとその初期化方法では、これまで見てきたとおりほとんどのケースでMFがノイズと区別がつかないレベルで急速に減衰するため、そういった長期記憶の実現は一般に困難です。\n",
    "特に線形な記憶特性しか持たない線形ESNの場合は単調にMFは減少する (非線形の場合にはその限りでない点に注意) ので、長期記憶を実現するには工夫が必要になります。\n",
    "\n",
    "RCの文脈とは別に、長期記憶の重要性はGPTに代表される大規模言語モデル (LLM) の文脈で近年高まっています。\n",
    "GPT-2<sup>[2]</sup>の登場以降、長文の処理や生成にTransformer<sup>[3]</sup>が組み込まれたニューラルネットワークが盛んに用いられるようになりました。\n",
    "Transformerでは処理したい文章はトークン毎に分割され、各トークンがモデルの入力としてはじめから順番に入力されます。\n",
    "その際に、現実の小説や評論、あるいは人間との対話がそうであるように、文章全体を包括的に理解するためにはしばしば数千〜数万トークンにわたる長期の文脈を保持しかつ適切に処理しなければなりません。\n",
    "しかしながらAttention機構が組み込まれたTransformerでは、入力トークン列の長さが $L$ のとき時間計算量が $O(L^2)$ となるため、タスクの長期依存性が高くなるほど計算が困難になります。\n",
    "またニューラルネットワークの隠れ状態の次元 $N$ に対して、パラメータの保持と行列ベクトル積にそれぞれ $O(N^2)$ のメモリと計算時間を必要とするため、できるだけ少ないパラメータ数での実現が好ましいと言えます。\n",
    "\n",
    "こうした背景から登場したのが、2020年に公開されたベンチマークタスク群であるLong-Range Arena (LRA)<sup>[4]</sup>です。\n",
    "LRAでは長期記憶が必要な様々なタスクが用意されており、これを機に長期記憶タスクに適したニューラルネットワークの研究・開発が盛んになりました。\n",
    "この章で扱われる **状態空間モデル (State Space Models; SSM)<sup>[5,6,7,8]</sup>** と呼ばれるアーキテクチャはこのLRAを通じて脚光を浴びました。\n",
    "SSMを組み込んだ **S4 (Structured State Space Sequence model)<sup>[6]</sup>** は、LRAにおいて当時のSoTAを大きく上回る性能を示したのです。\n",
    "特にLRAの一つであるPathfinderタスクの中で最も難しいPathfinder-X (Path-X)において、Transformerをはじめとする既存のAttentionベースのモデルがほぼ解けなかった (ランダム推測と同程度の精度しか出せない) 中で、S4は唯一高い精度を示しました。\n",
    "またSSMはその後、選択メカニズムと合わせてMamba<sup>[9]</sup>においても組み込まれ、Transformerを改良し代替しうる高速かつ効率的な生成モデルとして、2025年現在継続的に注目されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b958c2",
   "metadata": {},
   "source": [
    "### リザバーとしてのSSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a53b0",
   "metadata": {},
   "source": [
    "さてこのSSMですが、内部結合の設定が特殊ではあるものの、実はその中身は線形RNNに分類されます。\n",
    "またESNのように内部結合パラメータを固定化しても、その長期記憶特性を損なわずに学習・推論できます。\n",
    "つまりSSMは長期記憶を有するリザバーとみなせるのです。\n",
    "これがまさしく本RC bootcampでSSMを取り上げる理由です。\n",
    "実際、S4が提案された論文では、複数のSSMと活性化関数を交互に配置した多層構造が用意され、そのモデルパラメータが学習されました。\n",
    "この際SSM、すなわち線形RNN部分のパラメータを初期化後に固定化して学習させても、誤差逆伝搬法<sup>[10]</sup>で内部結合を含め全体を学習させる条件と比較して、あまり変わらない精度が得られたと報告されています<sup>[6]</sup>。\n",
    "この条件のモデルは線形ESNが組み込まれた、いわゆる深層リザバー計算<sup>[11]</sup>のアーキテクチャと実質的に等価であるといえ、そこでは長期記憶の保持をリザバーが担っているといえます。\n",
    "また同様の結果はその後の研究<sup>[12]</sup>でも実験的に確認され、かつMFと絡めてそのメカニズムが議論されました。\n",
    "これらの一連の結果はSSM部分のパラメータの初期化方法自体に長期記憶を実現するための秘訣が含まれている可能性を示唆しています。\n",
    "\n",
    "このようにSSMは長期記憶を扱う特殊な線形リザバーとみなせ、これまで学習してきたRCに対する知識を活かしてより深くそのメカニズムを理解できると考えられます。\n",
    "この章では、SSMの理論的背景にあるHiPPOフレームワーク<sup>[5]</sup>と、SSMが組み込まれたアーキテクチャであるS4・S4D<sup>[6,7]</sup>を題材に、RCの観点から長期記憶を扱うニューラルネットワークの実装方法とそのメカニズムの理解を目指しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be37cd09",
   "metadata": {},
   "source": [
    "### SSMの定式化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c272c944",
   "metadata": {},
   "source": [
    "まずはSSMの定式化を行います。\n",
    "SSMは $t$ 上で定義される1次元の入力時系列 $u(t)$ から $N$ 次元の内部状態 $x(t)~(N\\in\\mathbb{Z^{+}})$ を介して、1次元の出力時系列 $y(t)$ を生成するseq2seqモデル (時系列から時系列へ変換するモデル) の総称です。\n",
    "この際SSMを表現するために、以下の連続時間 $t\\in[-\\infty, \\infty]$ 上で定義される以下の形式がよく用いられます。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dx}{dt}(t) &= A(t)x(t) + B(t)u(t) \\\\\n",
    "y(t) &= C(t)x(t) + D(t)u(t)\n",
    ",\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a136c7",
   "metadata": {},
   "source": [
    "ここで、$x(t)\\in\\mathbb{R}^{N}$は状態、$A(t) \\in \\mathbb{R}^{N \\times N}$ は状態行列、 $B(t) \\in \\mathbb{R}^{N \\times 1}$ は入力行列、 $C(t) \\in \\mathbb{R}^{1 \\times N}$ は出力行列、そして $D(t) \\in \\mathbb{R}^{1 \\times 1}$ は直達行列と呼ばれるSSMの挙動を決定するパラメータです。\n",
    "式の形に見覚えがある方がいるかもしれませんが、制御工学の文脈で古くから用いられてきた形式です。\n",
    "今SSMに長期記憶特性を持たせるためには、 $x(t), y(t)$ が $u(t)$の 過去の情報を長い期間反映したものである必要があります。\n",
    "特に式(2)は実質的に動的な線形リードアウト層であるので、 $x(t)$ の挙動を決定する式(1)とそのパラメータ$A(t), B(t)$が特に重要であると言えます。\n",
    "そこでパラメータ $A(t), B(t)$ を有するSSMをSSM $(A(t), B(t))$ と表記します。\n",
    "\n",
    "また一般性のためSSMのパラメータを時変な形式で書きましたが、特にパラメータを時不変にした $A(t)=A, B(t)=B, C(t)=C, D(t)=D$ が用いられます。\n",
    "この場合システムは線形時不変 (LTI; Linear Time-Invariant)で、この形式で表されるSSMをTSMM (Time-invariant State Space Model) と呼称しSSM $(A,B)$ と表記します。\n",
    "この際 $y(t)$ はカーネル関数 $K(t)$ (インパルス応答、すなわち $u(t)=\\delta (t)$ に対する応答と同じ)を 用いて以下の式で明示的に計算できます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "K(t) &= C e^{tA} B \\\\\n",
    "y(t) &= (K * u)(t)  \\\\\n",
    "&=\\int_{0}^{t} K(t-s) u(s) ds\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf737b65",
   "metadata": {},
   "source": [
    "以下特に表記がなければ $D=0$ とします。\n",
    "後述しますが、この式は畳み込み計算のアルゴリズムによって、コンピュータ上で効率的に計算できます。\n",
    "しばしばシステムがTSSMに限定される理由はこの計算のしやすさにあります。\n",
    "また $e^{tA}B$ は $N$ 次元のベクトルであり、その各成分の値 $K_n(t):= (e^{tA}B)_n$ はSSMの基底関数と呼ばれます。\n",
    "これは $C=e_{n}^\\top(:=[0,~\\ldots,~0, 1, 0,~\\ldots,~0])$ (ただし1は $n$ 番目の成分) としたときのカーネル関数に対応します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe58f36e",
   "metadata": {},
   "source": [
    "### HiPPOフレームワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd577c7",
   "metadata": {},
   "source": [
    "次にSSMから一旦離れて、**HiPPO (High-order Polynomial Projection Operator) フレームワーク**の概念を説明します。\n",
    "HiPPOフレームワークは時々刻々とシステムに入力される時系列データを時系列そのものを保持せずに、システムの有限次元の状態 $x(t) \\in \\mathbb{R}^{N}$ の上で圧縮して表現するために導入された枠組みです。\n",
    "換言すれば時刻 $t$ において、 $t$ 以前の過去時系列 $u_{\\leq t}:=u(s)|_{s \\leq t}$ の $N$次元ベクトル $x(t)$ として表現を目指します。\n",
    "ここでまず重要な点として、HiPPOフレームワークではこの圧縮表現の問題を、関数空間における基底関数に対する係数の更新問題に置き換えて定式化します。\n",
    "すなわち $\\{p^{(t)}_{n}(s)\\}_{n=0}^{N-1}$ を時刻 $t$ における任意の基底関数 (基底が時変の場合も含めて一般化しています) として、 以下の形での $u_{\\leq t}$ の近似を目指します。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "u_{\\leq t}(s) &\\approx p^{(t)}(s) \\\\\n",
    "&:=\\sum_{n=0}^{N-1} x_{n}(t) p^{(t)}_{n}(s) \\quad (s \\in [-\\infty,t])\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49d8083",
   "metadata": {},
   "source": [
    "別の言葉で表現すれば、$\\{p^{(t)}_{n}(s)\\}_{n=0}^{N-1}$ を基底関数とした関数空間の$N$ 次元の部分空間に、$u_{\\leq t}$ を 射影した関数が $p^{(t)}(s)$ であり、その係数が $x(t)=(x_0(t),~\\ldots,~x_{N-1}(t))^{\\top}$ であるといえます。\n",
    "\n",
    "さて上記の近似を評価する上で関数空間上における距離が必要になります。\n",
    "この際時刻 $t$ 以前の時間 $[-\\infty,t]$ 上で定義されるある測度 $\\mu^{(t)}(s)$ (ただし$\\int_{-\\infty}^{t} d\\mu^{(t)}(s) = 1$) を用いて以下の形で内積が定義されます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\langle {u,p^{(t)}} \\rangle_{\\mu^{(t)}} = \\int_{-\\infty}^{t} u(s) p^{(t)}(s) d\\mu^{(t)}(s)\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b62c62",
   "metadata": {},
   "source": [
    "後ほど再度登場し説明されますが、測度 $\\mu^{(t)}$ はいわば連続時間における重み付けであり、 $[-\\infty, t]$ のうちどの部分の入力時系列を重要視して近似するかを決定します。\n",
    "同様に $L_{2}$ ノルムは以下の式で定義されます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\|u\\|_{L_{2}(\\mu^{(t)})} = \\sqrt{\\langle {u,u} \\rangle_{\\mu^{(t)}}}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d229313c",
   "metadata": {},
   "source": [
    "最終的にHiPPOフレームワークの問題は以下の距離を最小化する $\\{x_n{(t)}\\}_{n=0}^{N-1}$ の最適化問題として定式化されます。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p^{(t)} = \\arg\\min \\|u_{\\leq t} - p^{(t)}\\|_{L_{2}(\\mu^{(t)})} \\tag{3}\n",
    ".\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62374fbd",
   "metadata": {},
   "source": [
    "HiPPOフレームワークでは、$p^{(t)}$ を明示的に計算する代わりに、式(3)を最小化する係数 $\\{x_{n}(t)\\}_{n=0}^{N-1}$ の計算を各時刻 $t$ において目指します。\n",
    "言い換えれば、以下の式で定義される写像 $\\mathrm{hippo}: (\\mathbb{R}_{\\leq t} \\to \\mathbb{R}) \\to (\\mathbb{R}_{\\leq t} \\to \\mathbb{R}^{N})$ の構築がHiPPOフレームワーク全体の目的となります。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{hippo}(u_{\\leq t}) := \\{x_{n}(t)\\}_{n=0}^{N-1} \\quad \\text{s.t. minimizing} \\quad \\left\\|u_{\\leq t} - \\sum_{n=0}^{N-1} x_{n}(t) p^{(t)}_{n}\\right\\|_{L_{2}(\\mu^{(t)})}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96497afc",
   "metadata": {},
   "source": [
    "このようにHiPPOフレームワークは、時刻 $t$ 以前の入力時系列を有限の $N$ 次元ベクトルで表現するために、所定の基底関数に対する係数を各時刻 $t$ において計算します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70daafcb",
   "metadata": {},
   "source": [
    "### HiPPOとSSMの関係"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ab12ac",
   "metadata": {},
   "source": [
    "ここまで抽象的な説明が続きましたが、そのような写像 $\\mathrm{hippo}$ は具体的にどのように構築され計算されるでしょうか？\n",
    "実は、適切に測度 $\\mu^{(t)}$ と基底関数 $\\{p_{i}^{(t)}\\}_{i=0}^{N-1}$ の組み合わせを選ぶと、$\\mathrm{hippo}$ の計算は式(1)の形式で表現されるあるSSMの解の求積に対応します。\n",
    "言い換えれば、式 (1) において適切に $A(t), B(t)$ を設計してSSM $(A(t), B(t))$ を構築すると、その状態 $x(t)$ の時間発展 (求積) によって、ある測度と基底関数における $\\mathrm{hippo}(u_{\\leq t})$ の計算が完了してしまうのです。\n",
    "これがHiPPOフレームワークの重要な成果です<sup>[5]</sup>。\n",
    "またある測度 $\\mu^{(t)}$ において基底関数 $\\{p^{(t)}_{n}\\}_{n=0}^{N-1}$ が直交基底であるとき、すなわち\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_n(t) &= \\langle {u_{\\leq t}, p^{(t)}_n} \\rangle_{\\mu^{(t)}} \\\\\n",
    "\\langle {p^{(t)}_{i}, p^{(t)}_{j}} \\rangle_{\\mu^{(t)}} &= \\delta_{ij} \\quad (i,j=0,1,~\\ldots,~N-1)\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d449ec2",
   "metadata": {},
   "source": [
    "を満たすとき、そのようなSSM $(A(t), B(t))$ はOSSM (Orthogonal State Space Model) と呼ばれます。\n",
    "また特にTSSMかつOSSMである場合、そのSSM $(A, B)$ はTOSSM (Time-invariant Orthogonal State Space Model) と呼称されます。\n",
    "このとき $p^{(t)}(s)=: p(t-s)$ 、 $\\mu^{(t)}(s)=:\\mu(t-s)$ と $t-s$ のみに依存する形式で表現され、かつSSMの基底関数 ($p^{(t)}_{n}$ とは異なる)に関して $K_n(t-s) = p^{(t)}_{n}(s) \\mu^{(t)}(s)$ を満たすので非常に扱いやすくなります。\n",
    "実際にHiPPOフレームワークが提案された論文<sup>[5]</sup>ではTOSSMとなるSSM $(A, B)$ を構成する具体例が紹介されています。\n",
    "次の節から論文で紹介されたいくつかのTOSSMの実装を通じて、HiPPOフレームワークを理解していきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd72c5",
   "metadata": {},
   "source": [
    "## 演習問題と実演"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d5951",
   "metadata": {},
   "source": [
    "ここからは実際にコードの実装を行いながら、SSMとHiPPOフレームワーク、ならびにそれらが組み込まれたアーキテクチャの理解を目指します。\n",
    "前回と同様、各種ライブラリおよび実装済みの関数の`import`を行うために次のセルを実行してください。\n",
    "なお内部実装を再確認するには、`import inspect`以下の行をコメントアウトするか`...?? / ??...`を使用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3455b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "matplotlib.rc(\"animation\", html=\"jshtml\")\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import drive  # type: ignore Csonnecting to google drive\n",
    "\n",
    "    if False:  # Set to True if you want to use Google Drive and save your work there.\n",
    "        drive.mount(\"/content/gdrive\")\n",
    "        %cd /content/gdrive/My Drive/rc-bootcamp/\n",
    "        # NOTE: Change it to your own path if you put the zip file elsewhere.\n",
    "        # e.g., %cd /content/gdrive/My Drive/[PATH_TO_EXTRACT]/rc-bootcamp/\n",
    "    else:\n",
    "        pass\n",
    "        %cd /content/\n",
    "        !git clone --branch ja_sol https://github.com/rc-bootcamp/rc-bootcamp.git\n",
    "        %cd /content/rc-bootcamp/\n",
    "else:\n",
    "    sys.path.append(\".\")\n",
    "\n",
    "from utils.reservoir import ESN, Linear\n",
    "from utils.style_config import Figure, plt\n",
    "from utils.tester import load_from_chapter_name\n",
    "from utils.tqdm import tqdm, trange\n",
    "\n",
    "test_func, show_solution = load_from_chapter_name(\"11_state_space_model\")\n",
    "\n",
    "# Uncomment it to see the implementations.\n",
    "# import inspect\n",
    "# print(inspect.getsource(Linear))\n",
    "# print(inspect.getsource(ESN))\n",
    "\n",
    "# Or just use ??.../...?? (uncomment the following lines).\n",
    "# Linear??\n",
    "# RidgeReadout??\n",
    "# ESN??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15e1472",
   "metadata": {},
   "source": [
    "### TOSSMの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5969a9",
   "metadata": {},
   "source": [
    "#### HiPPO-LegT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594fb38",
   "metadata": {},
   "source": [
    "まずHiPPO-LegT (truncated Legendre polynomials) と呼ばれるTOSSMを実装しましょう。\n",
    "HiPPO-LegTは以下の式で表される SSM $(A, B)$ です。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A_{nk} &= \\frac{1}{\\theta} \\sqrt{(2n+1)(2k+1)} \\begin{cases}\n",
    "-1 &\\quad \\text{if} ~n \\geq k \\\\\n",
    "(-1)^{n-k+1} &\\quad \\text{if} ~n \\leq k \\\\\n",
    "\\end{cases} \\tag{HiPPO-LegT A} \\\\\n",
    "B_n &= \\frac{1}{\\theta} \\sqrt{2n+1} \\tag{HiPPO-LegT B}\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0986180",
   "metadata": {},
   "source": [
    "ただし $0 \\leq n, k \\leq N-1$ です。\n",
    "詳細な証明は省きますが、このSSM $(A, B)$に対応するHiPPOの測度 $\\mu^{(t)}(s)$ と直交基底関数 $\\{p^{(t)}_n(s)\\}_{n=0}^{N-1}$  は以下の式で与えられます (導出は論文<sup>[5,8]</sup>を確認してください)。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu^{(t)}(s) &= \\frac{1}{\\theta} \\mathbb{1}_{[t-\\theta, t]}(s) \\quad (\\theta > 0) \\\\\n",
    "p^{(t)}_n(s) &= \\sqrt{2n+1} L_n\\left(1 - \\frac{2(t-s)}{\\theta}\\right) \\cdot \\mathbb{1}_{[t-\\theta, t]}(s)\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a014e",
   "metadata": {},
   "source": [
    "ここで $\\mathbb{1}_{[t-\\theta, t]} (s)$ は指示関数であり、$s \\in [t-\\theta, t]$ のときのみ 1、それ以外 0を出力する関数になります。\n",
    "この場合、測度 $\\mu^{(t)}(s)$ が過去 $\\theta$ 分 **のみ** の入力時系列情報を加味する重み付けを行うと言えます。\n",
    "また $L_n$ は $n$ 次のルジャンドル多項式、$\\theta$ は時間幅を表します。\n",
    "このようにHiPPO-LegTでは $[-1, 1]$から $[t-\\theta, t]$ 上で直交するように変換された $n$次のルジャンドル多項式 $p^{(t)}_n(s)$ を基底関数として用い、 過去 $\\theta$ 分の入力時系列を $[t-\\theta, t]$ の範囲で近似します (※論文とは異なり、$\\mathbb{1}_{[t-\\theta, t]}$ が基底関数に含まれていますが、これは範囲外での $L_n$ の発散を防ぐためで、実践的には問題ありません)。\n",
    "\n",
    "まずはじめにこの $A, B$ を実装しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f207d",
   "metadata": {},
   "source": [
    "Q1.1.\n",
    "\n",
    "上記の式で定義される HiPPO-LegT の $A, B$ を構築する関数`transition_leg_t`実装せよ。\n",
    "ただし 行列のサイズ $N > 0$ および時間幅 $\\theta > 0$ は引数として与えられるものとする。\n",
    "\n",
    "- `transition_leg_t`\n",
    "  - Argument(s):\n",
    "    - `N`: `int`\n",
    "      - $N > 0$\n",
    "    - `theta`: `float`\n",
    "      - $\\theta > 0$\n",
    "  - Return(s):\n",
    "    - `A`: `np.ndarray`\n",
    "      - `shape`: `(N, N)`\n",
    "      - `dtype`: `np.float64`\n",
    "    - `B`: `np.ndarray`\n",
    "      - `shape`: `(N, 1)`\n",
    "      - `dtype`: `np.float64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d8c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_leg_t(N, theta=1.0):\n",
    "    # TODO\n",
    "    arange = np.arange(N)\n",
    "    base = (2 * arange + 1) ** 0.5\n",
    "    diff = arange[:, None] - arange[None, :]\n",
    "    A = base[:, None] * base[None, :]\n",
    "    A[(diff >= 0) | (diff % 2 == 0)] *= -1\n",
    "    B = np.array(base)[:, None]\n",
    "    A /= float(theta)\n",
    "    B /= float(theta)\n",
    "    return A, B\n",
    "    # end of TODO\n",
    "\n",
    "\n",
    "test_func(transition_leg_t, \"01_01\", multiple_output=True)\n",
    "# show_solution(\"01_01\", \"transition_leg_t\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127390b",
   "metadata": {},
   "source": [
    "次にHiPPO-LegTに対応する測度 $\\mu^{(t)}(s)=\\frac{1}{\\theta} \\mathbb{1}_{[t-\\theta, t]}(s)$ と基底関数 $p^{(t)}_n(s) = \\sqrt{2n+1} L_n\\left(1 - \\frac{2(t-s)}{\\theta}\\right) \\cdot \\mathbb{1}_{[t-\\theta, t]}(s)$ を実装してみましょう。\n",
    "特に $t$ の変化に対して形状が変わらない (スライドで不変) ので $t=0$ として $s\\in (-\\infty, 0]$ 上でそれぞれを実装しましょう。\n",
    "まずは測度 $\\mu^{(0)}(s)$ を実装します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b877e3d",
   "metadata": {},
   "source": [
    "Q1.2.\n",
    "\n",
    "長さ $T$ の時間列 $S=\\{s_0, s_1,~\\ldots,~s_{T-1}\\}$ ならびにパラメータ $\\theta>0$ に対して、$t=0$ の時のHiPPO-LegTの測度 $\\mu^{(0)}(s)$ を計算する関数`measure_leg_t`を実装せよ。\n",
    "\n",
    "- `measure_leg_t`\n",
    "  - Argument(s):\n",
    "    - `S`: `np.ndarray`\n",
    "      - `shape`: `(T,)`\n",
    "      - `dtype`: `np.float64`\n",
    "    - `theta`: `float`\n",
    "      - $\\theta > 0$\n",
    "  - Return(s):\n",
    "    - `measure`: `np.ndarray`\n",
    "      - `shape`: `(T,)`\n",
    "      - `dtype`: `np.float64`\n",
    "\n",
    "- [`numpy.where`](https://numpy.org/doc/stable/reference/generated/numpy.where.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6baa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_leg_t(S, theta=1.0):\n",
    "    # TODO\n",
    "    measure = np.where((-theta <= S) & (S <= 0), 1.0 / theta, 0.0)\n",
    "    return measure\n",
    "    # end of TODO\n",
    "\n",
    "\n",
    "test_func(measure_leg_t, \"01_02\")\n",
    "# show_solution(\"01_02\", \"measure_leg_t\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56157416",
   "metadata": {},
   "source": [
    "次は基底関数 $p^{(0)}_n(s)$ を実装しましょう。\n",
    "IPCを学んだ章で実装したメモ化によって効率的に計算する `Legendre` クラスをそのまま利用できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca28911",
   "metadata": {},
   "source": [
    "Q1.3.\n",
    "\n",
    "$x(t)$ の次元 $N > 0$ 、長さ $T$の 時間列 $S = \\{s_0, s_1,~\\ldots,~s_{T-1}\\}$ ならびにパラメータ $\\theta > 0$ が与えられたとき、$t=0$ の時のHiPPO-LegTの基底関数 $p^{(0)}_n(s)$ を計算する関数 `basis_leg_t` を実装せよ。\n",
    "\n",
    "- `basis_leg_t`\n",
    "  - Argument(s):\n",
    "    - `N`: `int`\n",
    "      - $N > 0$\n",
    "    - `theta`: `float`\n",
    "      - $\\theta > 0$\n",
    "    - `S`: `np.ndarray`\n",
    "      - `shape`: `(T,)`\n",
    "      - `dtype`: `np.float64`\n",
    "  - Return(s):\n",
    "    - `basis`: `np.ndarray`\n",
    "      - `shape`: `(N, T)`\n",
    "      - `dtype`: `np.float64`\n",
    "\n",
    "<details><summary>tips</summary>\n",
    "\n",
    "- `Legendre??` によって実装を確認せよ。あるいは IPC の章を参照せよ。\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac2edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipc_module.polynomial import Legendre\n",
    "\n",
    "\n",
    "def basis_leg_t(N, S, theta=1.0):\n",
    "    # TODO\n",
    "    basis = np.zeros((N, len(S)))\n",
    "    non_zero = (-theta <= S) & (S <= 0)\n",
    "    poly = Legendre(1 + 2 * S[non_zero] / theta)\n",
    "    for idx in range(N):\n",
    "        basis[idx, non_zero] = poly[idx]  # Evaluate the n-th order polynomial.\n",
    "    basis[:, non_zero] *= ((2 * np.arange(N) + 1) ** 0.5)[:, None]\n",
    "    return basis\n",
    "    # end of TODO\n",
    "\n",
    "\n",
    "test_func(basis_leg_t, \"01_03\")\n",
    "# show_solution(\"01_03\", \"basis_leg_t\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c7aab0",
   "metadata": {},
   "source": [
    "HiPPO-LegTでは $[t-\\theta, t]$ 上での値が加味されるので、$t=0$ として $s \\in [-2\\theta, 0]$ 上での $p^{(0)}_n(s)$ の挙動を描画してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_basis(ss, basis, products, plot_num):\n",
    "    fig = Figure(figsize=(14, 4))\n",
    "    fig.create_grid(1, 2, width_ratios=(2, 1))\n",
    "    fig[0].plot(ss, basis[:plot_num].T, lw=1.0)\n",
    "    fig[0].tick_params(axis=\"both\", labelsize=12)\n",
    "    fig[0].set_xlabel(\"Time $s$\", fontsize=14)\n",
    "    fig[0].set_ylabel(\"Basis $p^{(0)}_n(s)$\", fontsize=14)\n",
    "    fig[1].tick_params(axis=\"both\", labelsize=12)\n",
    "    _im, cb = fig[1].plot_matrix(\n",
    "        products,\n",
    "        cmap=\"Blues\",\n",
    "        vmin=0,\n",
    "        vmax=None,\n",
    "        aspect=\"equal\",\n",
    "        colorbar=True,\n",
    "    )\n",
    "    fig[1].set_xticks(range(plot_num))\n",
    "    fig[1].set_xticklabels(range(plot_num))\n",
    "    fig[1].set_yticks(range(plot_num))\n",
    "    fig[1].set_yticklabels(range(plot_num))\n",
    "    cb.ax.tick_params(labelsize=12)\n",
    "    return fig\n",
    "\n",
    "\n",
    "num_basis, theta, dt = 16, 2.0, 1e-4\n",
    "plot_num = 8\n",
    "\n",
    "ss = np.linspace(-2 * theta, 0, int(2 * theta / dt) + 1)\n",
    "measure = measure_leg_t(ss, theta=theta)\n",
    "basis = basis_leg_t(num_basis, ss, theta=theta)\n",
    "products = (basis[:plot_num] * measure) @ basis[:plot_num].T * dt  # Inner products among bases p for μ\n",
    "fig = plot_basis(ss, basis, products, plot_num)\n",
    "fig[0].set_title(\"HiPPO-LegT Basis Functions ($\\\\theta={:.2f}$)\".format(theta), fontsize=14)\n",
    "fig[1].set_title(\"Inner Products\", fontsize=14)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd10047",
   "metadata": {},
   "source": [
    "ご覧のとおり、$[-\\theta, 0]$ 上ではルジャンドル多項式が変換された波形が見られる一方で、$[-2\\theta, -\\theta)$ 上では0になっていることがわかります。\n",
    "また右の図より、これらの基底の直交性 ($\\langle {p^{(0)}_i, p^{(0)}_j} \\rangle_{\\mu^{(0)}} = \\delta_{ij}$ ) も確認できます。\n",
    "HiPPO-LegTはTOSSMなので、各時刻 $t$ における測度 $\\mu^{(t)}(s)$ と基底関数 $p^{(t)}_{n}(s)$ はこのグラフを右に $t$ だけシフトさせたものになります。\n",
    "したがってこの基底関数の値はどの時刻でも再利用できます。\n",
    "\n",
    "さてデモに移る前に、コンピュータ上で常微分方程式の求積を行うために式(1)の常微分方程式の離散化を行わなければなりません。\n",
    "これまでオイラー法やルンゲ・クッタ法などを学習してきましたが、TSSMに関しては制御工学の文脈で様々な求積法が開発されてきたので、ここではそれらの方法を実装しましょう。\n",
    "サンプル時間幅を $\\Delta t > 0$ と設定し、$t=k\\Delta t~(k=0,1,2,~\\ldots)$ における状態を $x[k] := x(k\\Delta t)$ と置くと、TSSM $(A,B)$ の離散化は以下の式で与えられます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x[k+1] &= \\bar{A} x[k] + \\bar{B} u[k]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c135c77",
   "metadata": {},
   "source": [
    "このシステムは離散線形ESNと同じ形式なのでここでは便宜的に LESN $(\\bar{A}, \\bar{B})$ と書きます。\n",
    "$\\bar{A}, \\bar{B}$ を求める離散化の方法はいくつか知られますが、S4・S4Dの論文で取り上げられたのは以下の２つの方法です。\n",
    "\n",
    "- 零次ホールド (Zero-Order Hold; ZOH)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bar{A} &= e^{\\Delta t A} \\\\\n",
    "\\bar{B} &= A^{-1}(e^{\\Delta t A} - I)B\n",
    ".\\end{align*}\n",
    "$$\n",
    "\n",
    "- 双線形変換 (bilinear)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bar{A} &= \\left(I - \\frac{\\Delta t}{2} A\\right)^{-1}\\left(I + \\frac{\\Delta t}{2} A\\right) \\\\\n",
    "\\bar{B} &= \\left(I - \\frac{\\Delta t}{2} A\\right)^{-1} \\Delta t B\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cb0d35",
   "metadata": {},
   "source": [
    "次の演習問題でそれぞれを実装していきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bf15d0",
   "metadata": {},
   "source": [
    "Q1.3.\n",
    "\n",
    "行列 $A\\in\\mathbb{R}^{n \\times n}$ と $B\\in\\mathbb{R}^{n \\times 1}$、およびサンプル時間幅 $\\Delta t > 0$ が与えられたとき、上記のZOHおよび双線形変換による離散化を行う関数`discretize_zoh`と `discretize_bilinear`を実装せよ。\n",
    "\n",
    "- `discretize_zoh`, `discretize_bilinear`\n",
    "  - Argument(s):\n",
    "    - `dt`: `float`\n",
    "      - $\\Delta t > 0$\n",
    "    - `A`: `np.ndarray`\n",
    "      - `shape`: `(n, n)`\n",
    "      - `dtype`: `np.float64`\n",
    "    - `B`: `np.ndarray`\n",
    "      - `shape`: `(n, 1)`\n",
    "      - `dtype`: `np.float64`\n",
    "  - Return(s):\n",
    "    - `A_bar`: `np.ndarray`\n",
    "      - `shape`: `(n, n)`\n",
    "      - `dtype`: `np.float64`\n",
    "    - `B_bar`: `np.ndarray`\n",
    "      - `shape`: `(n, 1)`\n",
    "      - `dtype`: `np.float64`\n",
    "\n",
    "<details><summary>tips</summary>\n",
    "\n",
    "- [`scipy.linalg.expm`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.expm.html) は行列の指数関数を計算する。\n",
    "- 逆行列の計算の代わりに [`scipy.linalg.lstsq`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html) を使用すると数値的に安定する。\n",
    "- [`scipy.signal.cont2discrete`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.cont2discrete.html) では、`zoh`と`bilinear`両方とも実装されており、かつ他の例も提供されているが今回は直接実装せよ。\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd7c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_zoh(dt, A, B):\n",
    "    # TODO\n",
    "    A_bar = scipy.linalg.expm(A * dt)\n",
    "    B_bar = scipy.linalg.lstsq(A * dt, (A_bar - np.eye(A.shape[0])) @ (B * dt))[0]\n",
    "    return A_bar, B_bar\n",
    "    # end of TODO\n",
    "\n",
    "\n",
    "def discretize_bilinear(dt, A, B):\n",
    "    # TODO\n",
    "    eye = np.eye(A.shape[0])\n",
    "    A_bar = np.linalg.lstsq(eye - A * dt / 2, eye + A * dt / 2)[0]\n",
    "    B_bar = np.linalg.lstsq(eye - A * dt / 2, B * dt)[0]\n",
    "    return A_bar, B_bar\n",
    "    # end of TODO\n",
    "\n",
    "\n",
    "def discretize(dt, A, B, method=\"zoh\"):\n",
    "    # DO NOT CHANGE HERE.\n",
    "    if method == \"zoh\":\n",
    "        return discretize_zoh(dt, A, B)\n",
    "    elif method == \"bilinear\":\n",
    "        return discretize_bilinear(dt, A, B)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "\n",
    "def solution(dt, A, B):\n",
    "    # DO NOT CHANGE HERE.\n",
    "    A_bar_zoh, B_bar_zoh = discretize(dt, A, B, method=\"zoh\")\n",
    "    A_bar_bil, B_bar_bil = discretize(dt, A, B, method=\"bilinear\")\n",
    "    return A_bar_zoh, B_bar_zoh, A_bar_bil, B_bar_bil\n",
    "\n",
    "\n",
    "test_func(solution, \"01_04\", multiple_output=True)\n",
    "# show_solution(\"01_04\", \"discretize_zoh\")  # Uncomment it to see the solution.\n",
    "# show_solution(\"01_04\", \"discretize_bilinear\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03e61fd",
   "metadata": {},
   "source": [
    "以上で準備ができたので具体的に時系列を構成して試してみましょう。\n",
    "まずは長さ`period` 時間幅 `dt` の単純なランダムな時系列を作成しましょう。\n",
    "以下の関数`white_signal`は論文<sup>[5,8]</sup>で使用された時系列生成関数を改変したものであり、`cutoff_freq`で指定された周波数以下の成分のみを持つ、実効値 `rms` (パワーの平方根) のホワイトノイズを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b7b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_signal(period, dt, cutoff_freq, rms=0.5, seed=None, start_from_zero=True, batch_shape=()):\n",
    "    \"\"\"\n",
    "    Ref: https://github.com/state-spaces/s4/blob/main/notebooks/hippo_function_approximation.ipynb\n",
    "    \"\"\"\n",
    "    rnd = np.random.default_rng(seed)\n",
    "    if cutoff_freq is not None and cutoff_freq < 1.0 / period:\n",
    "        raise ValueError(\n",
    "            f\"Make ``{cutoff_freq=} >= 1. / {period=}`` to produce a non-zero signal\",\n",
    "        )\n",
    "    nyquist_cutoff = 0.5 / dt\n",
    "    if cutoff_freq > nyquist_cutoff:\n",
    "        raise ValueError(\n",
    "            f\"{cutoff_freq} must not exceed the Nyquist frequency for the given dt ({{{{nyquist_cutoff:0.3f}}}})\"\n",
    "        )\n",
    "    n_coefs = int(np.ceil(period / dt / 2.0))\n",
    "    shape = (*batch_shape, n_coefs + 1)\n",
    "    sigma = rms * np.sqrt(n_coefs**2)\n",
    "    rnd_real, rnd_imag = rnd.spawn(2)\n",
    "    coefs = 1j * rnd_imag.normal(0.0, sigma, size=shape)\n",
    "    coefs += rnd_real.normal(0.0, sigma, size=shape)\n",
    "    set_to_zero = np.fft.rfftfreq(2 * n_coefs, d=dt) > cutoff_freq\n",
    "    set_to_zero[..., -1] = True  # Nyquist frequency\n",
    "    set_to_zero[..., 0] = True  # Remove DC component.\n",
    "    coefs *= ~set_to_zero\n",
    "    power_correction = np.sqrt(np.sum(~set_to_zero))\n",
    "    if power_correction > 0.0:\n",
    "        coefs /= power_correction\n",
    "    sig = np.fft.irfft(coefs, axis=-1)\n",
    "    if start_from_zero:\n",
    "        sig = sig - sig[..., :1]  # Start from 0.\n",
    "    ts = np.arange(sig.shape[-1]) * dt\n",
    "    return ts, sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5684a5",
   "metadata": {},
   "source": [
    "以下のセルにおいて、`white_signale`関数を用いて入力時系列 $u(t)$ を生成し、実装した SSM $(A, B)$ に $u(t)$ を与えて $x(t)$ を計算しましょう。\n",
    "長期記憶特性を検証するために $u(t)$ にはより長周期のsin波を加えています。\n",
    "またSSM $(A, B)$ を離散すると LESN $(\\bar{A}, \\bar{B})$ になるので、これまでの章で度々登場した`ESN`と`Linear`クラスを活用して実装しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1002b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_ssm(transition, basis_num, dt, disc_method, dim_in=1, **transition_kwargs):\n",
    "    A, B = transition(basis_num, **transition_kwargs)\n",
    "    A_bar, B_bar = discretize(dt, A, B, method=disc_method)\n",
    "    esn = ESN(basis_num, sr=1.0, f=None, weight=A_bar)  # Linear RNN (f=None)\n",
    "    w_in = Linear(dim_in, basis_num, weight=B_bar)  # Input weight\n",
    "    return esn, w_in\n",
    "\n",
    "\n",
    "def sample_ssm_dynamics(us, esn, w_in, x0=None):\n",
    "    if x0 is None:\n",
    "        x = np.zeros((esn.dim,))  # Initial state\n",
    "    else:\n",
    "        x = x0\n",
    "    xs = np.zeros((len(us), *x.shape))\n",
    "    for idx in trange(len(us)):\n",
    "        u = us[idx : idx + 1]  # (1,)\n",
    "        x = esn(x, w_in(u))  # (dim,)\n",
    "        xs[idx] = x\n",
    "    return xs\n",
    "\n",
    "\n",
    "def plot_ssm_dynamics(dt, ts, us, xs, halfrange=0.1):\n",
    "    duration = ts[-1] - ts[0]\n",
    "    fig = Figure(figsize=(8, 4))\n",
    "    fig.create_grid(2, 2, height_ratios=[1, 1], width_ratios=[40, 1], hspace=0.05, wspace=0.05)\n",
    "    for idx in np.ndindex((2, 2)):\n",
    "        fig[idx].tick_params(axis=\"both\", labelsize=12)\n",
    "    fig[0, 0].plot(ts, us, lw=1.0, color=\"k\")\n",
    "    fig[0, 0].set_xlim([ts[0] - duration * 0.005, ts[-1] + duration * 0.005])\n",
    "    fig[0, 0].set_ylabel(\"Input $u(t)$\", fontsize=14)\n",
    "    fig[0, 0].set_xticklabels([])\n",
    "    fig[0, 1].axis(\"off\")\n",
    "    fig[1, 0].plot_matrix(\n",
    "        xs.T,\n",
    "        column=ts,\n",
    "        aspect=\"auto\",\n",
    "        cax=fig[1, 1],\n",
    "        cmap=\"bwr\",\n",
    "        zscale=\"centered\",\n",
    "        halfrange=halfrange,\n",
    "        xticks_kws=dict(num_tick=6),\n",
    "        yticks_kws=dict(num_tick=2),\n",
    "    )\n",
    "    fig[1, 0].set_ylabel(\"State $x(t)$\", fontsize=14)\n",
    "    fig[1, 0].tick_params(axis=\"both\", labelsize=12)\n",
    "    fig[1, 0].set_xlabel(\"Time $t$ ($\\\\Delta t$: {:.1e})\".format(dt), fontsize=14)\n",
    "    fig.align_labels()\n",
    "    return fig\n",
    "\n",
    "\n",
    "seed = 1234  # Random seed, change it as you like.\n",
    "dt, period = 1e-4, 5.0\n",
    "cutoff_freq = 20.0  # Cutoff frequency of the input signal\n",
    "rms = 0.5  # RMS (root mean square) of the signal\n",
    "theta = 2.5  # Parameter of truncated range\n",
    "basis_num = 128  # Dimension of the state space model\n",
    "disc_method = \"bilinear\"  # Discretization algorithm: \"zoh\" or \"bilinear\"\n",
    "\n",
    "ts, us = white_signal(period + dt, dt=dt, cutoff_freq=cutoff_freq, rms=rms, seed=seed, batch_shape=())\n",
    "us += np.sin(2 * np.pi / (4 / 3 * period) * ts)  # Add a sine wave with long period (4/3*period).\n",
    "esn, w_in = construct_ssm(transition_leg_t, basis_num, dt, disc_method, theta=theta)\n",
    "xs = sample_ssm_dynamics(us, esn, w_in)\n",
    "fig = plot_ssm_dynamics(dt, ts, us, xs, halfrange=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fead458",
   "metadata": {},
   "source": [
    "先程用意した基底関数を係数 $x(t)$ で重みづけて足し合わせると $[t-\\theta, t]$ の区間の $u(t)$ を近似できるはずです。\n",
    "以下のセルで $t=3.5~(k=35{,}000)$ 試してみましょう。\n",
    "果たしてうまく近似できているでしょうか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ffb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstructed_dynamics(t_now, basis, dt, ts, us, xs):\n",
    "    duration = ts[-1] - ts[0]\n",
    "    pos = int(t_now / dt)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 3))\n",
    "    ax.plot(ts, us, label=\"Input\", color=\"k\", lw=1.0)\n",
    "    ax.plot(ss + t_now, xs[pos] @ basis, color=\"C1\", lw=1.0)\n",
    "    ax.axvline(t_now, color=\"gray\", ls=\"--\")\n",
    "    ax.tick_params(axis=\"both\", labelsize=12)\n",
    "    ax.set_xlim([ts[0] - duration * 0.005, ts[-1] + duration * 0.005])\n",
    "    ax.set_xlabel(\"Time $t$ ($\\\\Delta t$: {:.1e})\".format(dt), fontsize=14)\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "t_now = 3.5\n",
    "ss = np.linspace(-period, 0, int(period / dt) + 1)\n",
    "basis = basis_leg_t(basis_num, ss, theta=theta)\n",
    "fig, ax = plot_reconstructed_dynamics(t_now, basis, dt, ts, us, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490d92ff",
   "metadata": {},
   "source": [
    "現在時刻 $t$ を表す変数 `t_now` を自由に変えてみて再実行してください。\n",
    "また`basis_num`や`theta`・`dt`・`period`などのパラメータも変えてみてください。\n",
    "\n",
    "下のセルは `t_now` を連続的に変化させるアニメーションを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fdf3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_reconstructed_dynamics(dt, ts, us, xs, measure, basis, plot_offset=0.0, fps=10, speed_coef=1.0):\n",
    "    duration = ts[-1] - ts[0]\n",
    "    fig, ax = plt.subplots(2, 1, sharex=False, figsize=(8, 4), gridspec_kw=dict(height_ratios=[2, 1], hspace=0.05))\n",
    "    for idx in range(len(ax)):\n",
    "        ax[idx].tick_params(axis=\"both\", labelsize=12)\n",
    "        ax[idx].set_xlim([ts[0] + plot_offset - duration * 0.005, ts[-1] + duration * 0.005])\n",
    "    ax[0].plot(ts, us, color=\"k\", lw=1.0, label=\"Input\")\n",
    "    ln_output = ax[0].plot([], [], color=\"C1\", lw=1.0)[0]\n",
    "    ln_upper = ax[0].axvline(0.0, color=\"#333333\", lw=1.0, ls=\"--\")\n",
    "    text = ax[0].set_title(\"t=0 (k=0)\", fontsize=14)\n",
    "    ln_measure = ax[1].plot(measure, color=\"C2\", lw=1.0)[0]\n",
    "    ln_lower = ax[1].axvline(0.0, color=\"#333333\", lw=1.0, ls=\"--\")\n",
    "\n",
    "    def callback(frame):\n",
    "        t_now = dt * frame\n",
    "        ln_output.set_data(ss + t_now, xs[frame] @ basis)\n",
    "        ln_upper.set_xdata([t_now])\n",
    "        text.set_text(f\"t={t_now:.2f} (k={frame})\")\n",
    "        ln_measure.set_data(ss + t_now, measure)\n",
    "        ln_lower.set_xdata([t_now])\n",
    "        return ln_output, ln_upper, ln_measure, ln_lower\n",
    "\n",
    "    ani = FuncAnimation(\n",
    "        fig,\n",
    "        callback,\n",
    "        frames=range(0, us.shape[0], int((1.0 / dt / fps) * speed_coef)),\n",
    "        interval=int(1000 / fps),\n",
    "        blit=False,\n",
    "    )\n",
    "    plt.close()\n",
    "    return ani\n",
    "\n",
    "\n",
    "ss = np.linspace(-period - theta, 0, int((period + theta) / dt) + 1)\n",
    "measure = measure_leg_t(ss, theta=theta)\n",
    "basis = basis_leg_t(basis_num, ss, theta=theta)\n",
    "animate_reconstructed_dynamics(dt, ts, us, xs, measure, basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31297ac",
   "metadata": {},
   "source": [
    "いずれの時刻 $t$ においても 少ない数の $N$ (デフォルトでは128次元) のみで、$\\mu^{(t)}(s) > 0$となる過去数万ステップ (デフォルトでは 25,000ステップ) の入力時系列をかなり正確に近似できました。\n",
    "この意味でHiPPO-LegTは長期記憶を保持できるLESNとして機能していると言えます。\n",
    "その秘訣を別の視点から探るために、今度はこれまでの章と同様にLESN $(\\bar{A}, \\bar{B})$ の内部結合 $\\bar{A}$ の固有値の分布とスペクトル半径を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eigen(ax, mat, margin_ratio=None):\n",
    "    assert mat.ndim == 2\n",
    "    assert mat.shape[0] == mat.shape[1]\n",
    "\n",
    "    def get_range(valx, valy, ratio=0.05):\n",
    "        xmin, xmax = valx.min(), valx.max()\n",
    "        ymin, ymax = valy.min(), valy.max()\n",
    "        xcen, ycen = (xmin + xmax) / 2, (ymin + ymax) / 2\n",
    "        half = np.maximum(xmax - xmin, ymax - ymin) * (1 + ratio) / 2\n",
    "        return (xcen - half, xcen + half), (ycen - half, ycen + half)\n",
    "\n",
    "    ts = np.linspace(0, 2 * np.pi, 1001)\n",
    "    es = np.linalg.eigvals(mat)\n",
    "    sr = np.max(np.abs(es))\n",
    "    cs = np.linspace(0, 1.0, len(es))\n",
    "    color = plt.get_cmap(\"hsv\")(cs)\n",
    "\n",
    "    ax.tick_params(axis=\"both\", labelsize=12)\n",
    "    ax.scatter(np.real(es), np.imag(es), s=3.0, c=color)\n",
    "    ax.plot(np.cos(ts), np.sin(ts), lw=1.0, ls=\":\", color=\"k\")\n",
    "    if margin_ratio is not None:\n",
    "        xlim, ylim = get_range(np.real(es), np.imag(es), margin_ratio)\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "    ax.set_xlabel(\"real\", fontsize=14)\n",
    "    ax.set_ylabel(\"imag\", fontsize=14)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_title(f\"Spectral radius: {sr:.4e}\", fontsize=14)\n",
    "\n",
    "\n",
    "def plot_eigen_all(mat, margin_ratio=0.5):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    plot_eigen(ax[0], mat)\n",
    "    plot_eigen(ax[1], mat, margin_ratio)\n",
    "    x0, x1 = ax[1].get_xlim()\n",
    "    y0, y1 = ax[1].get_ylim()\n",
    "    rect = matplotlib.patches.Rectangle(\n",
    "        (x0, y0),\n",
    "        x1 - x0,\n",
    "        y1 - y0,\n",
    "        linewidth=1,\n",
    "        edgecolor=\"red\",\n",
    "        facecolor=\"red\",\n",
    "        alpha=0.25,\n",
    "    )\n",
    "    ax[0].add_patch(rect)\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "fig, ax = plot_eigen_all(esn.weight, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5363bdfe",
   "metadata": {},
   "source": [
    "右図は左図を拡大したものです。\n",
    "通常のESNでは単位円上に一様に分布するように初期化しましたが、HiPPO-LegTでは実軸対称でかつ円形に分布しています。\n",
    "またスペクトル半径は1未満であるもののほぼ1に近い値を示しており、その長期記憶特性を支えていると考えられます。\n",
    "\n",
    "次にTOSSM $(A, B)$ のカーネル $K(t)$ に関して、$C=e_{n}^\\top$のときに求められる SSMの基底関数 $K_n(t)$ について、関係式 $K_n(-s)=p^{(0)}_n(s)\\cdot\\mu^{(0)}(s)$ の成立を確認してみましょう。\n",
    "離散化された基底関数 $\\bar{K}_{n}[k]$ は LESN $(\\bar{A}, \\bar{B})$ を用いて以下の式で計算できます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bar{K}_{n}[k] &= \\bar{A}^{k}\\bar{B}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72801e85",
   "metadata": {},
   "source": [
    "言い換えれば、初期値 $x[0]=\\bar{B}$ として LESN $(\\bar{A}, \\bar{B})$ を $k$ ステップ進めると $\\bar{K}_{n}[k]$ が得られます。\n",
    "次のセルで $\\bar{K}_{n}[k]$を計算し、 $p^{(t)}_n(s)\\cdot \\mu^{(t)}(s)$ と比較してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4252b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_sequential_kernel(L, A, B):\n",
    "    xs = np.zeros((A.shape[0], L), dtype=B.dtype)  # (L, N)\n",
    "    xs[:, 0] = B.squeeze(-1)\n",
    "    for idx in range(L - 1):\n",
    "        xs[:, idx + 1] = (A @ xs[:, idx][..., None]).squeeze(-1)\n",
    "    return xs\n",
    "\n",
    "\n",
    "def compare_basis_and_ssm_kernel(ss, measure, basis, ssm_basis, plot_num):\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(8, 5))\n",
    "    ax[0].tick_params(axis=\"both\", labelsize=12)\n",
    "    ax[0].plot(ss, (basis[:plot_num] * measure).T, lw=1.0)\n",
    "    ax[0].set_ylabel(\"$p^{(0)}_n(s)\\\\cdot\\\\mu^{(0)}(s)$\", fontsize=14)\n",
    "    ax[0].invert_xaxis()\n",
    "\n",
    "    ax[1].tick_params(axis=\"both\", labelsize=12)\n",
    "    ax[1].plot(ssm_basis[:plot_num].T, lw=1.0)\n",
    "    ax[1].set_ylabel(\"$\\\\bar{K}_n[k]/\\\\Delta t$\", fontsize=14)\n",
    "\n",
    "    fig.align_labels()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "num_basis, theta, dt = 128, 2.0, 1e-4\n",
    "plot_num = 6\n",
    "ss = np.linspace(-theta * 2, 0, int(theta * 2 / dt) + 1)\n",
    "basis = basis_leg_t(num_basis, ss, theta=theta)\n",
    "measure = measure_leg_t(ss, theta=theta)\n",
    "disc_method = \"zoh\"  # Discretization algorithm: \"zoh\" or \"bilinear\"\n",
    "\n",
    "A, B = transition_leg_t(num_basis, theta=theta)\n",
    "A_bar, B_bar = discretize(dt=dt, A=A, B=B, method=disc_method)\n",
    "ssm_basis = discrete_sequential_kernel(len(ss), A_bar, B_bar) / dt\n",
    "fig, ax = compare_basis_and_ssm_kernel(ss, measure, basis, ssm_basis, plot_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b17abcd",
   "metadata": {},
   "source": [
    "比較のため $p^{(0)}_{n}(s)$ のx軸を反転させて、かつ $\\bar{K}_n[k]$ を $\\Delta t$ で割ってスケーリングしています。\n",
    "$[-\\theta, 0]$ の範囲外も特にthresholdを設けていないのにも関わらず、カーネル $\\bar{K}_n[k]$ は $p^{(0)}_{n} (s) \\cdot \\mu^{(0)} (s)$ をうまく近似できています。\n",
    "また離散化のため多少誤差はありますが $N$ (`num_basis`) を増やすとより近似できるようになります。\n",
    "パラメータを変えてこの点を確かめてみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b641f0",
   "metadata": {},
   "source": [
    "#### HiPPO-FouT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c8310",
   "metadata": {},
   "source": [
    "HiPPOはその名前のとおり高次多項式への射影を行いますが、HiPPOの基底関数は多項式関数のみに限定されるわけではありません。\n",
    "今度はルジャンドル多項式の代わりにフーリエ基底を用いるHiPPO-FouT (truncated Fourier basis) を実装しましょう。\n",
    "HiPPO-FouTもTOSSMであり、$(A, B)$は以下の式で表されます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A_{nk} &= \\frac{1}{\\theta}\\begin{cases}\n",
    "-2 &\\quad \\text{if} ~n = k = 0 \\\\\n",
    "-2\\sqrt{2} &\\quad \\text{if} ~n = 0,~k~\\text{even} \\\\\n",
    "-2\\sqrt{2} &\\quad \\text{if} ~k = 0,~n~\\text{even} \\\\\n",
    "-4 &\\quad \\text{if} ~n, k~\\text{even} \\\\\n",
    "2\\pi k &\\quad \\text{if} ~n - k = 1,~k~\\text{even} \\\\\n",
    "-2\\pi n &\\quad \\text{if} ~k - n = 1,~n~\\text{even} \\\\\n",
    "0 &\\quad \\text{otherwise}\n",
    "\\end{cases} \\tag{HiPPO-FouT A} \\\\\n",
    "B_n &= \\frac{1}{\\theta}\\begin{cases}\n",
    "2 &\\quad \\text{if} ~n = 0 \\\\\n",
    "2\\sqrt{2} &\\quad \\text{if} ~n~\\text{even} \\\\\n",
    "0 &\\quad \\text{otherwise}\n",
    "\\end{cases} \\tag{HiPPO-FouT B}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80993d38",
   "metadata": {},
   "source": [
    "このSSM $(A, B)$に対応する測度 $\\mu^{(t)}(s)$ と基底関数 $\\{p^{(t)}_n(s)\\}_{n=0}^{N-1}$  は以下の式で与えられます (同様に導出は論文<sup>[5,8]</sup>を確認してください)。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu^{(t)}(s) &= \\frac{1}{\\theta}\\mathbb{1}_{[t-\\theta, t]}(s) \\\\\n",
    "p^{(t)}_n(s) &= \\mathbb{1}_{[t-\\theta, t]}(s) \\cdot \\begin{cases}\n",
    "1 &\\quad \\text{if} ~n = 0 \\\\\n",
    "\\sqrt{2}\\sin\\left(\\frac{(n-1)\\pi}{\\theta}(t-s)\\right) &\\quad \\text{if} ~n~\\text{odd} \\\\\n",
    "\\sqrt{2}\\cos\\left(\\frac{n\\pi}{\\theta}(t-s)\\right) &\\quad \\text{if} ~n~\\text{even}\n",
    "\\end{cases}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649e382",
   "metadata": {},
   "source": [
    "Q2.1.\n",
    "\n",
    "上記の式で定義される HiPPO-FouT の $A, B$ を構築する関数`transition_fou_t`実装せよ。\n",
    "ただし 行列のサイズ $N > 0$ および時間幅 $\\theta > 0$ は引数として与えられるものとする。\n",
    "\n",
    "- `transition_fou_t`\n",
    "  - Argument(s):\n",
    "    - `N`: `int`\n",
    "      - $N > 0$\n",
    "    - `theta`: `float`\n",
    "      - $\\theta > 0$\n",
    "  - Return(s):\n",
    "    - `A`: `np.ndarray`\n",
    "      - `shape`: `(N, N)`\n",
    "      - `dtype`: `np.float64`\n",
    "    - `B`: `np.ndarray`\n",
    "      - `shape`: `(N, 1)`\n",
    "      - `dtype`: `np.float64`\n",
    "\n",
    "<details><summary>tips</summary>\n\n",
    "- [`numpy.diag`](https://numpy.org/doc/stable/reference/generated/numpy.diag.html)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c1e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_fou_t(N, theta=1.0):\n",
    "    # TODO\n",
    "    A = np.zeros((N, N))\n",
    "    A[0, 0] = -2\n",
    "    A[0, 2::2] = -2 * math.sqrt(2)\n",
    "    A[2::2, 0] = -2 * math.sqrt(2)\n",
    "    A[2::2, 2::2] = -4\n",
    "    diag = 2 * np.pi * (np.arange(N - 1) // 2)\n",
    "    diag[1::2] = 0\n",
    "    A += np.diag(diag, -1) - np.diag(diag, 1)\n",
    "    B = -A[:, :1] / theta\n",
    "    A = A / theta\n",
    "    return A, B\n",
    "    # end of TODO\n",
    "\n",
    "\n",
    "test_func(transition_fou_t, \"02_01\", multiple_output=True)\n",
    "# show_solution(\"02_01\", \"transition_fou_t\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cca110d",
   "metadata": {},
   "source": [
    "次にHiPPO-FouTに対応する測度 $\\mu^{(t)}(s)$ と基底関数 $p^{(t)}_n(s)$ を実装してみましょう。\n",
    "HiPPO-LegTと同様に $t=0$ として $s\\in (-\\infty, 0]$ 上でそれぞれを実装しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b3002a",
   "metadata": {},
   "source": [
    "Q2.2.\n",
    "\n",
    "長さ $T$ の時間列 $S=\\{s_0, s_1,~\\ldots,~s_{T-1}\\}$ ならびにパラメータ $\\theta>0$が与えられたとき、$t=0$ の時のHiPPO-FouTの測度 $\\mu^{(0)}(s)$ を計算する関数`measure_fou_t`を実装せよ。\n",
    "同様に$x(t)$ の次元 $N > 0, S, \\theta > 0$ が与えられたとき、$t=0$ の時のHiPPO-FouTの基底関数 $p^{(0)}_n(s)$ を計算する関数 `basis_fou_t` を実装せよ。\n",
    "\n",
    "- `measure_leg_t`\n",
    "  - Argument(s):\n",
    "    - `S`: `np.ndarray`\n",
    "      - `shape`: `(T,)`\n",
    "      - `dtype`: `np.float64`\n",
    "    - `theta`: `float`\n",
    "      - $\\theta > 0$\n",
    "  - Return(s):\n",
    "    - `measure`: `np.ndarray`\n",
    "      - `shape`: `(T,)`\n",
    "      - `dtype`: `np.float64`\n",
    "\n",
    "- `basis_fou_t`\n",
    "  - Argument(s):\n",
    "    - `N`: `int`\n",
    "      - $N > 0$\n",
    "    - `S`: `np.ndarray`\n",
    "      - `shape`: `(T,)`\n",
    "      - `dtype`: `np.float64`\n",
    "    - `theta`: `float`\n",
    "      - $\\theta > 0$\n",
    "  - Return(s):\n",
    "    - `basis`: `np.ndarray`\n",
    "      - `shape`: `(N, T)`\n",
    "      - `dtype`: `np.float64`\n",
    "\n",
    "- [`numpy.where`](https://numpy.org/doc/stable/reference/generated/numpy.where.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8046d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_fou_t(S, theta=1.0):\n",
    "    # TODO\n",
    "    measure = np.where((-theta <= S) & (S <= 0), 1.0 / theta, 0.0)\n",
    "    return measure\n",
    "    # end of TODO\n",
    "\n",
    "\n",
    "def basis_fou_t(N, S, theta=1.0):\n",
    "    # TODO\n",
    "    basis = np.zeros((N, len(S)))\n",
    "    non_zero = (-theta <= S) & (S <= 0)\n",
    "    basis[0, non_zero] = 1\n",
    "    basis[1::2, non_zero] = -math.sqrt(2) * np.sin((np.arange(0, N - 1, 2)[:, None] * np.pi * S[non_zero] / theta))\n",
    "    basis[2::2, non_zero] = math.sqrt(2) * np.cos((np.arange(2, N, 2)[:, None] * np.pi * S[non_zero] / theta))\n",
    "    return basis\n",
    "    # end of TODO\n",
    "\n",
    "\n",
    "def solution(N, S, theta=1.0):\n",
    "    # DO NOT CHANGE HERE.\n",
    "    measure = measure_fou_t(S, theta=theta)\n",
    "    basis = basis_fou_t(N, S, theta=theta)\n",
    "    return measure, basis\n",
    "\n",
    "\n",
    "test_func(solution, \"02_02\", multiple_output=True)\n",
    "# show_solution(\"02_02\", \"measure_fou_t\")  # Uncomment it to see the solution.\n",
    "# show_solution(\"02_02\", \"basis_fou_t\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ac98f",
   "metadata": {},
   "source": [
    "HiPPO-LegTのときに実装した `plot_basis` 関数を用いて、$t=0$ のとき $s\\in[-2\\theta, 0]$ の範囲でHiPPO-FouTの基底関数 $p^{(0)}_n(s)$ を描画してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee40d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_basis, theta, dt = 16, 2.0, 1e-4\n",
    "plot_num = 8\n",
    "\n",
    "ss = np.linspace(-2 * theta, 0, int(2 * theta / dt) + 1)\n",
    "measure = measure_fou_t(ss, theta=theta)\n",
    "basis = basis_fou_t(num_basis, ss, theta=theta)\n",
    "products = (basis[:plot_num] * measure) @ basis[:plot_num].T * dt  # Inner products among bases p for μ\n",
    "fig = plot_basis(ss, basis, products, plot_num)\n",
    "fig[0].set_title(\"HiPPO-FouT Basis Functions ($\\\\theta={:.2f}$)\".format(theta), fontsize=14)\n",
    "fig[1].set_title(\"Inner Products\", fontsize=14)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60ab52",
   "metadata": {},
   "source": [
    "$n=1$のとき $p^{(0)}_1(s)=0$ となり内積の値は0になります。\n",
    "これは周波数 0のときの値 $\\cos(0)=1, \\sin(0)=0$ に対応しています。\n",
    "また$A$ の2列目ならびに2行目の値は $0$ なので、SSMの値 $x(t)$ についても $x_1(t) = 0$ となり、そのダイナミクスの階数は1つ減った $N-1$ 次元になります。\n",
    "多少冗長ではありますが $N=2M$次元で 周波数 $M-1$ までの基底関数をちょうど表現できるので、先行研究に倣ってこの形式を採用しています。\n",
    "\n",
    "先ほどと同じデモで、HiPPO-FouTを用いて長期記憶特性を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe67e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234  # Random seed, change it as you like.\n",
    "dt, period = 1e-4, 5.0\n",
    "cutoff_freq = 20.0  # Cutoff frequency of the input signal\n",
    "rms = 0.5  # RMS (root mean square) of the signal\n",
    "theta = 2.5  # Parameter of the truncated range\n",
    "basis_num = 128  # Dimension of the state space model\n",
    "disc_method = \"bilinear\"  # Discretization algorithm: \"zoh\" or \"bilinear\"\n",
    "\n",
    "ts, us = white_signal(period + dt, dt=dt, cutoff_freq=cutoff_freq, rms=rms, seed=seed, batch_shape=())\n",
    "us += np.sin(2 * np.pi / (4 / 3 * period) * ts)  # Add a sine wave with long period (4/3*period).\n",
    "esn, w_in = construct_ssm(transition_fou_t, basis_num, dt, disc_method, theta=theta)\n",
    "xs = sample_ssm_dynamics(us, esn, w_in)\n",
    "fig = plot_ssm_dynamics(dt, ts, us, xs, halfrange=0.2)\n",
    "fig[1, 0].axhline(cutoff_freq * 2 * theta + 2, color=\"gray\", lw=2.0, ls=\"--\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda5524",
   "metadata": {},
   "source": [
    "今カットオフ周波数がデフォルトでは $f=20$ Hz に設定されているので、$n > 2 \\theta f + 2 =102$ のとき (図中の点線)、$x_n(t)$ の値がほぼ0になっています。\n",
    "\n",
    "HiPPO-LegTと同じようにHiPPO-FouTでもアニメーションを描画してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1958248",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = np.linspace(-period - theta, 0, int((period + theta) / dt) + 1)\n",
    "measure = measure_fou_t(ss, theta=theta)\n",
    "basis = basis_fou_t(basis_num, ss, theta=theta)\n",
    "animate_reconstructed_dynamics(dt, ts, us, xs, measure, basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bda8fd",
   "metadata": {},
   "source": [
    "入力時系列がもともとフーリエ逆変換、すなわち $\\sin$ と $\\cos$ の重ね合わせによって構成されているため、先程のHiPPO-LegTよりもさらに正確に近似できています。\n",
    "HiPPO-FouTの挙動を確認するために、$\\theta$ や $N$ (`num_basis`) 、 $f$ (`cutoff_freq`) を変更して何度も試してみましょう。\n",
    "\n",
    "同様に、離散化されたHiPPO-FouTのLESN $(\\bar{A}, \\bar{B})$ の固有値の分布とスペクトル半径を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ba287",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_eigen_all(esn.weight, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684665a0",
   "metadata": {},
   "source": [
    "先程確認したHiPPO-FouTよりさらに極端で固有値が $z=1$ 付近の単位円周上に分布しており、スペクトル半径が1になっています。\n",
    "\n",
    "最後に離散化されたSSMの基底関数 $\\bar{K}_n[k]$ と、 $p^{(t)}_n(s)\\cdot \\mu^{(t)}(s)$ と比較してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abdca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_basis, theta, dt = 512, 2.0, 1e-4\n",
    "plot_num = 6\n",
    "ss = np.linspace(-theta * 2, 0, int(theta * 2 / dt) + 1)\n",
    "basis = basis_fou_t(num_basis, ss, theta=theta)\n",
    "measure = measure_fou_t(ss, theta=theta)\n",
    "disc_method = \"bilinear\"  # Discretization algorithm: \"zoh\" or \"bilinear\"\n",
    "\n",
    "A, B = transition_fou_t(num_basis, theta=theta)\n",
    "A_bar, B_bar = discretize(dt=dt, A=A, B=B, method=disc_method)\n",
    "ssm_basis = discrete_sequential_kernel(len(ss), A_bar, B_bar) / dt\n",
    "fig, ax = compare_basis_and_ssm_kernel(ss, measure, basis, ssm_basis, plot_num)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f4c63",
   "metadata": {},
   "source": [
    "HiPPO-FouTでもHiPPO-LegTと同様に、$\\bar{K}_n[k]$ が $p^{(0)}_n(s) \\cdot \\mu^{(0)}(s)$ をうまく近似できています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c13508",
   "metadata": {},
   "source": [
    "#### HiPPO-LegS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8fcef9",
   "metadata": {},
   "source": [
    "今度はHiPPO-LegS (scaled Legendre measure) と呼ばれるTOSSMを実装しましょう。\n",
    "これまでのHiPPO-LegTやHiPPO-FouT では過去 $\\theta$ 分の入力時系列を等価に加味する測度を用いていましたが、HiPPO-LegSでは全過去時間において指数的に減衰する重み付けを行う測度を用います。\n",
    "HiPPO-LegSは以下の式で表される SSM $(A, B)$ です。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A_{nk} &= \\frac{1}{\\tau}\\begin{cases}\n",
    "-\\sqrt{(2n+1)(2k+1)} &\\quad \\text{if} ~n > k \\\\\n",
    "-(n+1) &\\quad \\text{if} ~n = k \\\\\n",
    "0 &\\quad \\text{if} ~n < k \\\\\n",
    "\\end{cases} \\tag{HiPPO-LegS A} \\\\\n",
    "B_n &= \\frac{1}{\\tau}\\sqrt{2n+1} \\tag{HiPPO-LegS B}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02e1918",
   "metadata": {},
   "source": [
    "$\\tau > 0$ は減衰の速度を制御する時定数で、$\\tau$ 経過ごとに重み付けの値が $1/e$ に減衰します。\n",
    "HiPPO-LegSに対応する測度 $\\mu^{(t)}(s)$ と基底関数 $\\{p^{(t)}_n(s)\\}_{n=0}^{N-1}$  は以下の式で与えられます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu^{(t)}(s) &= \\frac{1}{\\tau}e^{-(t-s)/\\tau} \\mathbb{1}_{(-\\infty, t]}(s)\\\\\n",
    "p^{(t)}_n(s) &= L_n(2e^{-(t-s)/\\tau} - 1) \\mathbb{1}_{(-\\infty, t]}(s)\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd890f13",
   "metadata": {},
   "source": [
    "式の形から推測されるとおり、HiPPO-LegSは直近の入力時系列を重視しますが、HiPPO-LegTやHiPPO-FouTのように過去 $\\theta$ 以前の入力情報を完全に無視するわけではありません。\n",
    "まずはこの $A, B$ を実装しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba3c44",
   "metadata": {},
   "source": [
    "Q2.1.\n",
    "\n",
    "上記の式で定義される HiPPO-LegS の $A, B$ を構築する関数`transition_leg_s`実装せよ。\n",
    "ただし 行列のサイズ $N > 0$ と時定数 $\\tau > 0$ は引数として与えられるものとする。\n",
    "\n",
    "- `transition_leg_s`\n",
    "  - Argument(s):\n",
    "    - `N`: `int`\n",
    "      - $N > 0$\n",
    "    - `tau`: `float`\n",
    "      - $\\tau > 0$\n",
    "  - Return(s):\n",
    "    - `A`: `np.ndarray`\n",
    "      - `shape`: `(N, N)`\n",
    "      - `dtype`: `np.float64`\n",
    "    - `B`: `np.ndarray`\n",
    "      - `shape`: `(N, 1)`\n",
    "      - `dtype`: `np.float64`\n",
    "\n",
    "<details><summary>tips</summary>\n",
    "\n",
    "- [`numpy.diag`](https://numpy.org/doc/stable/reference/generated/numpy.diag.html)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64455174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_leg_s(N, tau=1.0):\n",
    "    # TODO\n",
    "    arange = np.arange(N)\n",
    "    base = (2 * arange + 1) ** 0.5\n",
    "    diff = arange[:, None] - arange[None, :]\n",
    "    A = -base[:, None] * base[None, :]\n",
    "    A[diff < 0] = 0\n",
    "    A[arange, arange] = -arange - 1\n",
    "    B = np.array(base)[:, None]\n",
    "    A /= float(tau)\n",
    "    B /= float(tau)\n",
    "    return A, B\n",
    "    # end of TODO\n",
    "\n",
    "\n",
    "test_func(transition_leg_s, \"03_01\", multiple_output=True)\n",
    "# show_solution(\"03_01\", \"transition_leg_s\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803e70b",
   "metadata": {},
   "source": [
    "次にHiPPO-LegSに対応する測度 $\\mu^{(t)}(s)$ を実装してみましょう。\n",
    "HiPPO-LegSも同様に $t=0$ として $s\\in (-\\infty, 0]$ 上の $\\mu^{(0)}(s)$ を考えましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc226a",
   "metadata": {},
   "source": [
    "Q3.2.\n",
    "\n",
    "長さ $T$ の時間列 $S=\\{s_0, s_1,~\\ldots,~s_{T-1}\\}$ と 時定数 $\\tau > 0$ が与えられる。\n",
    "$t=0$ の時のHiPPO-LegSの測度 $\\mu^{(0)}(s)$ を計算する関数`measure_leg_s`を実装せよ。\n",
    "\n",
    "- `measure_leg_s`\n",
    "  - Argument(s):\n",
    "    - `S`: `np.ndarray`\n",
    "      - `shape`: `(T,)`\n",
    "      - `dtype`: `np.float64`\n",
    "    - `tau`: `float`\n",
    "      - $\\tau > 0$\n",
    "  - Return(s):\n",
    "    - `measure`: `np.ndarray`\n",
    "      - `shape`: `(T,)`\n",
    "      - `dtype`: `np.float64`\n",
    "\n",
    "- [`numpy.exp`](https://numpy.org/doc/stable/reference/generated/numpy.exp.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf57ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_leg_s(S, tau=1.0):\n",
    "    # TODO\n",
    "    measure = np.exp(S / tau) * (1 / tau)\n",
    "    return measure\n",
    "    # end of TODO\n",
    "\n",
    "\n",
    "test_func(measure_leg_s, \"03_02\")\n",
    "# show_solution(\"03_02\", \"measure_leg_s\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a258bf6",
   "metadata": {},
   "source": [
    "続けて HiPPO-LegSの基底関数 $p^{(0)}_n(s)$ も実装します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d933ffae",
   "metadata": {},
   "source": [
    "Q3.3.\n",
    "\n",
    "$x(t)$ の次元 $N > 0$ 、長さ $T$ の時間列 $S=\\{s_0, s_1,~\\ldots,~s_{T-1}\\}$ と時定数 $\\tau > 0$ が与えられたとき、$t=0$ の時のHiPPO-LegSの基底関数 $p^{(0)}_n(s)$ を計算する関数 `basis_leg_s` を実装せよ。\n",
    "\n",
    "- `basis_leg_s`\n",
    "  - Argument(s):\n",
    "    - `N`: `int`\n",
    "      - $N > 0$\n",
    "    - `S`: `np.ndarray`\n",
    "      - `shape`: `(T,)`\n",
    "      - `dtype`: `np.float64`\n",
    "    - `tau`: `float`\n",
    "      - $\\tau > 0$\n",
    "  - Return(s):\n",
    "    - `basis`: `np.ndarray`\n",
    "      - `shape`: `(N, T)`\n",
    "      - `dtype`: `np.float64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb32102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basis_leg_s(N, S, tau=1.0):\n",
    "    # TODO\n",
    "    basis = np.zeros((N, len(S)))\n",
    "    non_zero = S <= 0\n",
    "    poly = Legendre(2 * np.exp(S[non_zero] / tau) - 1)\n",
    "    for idx in range(N):\n",
    "        basis[idx, non_zero] = poly[idx]  # Evaluate the n-th order polynomial\n",
    "    basis[:, non_zero] *= ((2 * np.arange(N) + 1) ** 0.5)[:, None]\n",
    "    return basis\n",
    "    # end of TODO\n",
    "\n",
    "\n",
    "test_func(basis_leg_s, \"03_03\")\n",
    "# show_solution(\"03_03\", \"basis_leg_s\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e508badb",
   "metadata": {},
   "source": [
    "まずは実装した基底関数の直交性を次のセルで確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8424311",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_basis, tau, dt = 16, 1.0, 1e-4\n",
    "plot_num = 8\n",
    "\n",
    "ss = np.linspace(-tau * 10, 0, int(tau * 10 / dt) + 1)\n",
    "measure = measure_leg_s(ss, tau=tau)\n",
    "basis = basis_leg_s(num_basis, ss, tau=tau)\n",
    "products = (basis[:plot_num] * measure) @ basis[:plot_num].T * dt  # Inner products among bases p for μ\n",
    "fig = plot_basis(ss, basis, products, plot_num)\n",
    "fig[0].set_title(\"HiPPO-LegS Basis Functions\", fontsize=14)\n",
    "fig[1].set_title(\"Inner Products\", fontsize=14)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c597fa",
   "metadata": {},
   "source": [
    "基底関数 $p^{(t)}_n(s)$ は $(-\\infty, t]$ 上で定義されますが、$s \\to -\\infty$ のときも $0$ 以外の値を取ります。\n",
    "しかし測度 $\\mu^{(t)}(s)$ が指数関数的に減衰するため、内積 $\\langle {p^{(0)}_i, p^{(0)}_j} \\rangle_{\\mu^{(0)}}$ が計算できます。\n",
    "右図に示されるとおり概ね $\\langle {p^{(0)}_i, p^{(0)}_j} \\rangle_{\\mu^{(0)}} = \\delta_{ij}$ となり、その $\\mu^{(t)}(s)$ 上での直交性を確認できます。\n",
    "\n",
    "同じ時系列を使ってHiPPO-LegSの特性を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298c00df",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234  # Random seed, change it as you like.\n",
    "dt, period = 1e-4, 5.0\n",
    "cutoff_freq = 20.0  # Cutoff frequency of the input signal\n",
    "rms = 0.5  # RMS (root mean square) of the signal\n",
    "tau = 1.0  # Parameter of the exponential scale\n",
    "basis_num = 128  # Dimension of the state space model\n",
    "disc_method = \"bilinear\"  # Discretization algorithm: \"zoh\" or \"bilinear\"\n",
    "\n",
    "ts, us = white_signal(period + dt, dt=dt, cutoff_freq=cutoff_freq, rms=rms, seed=seed, batch_shape=())\n",
    "us += np.sin(2 * np.pi / (4 / 3 * period) * ts)  # Add a sine wave with long period (4/3*period).\n",
    "esn, w_in = construct_ssm(transition_leg_s, basis_num, dt, disc_method, tau=tau)\n",
    "xs = sample_ssm_dynamics(us, esn, w_in)\n",
    "fig = plot_ssm_dynamics(dt, ts, us, xs, halfrange=0.2)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23cb601",
   "metadata": {},
   "source": [
    "同様に再構築された時系列 $\\sum_{n=0}^{N-1} x(t)_n p^{(t)}_n(s)$ と測度の時間変化を描画するアニメーションを生成してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = np.linspace(-period, 0, int(period / dt) + 1)\n",
    "measure = measure_leg_s(ss, tau=tau)\n",
    "basis = basis_leg_s(basis_num, ss, tau=tau)\n",
    "animate_reconstructed_dynamics(dt, ts, us, xs, measure, basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c79cc",
   "metadata": {},
   "source": [
    "測度 $\\mu^{(t)}(s)$ の特性のおかげで、HiPPO-LegSは直近の入力時系列をより正確に再構築します。\n",
    "しかし $\\mu^{(t)}(s)$ の値が小さくなる範囲でも大まかにその波形を捉えている様子が見られます。\n",
    "時定数 $\\tau$ を変えてみて、その挙動の違いを確認してみてください。\n",
    "\n",
    "今度も同様にLESN $(\\bar{A}, \\bar{B})$ の固有値の分布とスペクトル半径を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_eigen_all(esn.weight, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc47a4cb",
   "metadata": {},
   "source": [
    "HiPPO-LegSにおける $A$ は下三角行列であるため、その固有値はその対角成分すなわち $-(n+1)/\\tau~(n=0,1,2,~\\ldots,~N-1)$ となり、$\\bar{A}$ も実軸上に分布します。\n",
    "\n",
    "同じくSSMの基底関数 $\\bar{K}_n[k]$ と、 $p^{(t)}_n(s)\\cdot \\mu^{(t)}(s)$ も比較してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30db163",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_basis, tau, dt = 1024, 1.0, 1e-4\n",
    "plot_num = 6\n",
    "ss = np.linspace(-tau, 0, int(tau / dt) + 1)\n",
    "basis = basis_leg_s(num_basis, ss, tau=tau)\n",
    "measure = measure_leg_s(ss, tau=tau)\n",
    "disc_method = \"bilinear\"  # Discretization algorithm: \"zoh\" or \"bilinear\"\n",
    "\n",
    "A, B = transition_leg_s(num_basis)\n",
    "A_bar, B_bar = discretize(dt=dt, A=A, B=B, method=disc_method)\n",
    "ssm_basis = discrete_sequential_kernel(len(ss), A_bar, B_bar) / dt\n",
    "fig, ax = compare_basis_and_ssm_kernel(ss, measure, basis, ssm_basis, plot_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46af11c",
   "metadata": {},
   "source": [
    "図に示されるようにHiPPO-LegSにおける $\\bar{K}_n[k]$ は緩やかに減衰する形状を有しています。\n",
    "結果的にHiPPO-LegSではその畳込みである $y(t)$ も過去の入力時系列の情報を長期間に渡って反映できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbbab81",
   "metadata": {},
   "source": [
    "Q3.4. (Advanced)\n",
    "\n",
    "- TOSSMではなくなるが、HiPPO-LegS での $A, B$ を用いた SSM $(\\frac{1}{t}A, \\frac{1}{t}B)$ は、$\\mu^{(t)}_n(s) = \\frac{1}{t} \\mathbb{1}_{[0, t]}(s)$ および $p^{(t)}_n(s) = L_n\\left(2s/t - 1\\right) \\mathbb{1}_{[0, t]}(s)$ に対するOSSMとなる<sup>[5]</sup>。 すなわち $[0, t]$ 上で一様に重み付けされたルジャンドル多項式への射影を行う。この時変なOSSMを実装し、挙動を確認せよ。\n",
    "    - メモ1: [元論文のデモ](https://github.com/state-spaces/s4/blob/main/notebooks/hippo_function_approximation.ipynb) におけるクラス `HiPPOScale` に実装があるので参考にせよ。\n",
    "    - メモ2: 実はこのSSM $(\\frac{1}{t}A, \\frac{1}{t}B)$ が先に[5]において提案され、SSM $(A, B)$ の方は、[8]においてそれがTOSSMであると証明されたがわかりやすさのために、本稿では後者を先に紹介した。\n",
    "- 論文<sup>[5]</sup>では他の基底関数についても言及されており、例えば[ラゲール多項式](https://en.wikipedia.org/wiki/Laguerre_polynomials)や[チェビシェフ多項式](https://en.wikipedia.org/wiki/Chebyshev_polynomials)を用いたSSMの例が示されている。これらを実装し同様に挙動を確認せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c11a76",
   "metadata": {},
   "source": [
    "### S4・S4Dの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2926f47d",
   "metadata": {},
   "source": [
    "ここまではHiPPOの理論に基づき長期記憶を保持できるSSMを構築する方法を用いましたが、ここからはより実践的な視点から効率的にSSMを計算しかつニューラルネットワークに組み込む方法を学びます。\n",
    "\n",
    "なおこの節は主に技術的な詳細に立ち入りかつ一部難解な箇所がありますが、本質的にはいずれも線形RNNの実装の一形態に過ぎません。\n",
    "またここで紹介されるすべてのテクニックが使用されるわけでもありません。\n",
    "実際にSSMのニューラルネットワーク内での学習がどのように実装されるか知りたい方は、以下のS4・S4Dにおけるセルを実行した上で、先の節に進んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f195514",
   "metadata": {},
   "source": [
    "#### S4と安定かつ高速な線形RNNの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ddab9e",
   "metadata": {},
   "source": [
    "まずは **S4 (Structured State Space for Sequence Modeling)<sup>[6]</sup>** において導入された安定かつ高速に線形RNNを計算する方法を扱います。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a1088",
   "metadata": {},
   "source": [
    "##### 対角化による高速化と近似の必要性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13bc88",
   "metadata": {},
   "source": [
    "ここまで紹介した3つのTOSSMは、その名前のとおり時不変なシステムであるので、線形ESNの時間発展として計算できます。\n",
    "一般に $T$ ステップの入力時系列に対してESNの内部ダイナミクスを計算するためには、$T$ 回の行列ベクトル積が必要なので、結果的に $O(N^2 T)$ の計算量が必要となります。\n",
    "しかし今回活性化関数を持たない線形ESNを考えるので、内部結合 $\\bar{A}$ の対角化を予め計算して、その計算量を $O(NT)$ に削減できます。\n",
    "すなわちある正則行列 $V$ によって $V^{-1} \\bar{A} V = \\Lambda$ と対角化できるとき、$\\bar{A}^k = V \\Lambda^k V^{-1}$ ($A$ も $\\bar{A}$ も同じ $V$ を用いて対角化できます) が成立するので、離散化されたSSMの基底関数 $\\bar{K}_n^{(T)} = \\left\\{\\left(\\bar{A}^k \\bar{B}\\right)_{n}\\right\\}_{k=0}^{T-1}$ ならびに、ある出力行列 $C\\in \\mathbb{R}^{1 \\times N}$ に対するカーネル $\\bar{K}^{(T)} = \\sum_{n=0}^{N-1} C_n \\bar{K}_{n}^{(T)}$ 以下のように計算できます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bar{K}_n^{(T)} &= \\left[\\bar{B}, \\bar{A}\\bar{B}, \\bar{A}^2\\bar{B},~\\ldots,~\\bar{A}^{T-1}\\bar{B}\\right]_{n} \\\\\n",
    "&= \\left(\\left[I, \\bar{A}, \\bar{A}^2,~\\ldots,~\\bar{A}^{T-1}\\right] \\bar{B}\\right)_{n} \\\\\n",
    "&= V_n \\left[I, \\bar{\\Lambda}, \\bar{\\Lambda}^2,~\\ldots,~\\bar{\\Lambda}^{T-1}\\right] V^{-1}\\bar{B} \\\\\n",
    "&= \\left(V_n \\odot \\left(V^{-1}\\bar{B}\\right)^\\top\\right) \\begin{bmatrix}\n",
    "1 & \\bar{\\lambda}_0 & \\bar{\\lambda}_0^2 & \\cdots & \\bar{\\lambda}_0^{T-1} \\\\\n",
    "1 & \\bar{\\lambda}_1 & \\bar{\\lambda}_1^2 & \\cdots & \\bar{\\lambda}_1^{T-1} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & \\bar{\\lambda}_{N-1} & \\bar{\\lambda}_{N-1}^2 & \\cdots & \\bar{\\lambda}_{N-1}^{T-1}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\left(V_n \\odot \\left(V^{-1}\\bar{B}\\right)^\\top\\right) \\mathcal{V}^{(T)}(\\mathrm{diag}(\\bar{\\Lambda})) \\\\\n",
    "\\bar{K}^{(T)}\n",
    "&= \\sum_{n=0}^{N-1} C_n \\bar{K}_n^{(T)} \\\\\n",
    "&= \\left((C V) \\odot (V^{-1}\\bar{B})^\\top \\right) \\mathcal{V}^{(T)}(\\mathrm{diag}(\\bar{\\Lambda}))\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba7096",
   "metadata": {},
   "source": [
    "ここで $\\odot$ はアダマール積を表します。\n",
    "また $N \\times T$ 行列 $\\mathcal{V}^{(T)}(\\{\\bar{\\lambda_k}\\}_{k=0}^{N-1})_{nl}:=\\bar{\\lambda}_{n}^{l}$ はVandermonde行列と呼ばれます。\n",
    "$N$ 次元ベクトルと $N \\times T$ 行列の積であるため、計算量は $O(NT)$ となります。\n",
    "最終的に出力行列 $C$ に対する SSMの出力 $y[k]$ は $\\bar{K}^{(T)}$ と入力時系列 $u[k]$ の畳み込み以下の形で計算されます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y[k] &= (\\bar{K}^{(T)} * u)[k] \\\\\n",
    "&= \\sum_{m=0}^{T-1} \\bar{K}^{(T)}[m] u[k-m]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d2350",
   "metadata": {},
   "source": [
    "この畳込み計算において、各$k$ に対して愚直に計算すると全体で $O(T^2)$ の計算量が必要ですが、高速フーリエ変換 (FFT; Fast Fourier Transform) とその逆変換 (IFFT; Inverse FFT) を活用して $O(T \\log T)$ の計算量で完了します。\n",
    "したがって畳み込みフィルタの構築とフーリエ変換を合わせてトータルでは $O(NT+T\\log T)$ の計算量でSSMの出力を計算できます。\n",
    "以下のコードはFFTとIFFTを用いてある出力行列 $C$ に対するSSMの出力を計算するデモンストレーションです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d8d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234  # Random seed, change it as you like.\n",
    "dt, period = 1e-4, 5.0\n",
    "cutoff_freq = 20.0  # Cutoff frequency of the input signal\n",
    "rms = 0.5  # RMS (root mean square) of the signal\n",
    "tau = 1.0  # Parameter of the exponential scale\n",
    "basis_num = 128  # Dimension of the state space model\n",
    "disc_method = \"bilinear\"  # Discretization algorithm: \"zoh\" or \"bilinear\"\n",
    "\n",
    "# input signal\n",
    "ts, us = white_signal(period + dt, dt=dt, cutoff_freq=cutoff_freq, rms=rms, seed=seed, batch_shape=())\n",
    "us += np.sin(2 * np.pi / (4 / 3 * period) * ts)  # Add a sine wave with long period (4/3*period).\n",
    "\n",
    "# state space model (A, B)\n",
    "esn, w_in = construct_ssm(transition_leg_s, basis_num, dt, disc_method, tau=tau)\n",
    "\n",
    "# output weight (C)\n",
    "rnd = np.random.default_rng(seed)\n",
    "C = rnd.normal(0.0, 1.0, size=(1, basis_num))  # Random readout\n",
    "w_out = Linear(basis_num, 1, weight=C)\n",
    "\n",
    "# O(N^2 T) implementation (naive RNN)\n",
    "xs = sample_ssm_dynamics(us, esn, w_in)  # (T, N)\n",
    "ys = w_out(xs)  # (T, 1)\n",
    "\n",
    "# O(NT + T log T) implementation (using FFT)\n",
    "kernel = discrete_sequential_kernel(\n",
    "    len(ts), esn.weight, w_in.weight\n",
    ")  # NOTE: Current time complexity is O(N^2 T); will optimize to O(NT) in the next section.\n",
    "kernel = w_out(kernel.T).squeeze(-1)  # (T,)\n",
    "ud = np.fft.rfft(us, n=2 * len(ts))  # FFT of input signal, (T,)\n",
    "kd = np.fft.rfft(kernel, n=2 * len(ts), axis=-1)  # FFT of kernel, (T,)\n",
    "yd = np.fft.irfft(ud * kd, n=2 * len(ts), axis=-1)[: len(ts)]  # Inverse FFT to get output signal, (T,)\n",
    "\n",
    "# Compare the results\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 5), sharex=True, gridspec_kw={\"height_ratios\": [1, 2], \"hspace\": 0.05})\n",
    "ax[0].plot(ts, us, color=\"k\", lw=1.0)\n",
    "ax[0].set_ylabel(\"Input $u(t)$\", fontsize=14)\n",
    "ax[0].tick_params(axis=\"both\", labelsize=12)\n",
    "ax[1].plot(ts, ys, label=\"RNN ($O(N^2 L)$)\", color=\"gray\", lw=2.5, ls=\"--\")\n",
    "ax[1].plot(ts, yd, label=\"FFT ($O(L \\\\log L)$)\", color=\"red\", lw=1.0)\n",
    "ax[1].tick_params(axis=\"both\", labelsize=12)\n",
    "ax[1].set_xlim([ts[0] - (ts[-1] - ts[0]) * 0.005, ts[-1] + (ts[-1] - ts[0]) * 0.005])\n",
    "ax[1].set_xlabel(\"Time $t$ ($\\\\Delta t$: {:.1e})\".format(dt), fontsize=14)\n",
    "ax[1].set_ylabel(\"Output $y(t)$\", fontsize=14)\n",
    "ax[1].legend(\n",
    "    fontsize=12,\n",
    "    loc=\"upper left\",\n",
    "    frameon=False,\n",
    "    bbox_to_anchor=(1.0, 1.0),\n",
    ")\n",
    "ax[0].set_title(f\"Maximum absolute error: {np.max(np.abs(ys[:, 0] - yd)):.2e}\", fontsize=14)\n",
    "fig.align_labels()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430bd6b0",
   "metadata": {},
   "source": [
    "確かにRNNによる出力とFFTによるそれが一致しています。\n",
    "このように長さ $T$ が既知の場合、一度カーネルが得られればそれを再利用してFFTによって効率的にSSMの出力を計算できます。\n",
    "またFFTのコードで示されるように、内部状態の計算なしにSSMの出力を直接計算できるので、メモリ使用量も削減できます。\n",
    "このようにFFTによる計算は効率的であるので、結果的にカーネルの計算の高速化がSSMの計算において重要なポイントとなります。\n",
    "特にカーネルがパラメータに依存し可変の場合、学習や推論のたびにカーネルを計算する必要があるため、その高速化の重要性はさらに増します。\n",
    "\n",
    "さて先程カーネルの計算を対角化して高速に構築できると述べましたが、実は一部のSSMでは数値的に安定しません。\n",
    "特にこれまで扱ったHiPPO-LegSでは、一定以上の$N$ になると 対角化による誤差が爆発的に増大しSSMの出力が不安定になります。\n",
    "次のセルはHiPPO-LegSのSSM $(A, B)$ とその離散化 LESN $(\\bar{A}, \\bar{B})$ について $\\bar{A}$ を対角化し $V$ で復元した際の誤差 $\\|\\bar{A} - V \\Lambda V^{-1}\\|_2/\\|\\bar{A}\\|_2$ を描画します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab787db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diagonalization_error(transition, num_bases, dt, disc_method):\n",
    "    errors, conds = [], []\n",
    "    for N in num_bases:\n",
    "        A, B, *_ = transition(N)\n",
    "        A_bar, _B_bar = discretize(dt=dt, A=A, B=B, method=disc_method)\n",
    "        eig, V = np.linalg.eig(A_bar)\n",
    "        V_inv = np.linalg.inv(V)\n",
    "        error = np.linalg.norm((V @ np.diag(eig) @ V_inv) - A_bar)\n",
    "        errors.append(error / np.linalg.norm(A_bar))\n",
    "        cond = np.linalg.cond(V)\n",
    "        conds.append(cond)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, sharex=True, figsize=(8, 6), gridspec_kw={\"hspace\": 0.2, \"height_ratios\": [3, 2]})\n",
    "    ax[0].plot(num_bases, errors, marker=\"o\", label=\"Ratio\")\n",
    "    ax[0].set_title(\"Error ratio $\\\\|\\\\bar{A} - V\\\\Lambda V^{-1}\\\\|_2 / \\\\|\\\\bar{A}\\\\|_2$\", fontsize=14)\n",
    "    ax[0].axhline(1e0, color=\"gray\", ls=\"--\", label=\"Ratio=1\")\n",
    "    ax[1].plot(num_bases, conds, marker=\"o\", label=\"cond($V$)\")\n",
    "    ax[1].set_title(\"Condition Number of $V$\", fontsize=14)\n",
    "    ax[1].set_xlabel(\"N: Number of Basis Functions\", fontsize=14)\n",
    "    ax[1].axhline(1 / np.finfo(V.dtype).eps, color=\"red\", ls=\"--\", label=\"Precision Limit\")\n",
    "    ax[1].axhline(1e0, color=\"gray\", ls=\"--\", label=\"cond($V$)=1\")\n",
    "    for idx in range(len(ax)):\n",
    "        ax[idx].tick_params(axis=\"both\", labelsize=12)\n",
    "        ax[idx].set_xscale(\"log\")\n",
    "        ax[idx].set_yscale(\"log\")\n",
    "        ax[idx].grid(which=\"both\", ls=\":\")\n",
    "        ax[idx].legend(\n",
    "            loc=\"upper left\",\n",
    "            fontsize=12,\n",
    "            frameon=False,\n",
    "            bbox_to_anchor=(1.0, 1.0),\n",
    "        )\n",
    "    fig.align_labels()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "num_bases = [2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "dt, disc_method = 1e-4, \"bilinear\"\n",
    "fig, ax = plot_diagonalization_error(transition_leg_s, num_bases, dt, disc_method)\n",
    "fig.suptitle(\"HiPPO-LegS A\", fontsize=16)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f5af0",
   "metadata": {},
   "source": [
    "ご覧のとおり誤差の値が爆発的に増大し$N=20$ 付近でその比率が1を超えています。\n",
    "また下図は $V$ の条件数、すなわち最大固有値と最小固有値の絶対値の比を示していますが、その値も非常に大きくなっています。\n",
    "これは行列 $V$ の要素が非常に大きくなり、その固有値の範囲が非常に広くなるためです。\n",
    "実際HiPPO-LegSのSSM $(A, B)$ に関して、行列 $A$ (正確には$A$と同値関係にある $\\mathrm{diag}({(-1)^{k} / B_k})\\cdot (-A) \\cdot \\mathrm{diag}((-1)^{k} B_k)$ ) は二項係数を要素に持つ行列 $V=\\dbinom{i+j}{i-j}_{ij}$ で対角化されます<sup>[6]</sup> (以下のコードでも確かめてみてください)。\n",
    "この $V$ の成分は非常に大きな値を取る (例えば $i=3k, j=k$ のとき $V_{ij} = \\binom{4k}{2k} \\sim 2^{4k}$ ) ため、浮動小数点が同時に扱える限界のレンジ (例えば倍精度浮動小数点では $2^{-52}= 2.22 \\times 10^{-16}$ の逆数) を少し $N$ を大きくするだけで超えてしまうのです。\n",
    "結果的に数値的に不正確になり、したがって HiPPO-LegS のカーネル $\\bar{K}[k]$ はそのままでは対角化して構築できません。\n",
    "\n",
    "```python\n",
    "# Sample code\n",
    "num_basis = 8\n",
    "arange = np.arange(num_basis)\n",
    "A, B = transition_leg_s(num_basis)\n",
    "B_new = B * (-1) ** (arange + 1)[:, None]\n",
    "A_new = np.diag(1 / B_new[:, 0]) @ (-A) @ np.diag(B_new[:, 0])\n",
    "V_new = scipy.special.comb(\n",
    "    arange[:, None] + arange[None, :],\n",
    "    arange[:, None] - arange[None, :],\n",
    ")\n",
    "assert np.allclose(V_new @ np.diag(np.arange(1, num_basis + 1)) @ np.linalg.inv(V_new), A_new)\n",
    "print(V_new)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c487b2",
   "metadata": {},
   "source": [
    "##### NPLR 形式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3b6f2",
   "metadata": {},
   "source": [
    "この問題に対処するためにS4では、SSMの $A$ の表現としてNPLR (Normal Plus Low-Rank) 表現と呼ばれる形式を導入し、これまで作成した SSMの計算の効率化と安定化の両立を図ります。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A_\\mathrm{NPLR} &= A^{(N)} - P Q^\\top \\\\\n",
    "&= V \\Lambda V^{*} - P Q^\\top \\\\\n",
    "&= V \\left(\\Lambda - (V^{*}P)(V^{*} Q)^{*}\\right) V^{*}\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da5fa4",
   "metadata": {},
   "source": [
    "ここで $V \\in \\mathbb{C}^{N \\times N}$ はユニタリ行列、$V^{*}$はその随伴行列、$\\Lambda = \\mathrm{diag}(\\lambda_0, \\lambda_1,~\\ldots,~\\lambda_{N-1}) \\in \\mathbb{C}^{N \\times N}$ は対角行列、$P, Q \\in \\mathbb{R}^{N \\times r}$ は $r \\ll N$ の低ランク行列です。\n",
    "つまりNPLR表現では、正規行列 (ユニタリ行列で対角化可能な行列) $A^{(N)}$ と低ランク行列 $P, Q$ の積との差で $A$ を分解して表現します。\n",
    "ユニタリ行列 $V$ は固有値の絶対値がすべて1、すなわちその条件数が必ず1になるので、先述の数値計算の不安定性を回避できます。\n",
    "また $P_V:= V^{*}P$ と $Q_V := V^{*}Q$ とし、$\\Lambda - P_V Q_V^{*}$ を $A$ の DPLR (Diagonal Plus Low-Rank) 表現と呼びます。\n",
    "\n",
    "さてそのような表現がこれまで構成したSSMで可能かどうかですが、実はここまで紹介した HiPPOフレームワークにおけるSSMはすべて $r=1$ または $r=2$ のNPLR表現を有します。\n",
    "例えばHiPPO-LegSでは $r=1$ の行列 $P=Q=(\\sqrt{(2n+1)/(2\\tau)})_n$ を用いて以下の $A^{(N)}$ を構築できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230fd49b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "(A^{(N)})_{nk} &= A_{nk} + P_n Q_k \\\\\n",
    "&= A_{nk} + \\frac{1}{\\tau}\\left(\\frac{1}{2} \\sqrt{2n+1} \\sqrt{2k+1} \\right)_{nk} \\\\\n",
    "&= \\frac{1}{2\\tau}\\begin{cases}\n",
    "-\\sqrt{(2n+1)(2k+1)} &\\quad \\text{if} ~n > k \\\\\n",
    "1 &\\quad \\text{if} ~n = k \\\\\n",
    "\\sqrt{(2n+1)(2k+1)} &\\quad \\text{if} ~n < k \\\\\n",
    "\\end{cases} \\tag{HiPPO-LegS $A^{(N)}$} \\\\\n",
    "B_n &= \\frac{1}{\\tau}\\sqrt{2n+1} \\tag{HiPPO-LegS B}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e9d84",
   "metadata": {},
   "source": [
    "$A^{(N)}$ はある実歪行列 $S$ と単位行列 $I$ の線形和で表現されるので $A^{(N)}$ は正規行列であり ( $(I+S)(I+S)^{*} = (I + S)^{*} (I + S)$ )、ユニタリ行列 $V$ で対角化できます。\n",
    "まずはこの $A^{(N)}$ を計算する関数を実装しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4ebc90",
   "metadata": {},
   "source": [
    "Q4.1.\n",
    "\n",
    "上記の式で定義される NPRL-LegS の $A^{(N)}, B, P$ を構築する関数`transition_leg_s_nplr`実装せよ。\n",
    "ただし 行列のサイズ $N > 0$ と時定数 $\\tau > 0$ は引数として与えられるものとする。\n",
    "\n",
    "- `transition_leg_s_nplr`\n",
    "  - Argument(s):\n",
    "    - `N`: `int`\n",
    "      - $N > 0$\n",
    "    - `tau`: `float`\n",
    "      - $\\tau > 0$\n",
    "  - Return(s):\n",
    "    - `An`: `np.ndarray`\n",
    "      - `shape`: `(N, N)`\n",
    "      - `dtype`: `np.float64`\n",
    "    - `B`: `np.ndarray`\n",
    "      - `shape`: `(N, 1)`\n",
    "      - `dtype`: `np.float64`\n",
    "    - `P`: `np.ndarray`\n",
    "      - `shape`: `(N, 1)`\n",
    "      - `dtype`: `np.float64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ced11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_leg_s_nplr(N, tau=1.0):\n",
    "    # TODO\n",
    "    arange = np.arange(N)\n",
    "    base = (2 * arange + 1) ** 0.5\n",
    "    diff = arange[:, None] - arange[None, :]\n",
    "    An = -base[:, None] * base[None, :]\n",
    "    An[arange, arange] = 1\n",
    "    An[diff <= 0] *= -1\n",
    "    B = np.array(base)[:, None]\n",
    "    An /= 2 * float(tau)\n",
    "    B /= float(tau)\n",
    "    P = np.array(base / (2 * float(tau)) ** 0.5)[:, None]\n",
    "    return An, B, P\n",
    "    # end of TODO\n",
    "\n",
    "\n",
    "test_func(transition_leg_s_nplr, \"04_01\", multiple_output=True)\n",
    "# show_solution(\"04_01\", \"transition_leg_s_nplr\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556939c6",
   "metadata": {},
   "source": [
    "次のセルで同様に、$A^{(N)}$ に関して対角化の安定性を確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bases = [2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "dt, disc_method = 1e-4, \"zoh\"\n",
    "fig, ax = plot_diagonalization_error(transition_leg_s_nplr, num_bases, dt, disc_method)\n",
    "ax[0].set_title(\"Error ratio $\\\\|\\\\bar{A}^{(N)} - V\\\\Lambda V^{-1}\\\\|_2 / \\\\|\\\\bar{A}^{(N)}\\\\|_2$\", fontsize=14)\n",
    "fig.suptitle(\"HiPPO-LegS $\\\\bar{A}^{(N)}$\", fontsize=16)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429deb10",
   "metadata": {},
   "source": [
    "$A^{(N)}$が正規行列のため、いずれの $N$ に対しても条件数が1になり、したがって誤差が小さいまま保たれている様子が確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6720dc2",
   "metadata": {},
   "source": [
    "##### NPLRの離散化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9638d8",
   "metadata": {},
   "source": [
    "さてNPLR表現について $A=V(\\Lambda - P_V Q_V^{*})V^{*}, P_V:=V^{*}P, Q_V:=V^{*}Q$ の形で表されるSSM $(A, B)$ を離散化し求積しなければなりません。\n",
    "ここでは離散化として双線形変換を用います。\n",
    "これまで紹介したとおり双線形変換は次の式で表現されます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bar{A} &= \\left(I - \\frac{\\Delta t}{2} A\\right)^{-1}\\left(I + \\frac{\\Delta t}{2} A\\right) \\\\\n",
    "\\bar{B} &= \\left(I - \\frac{\\Delta t}{2} A\\right)^{-1} \\Delta t B\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacdff02",
   "metadata": {},
   "source": [
    "ここで $P_V$ は低ランク行列であるため、離散化の際にWoodburyの行列恒等式を用いて、上式の各要素は以下の形で効率的に計算できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d12f19",
   "metadata": {},
   "source": [
    "ただし上式の各変数は以下の形で定義されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc105b4",
   "metadata": {},
   "source": [
    "これらを用いて $\\bar{A}$ と $\\bar{B}$ は以下の形に変形されます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V^{*}x[k+1] &= V^{*}\\left(\\bar{A} x[k] + \\bar{B} u[k]\\right) \\\\\n",
    "&= A_1 A_0 \\left(V^{*} x[k]\\right) + 2A_1 \\left(V^{*} B\\right) u[k]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95897413",
   "metadata": {},
   "source": [
    "すなわち $V^{*}$ で座標変換を行い、LESN $(A_1 A_0, 2A_1 V^{*}B)$ の時間発展を計算し $V$ で戻すと、 LESN $(\\bar{A}, \\bar{B})$ を用いた時間発展を計算できます。\n",
    "$A_{0}, A_{1}$ いずれも $N\\times N$ 行列で愚直に計算すると $O(N^2 T)$ の計算量が必要です。\n",
    "しかしいずれの要素もベクトルと対角行列で構成されるので $CV,V^{*}B$ を予め計算し、うまく積の順番を選ぶと $O(NT)$ の計算量で計算できます。\n",
    "まずは、$A^{(N)}, B, P$ から NPLR表現のSSMを離散化する関数を実装しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e844c2",
   "metadata": {},
   "source": [
    "Q4.2.\n",
    "\n",
    "$\\Delta t > 0$ 、 $A^{(n)} = V \\mathrm{diag}(\\Lambda) V^{*}$ を満たす $\\Lambda \\in \\mathbb{C}^{N}, V \\in \\mathbb{C}^{N \\times N}$ 、行列 $B, P, Q \\in \\mathbb{C}^{N \\times 1}$ が与えられたとき、NPLR表現のSSMを離散化する関数 `discretize_bilinear_nplr` を実装せよ。\n",
    "ただし関数はタプル $(A_1,A_0, 2A_1 V^{*}B)$ を出力する。\n",
    "\n",
    "- `discretize_bilinear_nplr`\n",
    "  - Argument(s):\n",
    "    - `dt`: `float`\n",
    "      - $\\Delta t > 0$\n",
    "    - `Lambda`: `np.ndarray`\n",
    "      - `shape`: `(N,)`\n",
    "      - `dtype`: `np.float64 | np.complex128`\n",
    "    - `V`: `np.ndarray`\n",
    "      - `shape`: `(N, N)`\n",
    "      - `dtype`: `np.float64 | np.complex128`\n",
    "    - `B`: `np.ndarray`\n",
    "      - `shape`: `(N, 1)`\n",
    "      - `dtype`: `np.float64 | np.complex128`\n",
    "    - `P`: `np.ndarray`\n",
    "      - `shape`: `(N, 1)`\n",
    "      - `dtype`: `np.float64 | np.complex128`\n",
    "    - `Q`: `np.ndarray`\n",
    "      - `shape`: `(N, 1)`\n",
    "      - `dtype`: `np.float64 | np.complex128`\n",
    "  - Return(s):\n",
    "    - `A_bar_nplr`: `np.ndarray`\n",
    "      - `shape`: `(N, N)`\n",
    "      - `dtype`: `np.float64 | np.complex128`\n",
    "    - `B_bar_nplr`: `np.ndarray`\n",
    "      - `shape`: `(N, 1)`\n",
    "      - `dtype`: `np.float64 | np.complex128`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3de7912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_bilinear_nplr(dt, Lambda, V, B, P, Q):\n",
    "    # TODO\n",
    "    PV = V.conj().T @ P\n",
    "    QV = V.conj().T @ Q\n",
    "    A0 = (2.0 / dt) * np.eye(Lambda.shape[0]) + np.diag(Lambda) - PV @ QV.conj().T\n",
    "    D = np.diag(1.0 / (2.0 / dt - Lambda))\n",
    "    A1 = D - ((D @ PV) * (1.0 / (1 + QV.conj().T @ D @ PV))) @ (QV.conj().T @ D)\n",
    "    A_bar_nplr = A1 @ A0\n",
    "    B_bar_nplr = 2 * A1 @ (V.conj().T @ B)\n",
    "    return A_bar_nplr, B_bar_nplr\n",
    "    # end of TODO\n",
    "\n",
    "\n",
    "test_func(discretize_bilinear_nplr, \"04_02\", multiple_output=True)\n",
    "# show_solution(\"04_02\", \"discretize_bilinear_nplr\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45555f4",
   "metadata": {},
   "source": [
    "以下のセルはその一致を確認します (※このコード自体は $O(N^2L)$ ですが`discrete_sequential_kernel`を改良すると $O(NL)$ になります)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943af398",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_basis, tau, dt, length = 8, 1.0, 1e-3, 2000\n",
    "\n",
    "# Standard HiPPO-LegS\n",
    "A, B = transition_leg_s(num_basis, tau=tau)\n",
    "A_bar, B_bar = discretize(dt=dt, A=A, B=B, method=\"bilinear\")\n",
    "ssm_basis = discrete_sequential_kernel(length, A_bar, B_bar) / dt\n",
    "\n",
    "# HiPPO-LegS with NPLR\n",
    "A_nplr, B_nplr, P = transition_leg_s_nplr(num_basis, tau=tau)\n",
    "Lambda, V = np.linalg.eig(A_nplr)\n",
    "A_bar_nplr, B_bar_nplr = discretize_bilinear_nplr(dt, Lambda, V, B_nplr, P, P)\n",
    "ssm_basis_nplr = V @ discrete_sequential_kernel(length, A_bar_nplr, B_bar_nplr) / dt\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, sharex=True, figsize=(8, 5))\n",
    "for idx in range(len(ax)):\n",
    "    ax[idx].tick_params(axis=\"both\", labelsize=12)\n",
    "ax[0].plot(ssm_basis.T, lw=1.0)\n",
    "ax[0].set_title(\"HiPPO-LegS (standard)\", fontsize=14)\n",
    "ax[1].plot(ssm_basis_nplr.real.T, lw=1.0)\n",
    "ax[1].set_title(\"HiPPO-LegS (NPLR)\", fontsize=14)\n",
    "\n",
    "assert np.allclose(ssm_basis, ssm_basis_nplr.real, atol=1e-8, rtol=1e-8)  # Check the correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788fd7cc",
   "metadata": {},
   "source": [
    "Q4.3. (Advanced)\n",
    "\n",
    "- `transition_leg_s_nplr`と`discrete_sequential_kernel`を改良し、 $\\bar{K}_n^{(L)}$ を $O(NT)$ の計算量で計算せよ。\n",
    "- HiPPO-FouTについても以下の $P=Q$ でNPLR表現が得られる。このNPLR表現を用いて同様に離散化を行い、結果の一致を確認せよ。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P_n = Q_n = \\frac{1}{\\sqrt{\\theta}}\\begin{cases}\n",
    "\\sqrt{2} &\\quad \\text{if} ~n = 0 \\\\\n",
    "2 &\\quad \\text{if} ~n~\\text{even} \\\\\n",
    "0 &\\quad \\text{otherwise}\n",
    "\\end{cases}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bffa088",
   "metadata": {},
   "source": [
    "##### Z変換とCauchy核の計算によるさらなる高速化 (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c7faa1",
   "metadata": {},
   "source": [
    "**注意:** S4論文での重要な技術的貢献として挙げられているポイントを実装例を通じて紹介します。後の節では登場しないので飛ばしても構いません。\n",
    "\n",
    "ここまでで紹介した 計算量$O(NT)$ でのカーネルの計算も実用上は十分に高速ですが、S4が提案された論文<sup>[6]</sup>ではカーネル $\\bar{K}^{(T)} \\in \\mathbb{R}^{T}$ の計算を $O(NT)$ から $\\tilde{O}(N+T)$ に改良する方法が示されているので紹介します。\n",
    "この際まず以下のSSMの母関数 (generating function) $\\hat{\\mathcal{K}}^{(T)}$ を考えます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\mathcal{K}}^{(T)}(z;\\bar{A},\\bar{B},C) &= \\sum_{k=0}^{T-1} C\\bar{A}^{k}\\bar{B} z^{k}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed90ce",
   "metadata": {},
   "source": [
    "この母関数はSSMの畳込みフィルタを時間領域から周波数領域に変換します。\n",
    "この変換はZ変換と呼ばれ、離散時間信号処理において重要な役割を果たします。\n",
    "特に離散列 (この場合 $\\{C\\bar{A}^{k}\\bar{B}\\}_{k}$) のZ変換がわかれば、1の $T$ 乗根 $\\omega_k := \\exp(2\\pi k/T)$ 上で評価し逆フーリエ変換を行うと、目的の畳み込みフィルタのカーネル $\\bar{K}^{(T)}$ を $O(T\\log(T))$ で復元できます。\n",
    "さらにこの母関数は行列のべき乗部分を以下のように逆行列を用いた形で変形できます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\mathcal{K}}^{(T)}(z;\\bar{A},\\bar{B},C)\n",
    "&= C  \\left(I - \\bar{A}^{T}z^{T}\\right) \\left(I - z\\bar{A}\\right)^{-1} \\bar{B} \\\\\n",
    "&= \\tilde{C} \\left(I - z\\bar{A}\\right)^{-1} \\bar{B}\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bbcdc2",
   "metadata": {},
   "source": [
    "ただし $\\tilde{C} := C\\left(I - \\bar{A}^{T}z^{T}\\right)$ です。\n",
    "重要なのはべき乗による逐次計算が逆行列の計算に置換されている点です。\n",
    "このままですと逆行列演算に際し $O(N^3)$ の計算量が各 $\\omega_k$ に対して計 $T$ 回必要になりますが、今座標変換 $B_V:=V^{*}\\bar{B}, \\tilde{C}_{V}:=\\tilde{C}V, P_V:=V^{*}P, Q_V:=V^{*}Q$ によりDPLR表現 $V^{*}AV = \\Lambda - P_V Q_V^{*}$が得られるので、先程の双線形変換とWoodburyの行列恒等式を駆使して以下の式で表現できます (導出は論文[6]を参照してください。特に2式目から3式目の変換が重要です)。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\mathcal{K}}^{(T)}(z;\\bar{A},\\bar{B},C)\n",
    "&= \\tilde{C} \\left(I - z\\bar{A}\\right)^{-1} B \\\\\n",
    "&= \\tilde{C} \\left[I - z \\left(I - \\frac{\\Delta t}{2} A\\right)^{-1}\\left(I + \\frac{\\Delta t}{2} A\\right) \\right]^{-1} \\left(I - \\frac{\\Delta t}{2} A\\right)^{-1} \\Delta t B \\\\\n",
    "&= \\frac{2}{1 + z} \\tilde{C} \\left[\\frac{2}{\\Delta t}\\frac{1-z}{1+z}I - A \\right]^{-1} B \\\\\n",
    "&= \\frac{2}{1 + z} \\tilde{C}_{V} \\left[\\frac{2}{\\Delta t}\\frac{1-z}{1+z}I - \\left(\\Lambda - P_V Q_V^{*}\\right) \\right]^{-1} B_{V} \\\\\n",
    "&= c(z) \\left[k_{z,\\Lambda}(\\tilde{C}_{V},B_{V}) - k_{z,\\Lambda}(\\tilde{C}_{V},P_V)(1+k_{z,\\Lambda}(Q_{V}^{*},P_{V}))^{-1}k_{z,\\Lambda}(Q_{V}^{*},B_{V})\\right]\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e63d57",
   "metadata": {},
   "source": [
    "ただし上式の $C, g, k_{z,\\Lambda}$ は以下の式で定義されます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "c(z) &:= \\frac{2}{1 + z} \\\\\n",
    "k_{z,\\Lambda}(u,v) &:= \\sum_{i=0}^{N-1} \\frac{u_i v_i}{g(z) - \\lambda_i} \\\\\n",
    "g(z) &:= \\frac{2}{\\Delta t}\\frac{1-z}{1+z}\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c5cec",
   "metadata": {},
   "source": [
    "重要な点としては行列演算がすべてスカラーの係数 $k_{z,\\Lambda}$ に置き換えられている点です。\n",
    "この $k_{z,\\Lambda}$ は[コーシー核 (Cauchy kernel)](https://en.wikipedia.org/wiki/Cauchy_matrix) $k_{ij} = 1/(\\omega_i - \\lambda_j)$ と関連があり、$T$ 個の1の $T$ 乗根に対して実はその近似値は理論上はトータルで $O((N+T)\\log^2(N+T))$ での計算量で得られます<sup>[6]</sup>。\n",
    "ただしその実装は複雑かつ煩雑なので、ここではその近似を用いず $k_{z,\\Lambda}$ を愚直に計算する $O(NT)$ の計算量での実装の紹介にとどめます。\n",
    "興味がある方は、S4論文の筆者らによるC++とcudaによる実装が[GitHub](https://github.com/state-spaces/s4/tree/main/extensions/kernels)で公開されているので、参照して実際に組み込んでみてください。\n",
    "\n",
    "以下の実装は[S4論文の筆者らによるデモ実装](https://srush.github.io/annotated-s4/#step-2-diagonal-case)を改変したものです。\n",
    "この実装では `cauchy_kernel` 関数において $O(NT)$ の計算量が必要で、繰り返しになりますがこの部分を $\\tilde{O}(N+T)$ に改善する余地があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauchy_kernel(v, omega, lambd):\n",
    "    \"\"\"Cauchy matrix multiplication: (n), (l), (n) -> (l)\"\"\"\n",
    "    return np.sum(v / (omega[:, None] - lambd[None, :]), axis=1)\n",
    "\n",
    "\n",
    "def dplr_kernel(dt, length, Lambda, P, Q, B, C):\n",
    "    omega = np.exp((-2j * np.pi) * (np.arange(length) / length))  # Roots of unity\n",
    "    aterm = (C, Q.conj())\n",
    "    bterm = (B, P)\n",
    "    c = 2.0 / (1.0 + omega)\n",
    "    g = (2.0 / dt) * ((1.0 - omega) / (1.0 + omega))\n",
    "    k00 = cauchy_kernel(aterm[0] * bterm[0], g, Lambda)\n",
    "    k01 = cauchy_kernel(aterm[0] * bterm[1], g, Lambda)\n",
    "    k10 = cauchy_kernel(aterm[1] * bterm[0], g, Lambda)\n",
    "    k11 = cauchy_kernel(aterm[1] * bterm[1], g, Lambda)\n",
    "    at_roots = c * (k00 - k01 * (1.0 / (1.0 + k11)) * k10)\n",
    "    out = np.fft.ifft(at_roots, length).reshape(length)\n",
    "    return out.real\n",
    "\n",
    "\n",
    "dt, period = 1e-4, 2.5\n",
    "num_basis, tau, plot_num = 64, 1.0, 6\n",
    "\n",
    "C = np.eye(num_basis)[5][None, :]  # Output matrix: choose the k-th basis function\n",
    "# C = np.random.default_rng(1234).normal(0.0, 1.0, size=(1, num_basis))  # Random readout\n",
    "length = int(period / dt) + 1\n",
    "\n",
    "# Naive HiPPO-LegS\n",
    "A, B = transition_leg_s(num_basis, tau=tau)\n",
    "A_bar, B_bar = discretize(dt, A, B, method=\"bilinear\")\n",
    "ssm_basis = discrete_sequential_kernel(length, A_bar, B_bar)\n",
    "ya = C @ ssm_basis\n",
    "\n",
    "# HiPPO-LegS with DPLR and Z-transform\n",
    "An, B, P = transition_leg_s_nplr(num_basis, tau=tau)\n",
    "Lambda, V = np.linalg.eig(An)\n",
    "P_V = V.conj().T @ P\n",
    "B_V = V.conj().T @ B\n",
    "C_V = C @ V\n",
    "A_dplr = np.diag(Lambda) - P_V @ P_V.conj().T  # Construct DPLR form from NPLR.\n",
    "A_bar_dplr, _B_bar = discretize(dt, A_dplr, B_V, method=\"bilinear\")\n",
    "C_tilde = C_V @ (np.eye(num_basis) - np.linalg.matrix_power(A_bar_dplr, length))\n",
    "yb = dplr_kernel(\n",
    "    dt, length, Lambda, P_V[:, 0], P_V[:, 0], B_V[:, 0], C_tilde[0, :]\n",
    ")  # NOTE: All arguments are 1D and it will cost O(Llog(L)) except cauchy_kernel.\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, sharex=True, figsize=(6, 6), gridspec_kw={\"hspace\": 0.05})\n",
    "ax[0].plot(ya.real.T / dt, label=\"Naive\", lw=2, color=\"gray\", ls=\"--\")\n",
    "ax[0].plot(yb.real / dt, label=\"DPLR\", color=\"red\", lw=1.0)\n",
    "ax[0].legend(\n",
    "    loc=\"upper left\",\n",
    "    fontsize=12,\n",
    "    frameon=False,\n",
    "    bbox_to_anchor=(1.0, 1.0),\n",
    ")\n",
    "ax[1].plot(ssm_basis.real[:plot_num].T / dt)\n",
    "for idx in range(len(ax)):\n",
    "    ax[idx].tick_params(axis=\"both\", labelsize=12)\n",
    "\n",
    "assert np.allclose(ya.real, yb.real, atol=1e-8, rtol=1e-8)  # Check the correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8197fa",
   "metadata": {},
   "source": [
    "多少実装が難解ですが、確かにZ変換を用いて愚直に計算する方法と同じカーネルが得られました (上図)。\n",
    "また下図は 比較としてSSMの基底関数が正しく構成されているか確認するために描画されています。\n",
    "$C$ をいくつか変えてみて同様に一致するか確認してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf789e0",
   "metadata": {},
   "source": [
    "#### S4DによるSSMの単純化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f10fd",
   "metadata": {},
   "source": [
    "ここまでは、主にS4において提案された安定かつ高速にSSMを計算する方法を説明しました。\n",
    "特に対角化とNPLR表現を用いて、SSMの固有値をうまく表現すると $O(NT)$ の計算量 (更に工夫すると $\\tilde{O}(N+T)$) でのカーネルの構築が、その後FFTとIFFTを用いて $O(T\\log T)$ の計算量でSSMの出力を計算が可能になります。\n",
    "この節ではS4の後に提案された **S4D (S4 with Diagonal State Matrix)<sup>[7]</sup>** によるSSMの単純化を説明します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99594fb3",
   "metadata": {},
   "source": [
    "##### DSS と対角成分の重要性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f43bc",
   "metadata": {},
   "source": [
    "ここまで説明したとおり、S4では SSM $(A, B)$ をHiPPOに基づき構築し、さらにそのDPLR表現 $A = V(\\Lambda - P_V Q_V^{*})V^{*}$ を用いてSSMが高速に計算されます。\n",
    "S4をニューラルネットワークに組み込むとき、$\\bar{A}, \\bar{B}$ 全体をパラメータ化するかわりに、この対角行列 $\\Lambda$ と低ランク成分 $P, Q$ ならびに離散化時の時間幅 $\\Delta t$ が、学習パラメータとしてHiPPOに基づき所定の値に初期化された後に調整されます。\n",
    "このときHiPPOに基づく初期化が重要で、実際ランダムな初期化と比較して大幅に性能が向上したと報告されています<sup>[6]</sup>。\n",
    "\n",
    "一方でS4の提案の後、DPLR表現のうち低ランク成分を無視、すなわち $P=Q=0$ とし $\\Lambda$ のみを学習可能なパラメータとする DSS (Diagonal State Space model) と呼ばれる簡略化されたモデルも提案されました<sup>[13]</sup>。\n",
    "これは元のSSMに対する大胆な近似ですが、実際にDSSがS4に匹敵する性能が得られたと報告されました。\n",
    "こうした背景から、DSSをベースにさらに簡略化されたモデルであるS4Dが登場しました。\n",
    "同時になぜ対角成分に制限されたDSSがS4と同等の性能を示すのかについて理論、実験両面から考察がなされており、いくつかは実際にデモンストレーションを通して直感的に確認できます。\n",
    "以下のセルでは、HiPPO-LegSによるSSM $(A, B)$ のカーネルと、NPLR表現で得られた $A^{(n)}$ を $A$ の代わりに用いるSSM $(A^{(n)}, B/2)$ のカーネルを比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef7404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ssm_basis(A, B, dt, length, disc_method):\n",
    "    A_bar, B_bar = discretize(dt=dt, A=A, B=B, method=disc_method)\n",
    "    ssm_basis = discrete_sequential_kernel(length, A_bar, B_bar) / dt\n",
    "    return ssm_basis\n",
    "\n",
    "\n",
    "def plot_ssm_basis(ax, ssm_basis, plot_num, title=\"SSM Basis Functions\"):\n",
    "    ax.tick_params(axis=\"both\", labelsize=12)\n",
    "    ax.plot(ssm_basis[:plot_num].T, lw=1.0)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "\n",
    "\n",
    "num_basis, tau, dt, period = 1024, 1.0, 1e-3, 1.5\n",
    "plot_num = 6\n",
    "\n",
    "length = int(period / dt) + 1\n",
    "\n",
    "A_org, B_org = transition_leg_s(num_basis, tau=tau)\n",
    "ssm_basis_hippo_leg_s = calc_ssm_basis(A_org, B_org, dt, length, \"zoh\")\n",
    "An, B, _P = transition_leg_s_nplr(num_basis, tau=tau)  # P is not used here.\n",
    "ssm_basis_leg_sd_zoh = calc_ssm_basis(An, B * 0.5, dt, length, \"zoh\")\n",
    "ssm_basis_leg_sd_bil = calc_ssm_basis(An, B * 0.5, dt, length, \"bilinear\")\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(8, 8), sharex=True, gridspec_kw={\"hspace\": 0.2})\n",
    "plot_ssm_basis(ax[0], ssm_basis_hippo_leg_s, plot_num, title=\"SSM $(A, B)$\")\n",
    "plot_ssm_basis(ax[1], ssm_basis_leg_sd_zoh, plot_num, title=\"SSM $(A^{(N)}, B/2)$ (ZOH)\")\n",
    "plot_ssm_basis(ax[2], ssm_basis_leg_sd_bil, plot_num, title=\"SSM $(A^{(N)}, B/2)$ (Bilinear)\")\n",
    "fig.suptitle(\"HiPPO-LegS Basis Functions\", fontsize=14)\n",
    "fig.align_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e01f9ef",
   "metadata": {},
   "source": [
    "離散化の方法によって多少の誤差が見られますが、全体としては非常に似たカーネルが得られている様子が確認できます。\n",
    "多少時間がかかりますが `num_basis` を大きくしたり、逆に小さくして近似の度合いを確認してください。\n",
    "実際HiPPO-LegSの場合に限定されますが、HiPPO-LegSのSSM $(A, B)$ は $N\\to \\infty$ のときに SSM $(A^{(N)}, B/2)$ に収束します。\n",
    "というのも、$N \\to \\infty$ のときに入力時系列は十分に基底関数の総和で近似できるとみなせ、$u_{\\leq t}(s) = \\sum_{n=0}^{\\infty} x_n(t) p_{n}(s)$ と展開されます。\n",
    "その際 $s=0$ の時を考慮すると以下の式が成立します。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "u(t) &= \\sum_{n=0}^{N} x_n(t) K_{n}(0) \\\\\n",
    "&= \\sum_{n=0}^{N} x_n(t) \\sqrt{2n+1} \\\\\n",
    "&= \\tau B^{\\top} x(t)\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb54a9",
   "metadata": {},
   "source": [
    "また $P=Q=\\sqrt{\\tau/2}B$ なので SSM $(A, B)$ は以下の形で変形されます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d}{dt} x(t) &= A x(t) + B u(t) \\\\\n",
    "&= A x(t) + \\frac{1}{2} B u(t) + \\frac{1}{2} B u(t) \\\\\n",
    "&= A x(t) + \\frac{\\tau}{2} B B^{\\top} x(t) + \\frac{1}{2} B u(t) \\\\\n",
    "&= A x(t) + P P^{\\top} x(t) + \\frac{1}{2} B u(t) \\\\\n",
    "&= A^{(N)}x(t) + \\frac{1}{2} B u(t)\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ceb9d",
   "metadata": {},
   "source": [
    "このように十分に $N$ が大きい際に低ランク成分を無視しても、元のSSMと同様の入力時系列に対する応答が得られると考えられます。\n",
    "\n",
    "同様に、HiPPO-FouTについても確認してみましょう。\n",
    "同じく $P=Q=\\sqrt{\\theta/2}B$ で NPLR表現が得られます。\n",
    "このとき SSM $(A, B)$ と SSM $(A^{(N)}, B/2)$ のカーネルは以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2274905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_fou_t_nplr(N, theta=1.0):\n",
    "    A, B = transition_fou_t(N, theta=theta)\n",
    "    P = B * (0.5 * theta) ** 0.5\n",
    "    An = A + P @ P.T\n",
    "    return An, B, P\n",
    "\n",
    "\n",
    "def calc_ssm_basis(A, B, dt, length, disc_method):\n",
    "    A_bar, B_bar = discretize(dt=dt, A=A, B=B, method=disc_method)\n",
    "    ssm_basis = discrete_sequential_kernel(length, A_bar, B_bar) / dt\n",
    "    return ssm_basis\n",
    "\n",
    "\n",
    "def plot_ssm_basis(ax, ssm_basis, plot_num, title=\"SSM Basis Functions\"):\n",
    "    ax.tick_params(axis=\"both\", labelsize=12)\n",
    "    ax.plot(ssm_basis[:plot_num].T, lw=1.0)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "\n",
    "\n",
    "num_basis, theta, dt, period = 512, 1, 1e-3, 2\n",
    "plot_num = 6\n",
    "\n",
    "length = int(period / dt) + 1\n",
    "\n",
    "A_org, B_org = transition_fou_t(num_basis, theta=theta)\n",
    "ssm_basis_hippo_fou_t = calc_ssm_basis(A_org, B_org, dt, length, \"bilinear\")\n",
    "An, B, _P = transition_fou_t_nplr(num_basis, theta=theta)  # P is not used here\n",
    "ssm_basis_fou_td_bil = calc_ssm_basis(An, B * 0.5, dt, length, \"bilinear\")\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 5), sharex=True, gridspec_kw={\"hspace\": 0.2})\n",
    "plot_ssm_basis(ax[0], ssm_basis_hippo_fou_t, plot_num, title=\"SSM $(A, B)$\")\n",
    "plot_ssm_basis(ax[1], ssm_basis_fou_td_bil, plot_num, title=\"SSM $(A^{(N)}, B/2)$ (Bilinear)\")\n",
    "fig.suptitle(\"HiPPO-LegS Basis Functions\", fontsize=14)\n",
    "fig.align_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1979b70",
   "metadata": {},
   "source": [
    "$[0, \\theta]$ の範囲はうまく近似されている様子が見られます。\n",
    "HiPPO-FouTのNPLR表現における $A^{(N)}$ は $2\\times 2$ の行列 $[0, 2\\pi n /\\theta; -2 \\pi n /\\theta, 0]$ が対角成分に並んだブロック行列なので、その固有値は純虚数で $\\pm i 2\\pi n /\\theta$ ($n=0,1,~\\ldots,~N/2-1$) になります。\n",
    "したがってこの図に示されるとおり、 SSM $(A^{(N)}, B/2)$ では各周波数に対する振動解が得られます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e1780",
   "metadata": {},
   "source": [
    "##### S4D-InvとS4D-Lin による近似"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1eb170",
   "metadata": {},
   "source": [
    "このように $A^{(N)}$ 単体のみに着目して大胆に近似しても、HiPPO-LegSやHiPPO-FouTのSSMのカーネルに近いダイナミクスが得られました。\n",
    "$A^{(N)}$ は正規行列であるため対角化しても安定化して計算でき、また $P=Q=0$ として実装が非常に簡単になります。\n",
    "このS4におけるDPLR表現で低ランク成分を無視し、かつ対角化して　$A^{(N)}$ の固有値のみ加味するSSMを用いたモデルが S4D です。\n",
    "それぞれを今 HiPPO-LegSやHiPPO-FouTに対応するS4Dを今S4D-LegS, S4D-FouT と呼びましょう。\n",
    "ZOHで離散化した場合の $\\bar{A} = \\exp(A^{(N)} \\Delta t)$ の固有値を描画してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_basis, dt = 64, 1e-2\n",
    "An_leg_s, *_ = transition_leg_s_nplr(num_basis)\n",
    "An_fou_t, *_ = transition_fou_t_nplr(num_basis)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "plot_eigen(ax[0], scipy.linalg.expm(An_leg_s * dt))\n",
    "plot_eigen(ax[1], scipy.linalg.expm(An_fou_t * dt))\n",
    "ax[0].set_title(f\"HiPPO-LegS ($A^{{(N)}},\\\\cdot$)\\n{ax[0].get_title()}\", fontsize=14)\n",
    "ax[1].set_title(f\"HiPPO-FouT ($A^{{(N)}},\\\\cdot$)\\n{ax[1].get_title()}\", fontsize=14)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af72b6d3",
   "metadata": {},
   "source": [
    "これまで見たLESN $(\\bar{A}, \\bar{B})$ のときとは異なり、いずれも実軸対称かつ原点中心の円周上にほぼ分布しています。\n",
    "これは $A^{(N)}$ の固有値、すなわち $\\Lambda$ の性質に起因しています。\n",
    "\n",
    "HiPPO-LegSのとき $A^{(N)}$ は、実歪行列と単位行列をそれぞれ $S, I$ として $A^{(N)} = \\frac{1}{2\\tau}(I + S)$ と表されます。\n",
    "このとき固有値の実部 $\\mathrm{Re}(\\lambda)$ は $\\frac{1}{2\\tau}$ となります。\n",
    "また虚部 $\\mathrm{Im}(\\lambda)$ は $S$ の固有値に一致します ( $S$ の固有値は純虚数)。\n",
    "$N$ が十分に大きくかつ偶数のとき、この固有値は次の式で与えられる S4D-Inv とその共役で近似できます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{1}{\\tau}\\left(-\\frac{1}{2} + i \\frac{N}{\\pi} \\left(\\frac{N}{2n+1} - 1\\right)\\right) \\tag{S4D-Inv}\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3992e084",
   "metadata": {},
   "source": [
    "ただし $n=0,1,~\\ldots,~N/2 - 1$ とします。\n",
    "\n",
    "HiPPO-FouTの $A^{(N)}$ の固有値は先ほど説明したとおり $\\pm i 2\\pi n/\\theta$ ( $n=0,1,~\\ldots,~N/2 - 1$ ) になります。\n",
    "S4D-Invにならい $-1/2$ を加え、共役成分を無視し、簡略化したものが以下の S4D-Lin と呼ばれるものです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f7e96",
   "metadata": {},
   "source": [
    "$-\\frac{1}{2}$ を加えたのはスペクトル半径を1以下にし、徐々に減衰するようにするためです。\n",
    "実際$\\exp(a+bi)$ は複素平面上で半径 $\\exp(a)$ の円周上の 偏角 $b$ の点を表すので、先程の図に示されるようにZOHで計算された $\\bar{A}$ の固有値は、半径 $\\exp(\\mathrm{Re}(\\lambda) \\Delta t)$ の円周上に分布します。\n",
    "$\\Delta t$ として今、$1$ より非常に小さい値を想定しているので、そのスペクトル半径は $1$ にほぼ近いものの $1$ より小さい値になります。\n",
    "\n",
    "実際にHiPPO-LegSとHiPPO-FouTの固有値の虚部が、それぞれここで定義されたS4D-InvとS4D-Linでどの程度近似できるか確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b657aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_s4d_inv(N, tau=1.0):\n",
    "    arange = np.arange(N // 2)\n",
    "    imag = N / np.pi * (N / (2 * arange + 1) - 1)\n",
    "    return (-0.5 + 1j * imag) / tau\n",
    "\n",
    "\n",
    "def lambda_s4d_lin(N, theta=1.0):\n",
    "    arange = np.arange(N // 2)\n",
    "    imag = arange * np.pi\n",
    "    return (-0.5 + 1j * imag) / theta\n",
    "\n",
    "\n",
    "num_basis = 128\n",
    "\n",
    "An_leg_s, *_ = transition_leg_s_nplr(num_basis)\n",
    "An_fou_t, *_ = transition_fou_t_nplr(num_basis)\n",
    "An_fou_t = An_fou_t * 0.5  # Scale to match the imaginary parts.\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "for idx, (An, approx_func, name_true, name_approx) in enumerate(\n",
    "    zip([An_leg_s, An_fou_t], [lambda_s4d_inv, lambda_s4d_lin], [\"LegS\", \"FouT\"], [\"S4D-Inv\", \"S4D-Lin\"], strict=True)\n",
    "):\n",
    "    eig_org = np.linalg.eigvals(An)[::2].imag\n",
    "    eig_app = approx_func(num_basis).imag\n",
    "    eig_org = np.abs(eig_org)\n",
    "    eig_app = np.abs(eig_app)\n",
    "    ax[idx].plot(np.sort(eig_org), label=f\"True ({name_true})\")\n",
    "    ax[idx].plot(np.sort(eig_app), label=f\"Approx. ({name_approx})\")\n",
    "    ax[idx].set_yscale(\"log\")\n",
    "    ax[idx].tick_params(axis=\"both\", labelsize=12)\n",
    "    ax[idx].legend(\n",
    "        loc=\"lower right\",\n",
    "        fontsize=12,\n",
    "        frameon=False,\n",
    "        bbox_to_anchor=(1.0, 0.0),\n",
    "    )\n",
    "    ax[idx].set_title(f\"{name_true} vs {name_approx}\", fontsize=14)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60f433",
   "metadata": {},
   "source": [
    "いずれのケースもかなりよく一致しています。\n",
    "またこのとき生成されるカーネルと固有値の複素数平面上での分布も確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6343ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_basis, theta, dt, period = 64, 1, 1e-2, 3\n",
    "plot_num = 6\n",
    "length = int(period / dt) + 1\n",
    "\n",
    "lambda_inv = lambda_s4d_inv(num_basis)[::-1]\n",
    "ssm_basis_s4d_inv = calc_ssm_basis(np.diag(lambda_inv), np.ones((num_basis // 2, 1)), dt, length, \"zoh\")\n",
    "lambda_lin = lambda_s4d_lin(num_basis)\n",
    "ssm_basis_s4d_lin = calc_ssm_basis(np.diag(lambda_lin), np.ones((num_basis // 2, 1)), dt, length, \"zoh\")\n",
    "\n",
    "envelop = np.exp(-0.5 * np.linspace(0, period, length))\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 5), sharex=True, gridspec_kw={\"hspace\": 0.2})\n",
    "plot_ssm_basis(ax[0], ssm_basis_s4d_inv[::2].real, plot_num, title=\"S4D-Inv (SSM $(\\\\Lambda, 1)$)\")\n",
    "plot_ssm_basis(ax[1], ssm_basis_s4d_lin[::2].real, plot_num, title=\"S4D-Lin (SSM $(\\\\Lambda, 1)$)\")\n",
    "for idx in range(len(ax)):\n",
    "    ax[idx].plot(envelop, color=\"gray\", lw=2.0, ls=\"--\")\n",
    "    ax[idx].plot(-envelop, color=\"gray\", lw=2.0, ls=\"--\")\n",
    "fig.align_labels()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "plot_eigen(ax[0], scipy.linalg.expm(np.diag(lambda_inv) * dt))\n",
    "plot_eigen(ax[1], scipy.linalg.expm(np.diag(lambda_lin) * dt))\n",
    "ax[0].set_title(f\"S4D-Inv ($\\\\Lambda,\\\\cdot$)\\n{ax[0].get_title()}\", fontsize=14)\n",
    "ax[1].set_title(f\"S4D-Lin ($\\\\Lambda,\\\\cdot$)\\n{ax[1].get_title()}\", fontsize=14)\n",
    "fig.align_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59147333",
   "metadata": {},
   "source": [
    "いずれも包絡線 $\\exp(-k\\Delta t/2)$ によって減衰する様々な時定数の振動解が得られています。\n",
    "虚部の性質からS4D-Linに比べてS4D-Invのほうが低周波数成分に集中しています。\n",
    "これは複素平面上で $\\Lambda$ が第一象限に集中している点と対応しています。\n",
    "また $-1/2$ をいずれも加えられているので、これらの点が半径 $\\exp(-\\Delta t / 2) < 1$ の円周上に分布している様子が確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489cff32",
   "metadata": {},
   "source": [
    "### SSMが組み込まれたニューラルネットワークの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8624e91b",
   "metadata": {},
   "source": [
    "**注意:** 以下の内容は実際にPytorchを用いてニューラルネットワークを実装しそのパラメータを学習します。\n",
    "GPUを使える環境にある場合は**GPUの使用を強く推奨します** (手元にない場合はGoogle Colaboratory上での実行をおすすめします)。\n",
    "その際は以下のガイドを参考に追加の設定を行ってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05cb74a",
   "metadata": {},
   "source": [
    "<details><summary>GPU環境で計算する場合の下準備</summary>\n",
    "\n",
    "`pytorch` がとても便利ですので、以下その導入方法を説明します。\n",
    "\n",
    "1. オンライン環境 (Google Colaboratory) の場合\n",
    "\n",
    "    無料版でもデフォルトでGPUを使用できる他、`pytorch` がすでにインストールされているので特に追加の設定をする必要はないですが、以下の手順でGPUが有効か確認できます。\n",
    "    「編集」 > 「ノートブックの設定」 > 「ハードウェア アクセラレータ」 > 「GPU」\n",
    "\n",
    "2. ローカル環境の場合 (uvの場合)\n",
    "\n",
    "    NVIDIA製のGPUの場合ドライバーをまずインストールしてください。\n",
    "    インストールされているかどうかは、`nvidia-smi` コマンドで確認できます。\n",
    "    インストールされていない場合は、公式の[配布ページ](https://www.nvidia.com/en-us/drivers/)からダウンロードできます。\n",
    "    あとは以下のコマンドでインストールできます。\n",
    "    ```bash\n",
    "    uv sync --extra gpu\n",
    "    ```\n",
    "    自動的に`pytorch` のインストールが開始されます。\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59352458",
   "metadata": {},
   "source": [
    "##### マルチヘッド多層SSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936df7cc",
   "metadata": {},
   "source": [
    "ここでここまで扱ったSSMがどのようにニューラルネットワークに組み込まれるかを説明します。\n",
    "今 $T$ ステップの入力時系列 $u[k]$ に対して、　各層に $N$ 次元のSSMが組み込まれた、全 $L(>0)$ 層のニューラルネットワークを考えます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x^{(l)}[k+1] &= \\bar{A}^{(l)} x^{(l)}[k] + \\bar{B}^{(l)} u^{(l)}[k] \\\\\n",
    "y^{(l)}[k] &= C^{(l)} x^{(l)}[k] + D^{(l)} u^{(l)}[k] \\\\\n",
    "u^{(l+1)}[k] &= \\phi\\left(y^{(l)}[k]\\right) + u^{(l)}[k]\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17cfbe",
   "metadata": {},
   "source": [
    "ここで $l=0,1,~\\ldots,~L-1$ は層の番号、$x^{(l)}[k] \\in \\mathbb{R}^{N}$ は $l$ 層目のSSMの内部状態、$y^{(l)}[k] \\in \\mathbb{R}$ は $l$ 層目のSSMの出力、$\\phi:\\mathbb{R} \\to \\mathbb{R}$ は活性化関数 (例えば $\\tanh$ やReLU) です。\n",
    "以下特に注記がなければ $\\phi$ はelement-wiseに作用します。\n",
    "また $u^{0}[k]:=u[k]$ とします。\n",
    "$u^{(l+1)}[k]$ の項に加えられる残差接続は勾配の消失を防ぐため与えられます。\n",
    "これらを示したのが図1(a)になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e18e9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; width: 750px; margin: auto; background-color: #f8f9fa; padding: 10px; border-radius: 10px;\">\n",
    "\n",
    "<img src=\"data:image/webp;base64,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\" alt=\"fig_11_1.webp\">",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aef9e0",
   "metadata": {},
   "source": [
    "<figcaption align = \"center\">\n",
    "\n",
    "図1 多層SSMの構造。\n",
    "(左) SSMが直列に接続された多層SSM。 Decoder部分は通常線形層で構成される。\n",
    "(右) 各層で複数のSSMが並列に接続されたマルチヘッド多層SSM。\n",
    "\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e8baa",
   "metadata": {},
   "source": [
    "実際には図1(b)に示されるように、各層に $H(>0)$ 個のSSMを並列に配置し、その出力を足し合わせる形で次の層への入力 $u^{(l+1,h)}[k]$ を計算します。\n",
    "これ自体はTransformerにおけるマルチヘッドアテンションに似た構成で、時定数のパラメータを変化させると多様な時系列を効率よく表現できます。\n",
    "各層に $H$ 個のSSMがある状況を改めて式で表すと以下のようになります。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x^{(l,h)}[k+1] &= \\bar{A}^{(l,h)} x^{(l,h)}[k] + \\bar{B}^{(l,h)} u^{(l,h)}[k] \\\\\n",
    "y^{(l,h)}[k] &= C^{(l,h)} x^{(l,h)}[k] + D^{(l,h)} u^{(l,h)}[k] \\\\\n",
    "u^{(l + 1,h)}[k] &= \\phi\\left(\\sum_{o=0}^{H-1} W^{(l,h)}_{o} \\phi\\left(y^{(l,o)}[k]\\right)\\right) + u^{(l,h)}[k]\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c28e0a",
   "metadata": {},
   "source": [
    "添字 $\\cdot^{(l,h)}$ は $l$ 層目の $h$ 番目のSSMに対応し、また $u^{(0,h)}[k]:=u[k]$ とします。\n",
    "$W^{(l,h)} \\in \\mathbb{R}^{H}$ によって各SSMの出力が足し合わされ、次の層への入力が計算されます。\n",
    "時系列 $u[k]$ 全体に対して分類や回帰タスクを行う場合は、全 $H$ 個の SSM $\\{u^{(L,h)}[T-1]\\}_{h=0}^{H-1}$ 、あるいは全時間の平均値である $\\{\\frac{1}{T}\\sum_{k=0}^{T-1} u^{(L,h)}[k]\\}_{h=0}^{H-1}$ に対して、典型的には線形層を通して最終的な出力を計算します。\n",
    "生成や予測タスクに対しては各時刻 $k$ を用いて、同様に何かしらのモデル(こちらも典型的には線形層)を通して出力を計算します。\n",
    "\n",
    "上記ネットワークは多層SSMで、途中に非線形な活性化関数を挟んでいるので誤差逆伝搬法を用いて学習を行います。\n",
    "この際、SSMの各パラメータ $\\bar{A}^{(l,h)}, \\bar{B}^{(l,h)}, C^{(l,h)}, D^{(l,h)}, W^{(l,h)}$ はすべて学習可能なパラメータとして扱います (リザバーとして扱う場合は $\\bar{A}$ を固定する状況に相当します)。\n",
    "どのSSMでも同じ構造を共有するので以下添字を再び省略して表記します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c807ce",
   "metadata": {},
   "source": [
    "##### パラメータの設定と初期化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc2c1d",
   "metadata": {},
   "source": [
    "これまで説明されたとおり、SSM $(A, B)$ の計算は (i) 離散化、 (ii) カーネルの計算、 (iii) 畳み込みの3段階で行われます。\n",
    "特にS4Dの節で説明されたように今、NPLR表現 $A=V(\\Lambda - P_V Q_V^{*})V^{*}$ のうち $P=Q=0$ として無視し $V=I$ とした上でZOHで離散化すると (i) と (ii) の計算は大幅に簡略化されます。\n",
    "このとき長さ $T$ のカーネル $K^{(T)}$ は、Vandermonde行列 $\\mathcal{V}^{(T)}(\\cdot) \\in \\mathbb{R}^{N \\times T}$ を用いて以下の式で計算されます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bar{K}^{(T)} &= \\left[C\\bar{B}, C\\bar{A}\\bar{B}, C\\bar{A}^2\\bar{B},~\\ldots,~C\\bar{A}^{T-1}\\bar{B}\\right]_{n} \\\\\n",
    "&= \\left(C \\odot \\bar{B}^\\top \\right) \\mathcal{V}^{(T)}(\\mathrm{diag}(\\bar{\\Lambda})) \\\\\n",
    "&= \\left(C \\odot \\left(\\Lambda^{-1}\\left(I - \\exp(\\Delta t \\Lambda)\\right) B\\right)^\\top\\right)\n",
    "\\mathcal{V}^{(T)}(\\mathrm{diag}(\\exp(\\Delta t\\Lambda)))\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e33bbc",
   "metadata": {},
   "source": [
    "$B, C$はいずれも線形変換であり、したがって $B=1$ として上式の値を変化させないある $C^{'} \\in \\mathbb{C}^{1 \\times N}$ が存在します。\n",
    "したがって新たにそれを $C$ として定義し直すと、上式は以下の形でさらに簡略化されます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bar{K}^{(T)} &=\n",
    "\\left(C_n \\frac{1-\\exp(\\Delta t\\lambda_n)}{\\lambda_n}\\right)_{n}^\\top\n",
    "\\left(\\exp(\\Delta t \\lambda_n)^k\\right)_{k,n}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e039a2ab",
   "metadata": {},
   "source": [
    "言い換えれば 出力行列 $C$ に全ての変換が集約された形になります。\n",
    "こうした変形が可能なのがSSM、あるいはLESNの強みと言えるでしょう。\n",
    "またこれまで $\\theta, \\tau$ などの時定数を調整するパラメータが用意されましたが、いずれも固有値 $\\Lambda$ を定数倍するパラメータなので、$\\Delta t$ を調整して $\\theta, \\tau$ の効果を表現できます。\n",
    "まとめると学習パラメータは $\\Delta t, \\Lambda, C, D, W$ で、とくにSSM部分を固定化する場合は $\\Delta t, \\Lambda$ を固定し、 $C, D, W$ のみを学習可能なパラメータとします。\n",
    "\n",
    "まず(i)と(ii)をまとめて計算してカーネルを構築する `S4DKernel` を実装しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "default_type = \"float32\"\n",
    "\n",
    "\n",
    "class S4DKernel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_ssm,\n",
    "        num_basis,\n",
    "        rnd=None,\n",
    "        dtype=default_type,\n",
    "        dt_min=0.001,\n",
    "        dt_max=0.1,\n",
    "        basis_type=\"s4d_inv\",\n",
    "        tunable_dt=False,\n",
    "        tunable_basis=False,\n",
    "    ):\n",
    "        super(S4DKernel, self).__init__()\n",
    "        assert num_basis % 2 == 0, \"N must be even\"\n",
    "        assert basis_type in [\"s4d_inv\", \"s4d_lin\"], 'basis_type must be \"s4d_inv\" or \"s4d_lin\"'\n",
    "        rnd = np.random.default_rng() if rnd is None else rnd\n",
    "\n",
    "        # Complex output matrix\n",
    "        C = rnd.normal(0, 1, size=(num_ssm, num_basis // 2, 2)).astype(dtype)  # (H, N/2, 2)\n",
    "        self.register_parameter(\"C\", nn.Parameter(torch.view_as_complex(torch.from_numpy(C))))\n",
    "\n",
    "        # Logarithmically spaced time scales\n",
    "        log_dt = rnd.uniform(np.log10(dt_min), np.log10(dt_max), size=(num_ssm,)).astype(dtype)\n",
    "        log_dt = torch.from_numpy(log_dt)\n",
    "        if tunable_dt:\n",
    "            self.register_parameter(\"log_dt\", nn.Parameter(log_dt))\n",
    "        else:\n",
    "            self.register_buffer(\"dt\", nn.Parameter(10**log_dt))\n",
    "\n",
    "        # Eigenvalues\n",
    "        A_real = np.full(num_basis // 2, 0.5, dtype=dtype)\n",
    "        if basis_type == \"s4d_inv\":\n",
    "            A_imag = num_basis / np.pi * (num_basis / (2 * np.arange(num_basis // 2, dtype=dtype) + 1) - 1)\n",
    "        elif basis_type == \"s4d_lin\":\n",
    "            A_imag = np.pi * np.arange(num_basis // 2, dtype=dtype)\n",
    "        if tunable_basis:\n",
    "            self.register_parameter(\"log_A_real\", nn.Parameter(torch.from_numpy(np.log(A_real))))\n",
    "            # self.register_buffer(\"log_A_real\", torch.from_numpy(np.log(A_real)))  # for fixed real part\n",
    "            self.register_parameter(\"A_imag\", nn.Parameter(torch.from_numpy(A_imag)))\n",
    "        else:\n",
    "            self.register_buffer(\"A\", torch.from_numpy(A_real + 1j * A_imag))\n",
    "\n",
    "    @property\n",
    "    def dt(self):\n",
    "        return 10**self.log_dt\n",
    "\n",
    "    @property\n",
    "    def A(self):\n",
    "        return -torch.exp(self.log_A_real) + 1j * self.A_imag\n",
    "\n",
    "    @property\n",
    "    def eigs(self):\n",
    "        return torch.exp(self.A * self.dt[:, None])\n",
    "\n",
    "    def forward(self, time_steps):\n",
    "        dt = self.dt  # (H,)\n",
    "        A = self.A  # (N/2,)\n",
    "        C = self.C  # (H, N/2)\n",
    "\n",
    "        # Vandermonde multiplication\n",
    "        dtA = A * dt[:, None]  # (H, N/2)\n",
    "        C = (2 * C) * (torch.exp(dtA) - 1.0) / A  # (H, N/2)\n",
    "        K = dtA[..., None] * torch.arange(time_steps, device=A.device)  # (H, N/2, T)\n",
    "        K = torch.einsum(\"hn, hnt -> ht\", C, torch.exp(K)).real\n",
    "        return K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7564b2b8",
   "metadata": {},
   "source": [
    "この `S4DKernel` は $\\Delta t, \\Lambda, C$ を学習可能なパラメータとして持ち、 `forward` メソッド (pytorchの `nn.Module` では `__call__`の呼び出しを通して `forward` が呼ばれます) でカーネルを計算します。\n",
    "デフォルトでは $\\Delta t$ ならびに $\\Lambda$ は固定化され、$C$ のみが学習可能なパラメータとして扱われます。\n",
    "\n",
    "まず時間幅 $\\Delta t$ は `dt` として保持されています。\n",
    "`num_ssm`個のSSMが用意されるわけですが、そのダイナミクスの多様性はランダムにサンプルされた $\\Delta t$ によって担保しています。\n",
    "具体的には `dt_min, dt_max` の範囲で対数スケールで一様にサンプリングされています。\n",
    "一方で学習パラメータとする場合 (`tunable_dt=True`を指定) は `log_dt` として保持し、`exp`を作用させる方法で正の値に制限しています。\n",
    "これは先程説明したとおり、LESNのスペクトル半径が $\\exp(\\mathrm{max}(\\mathrm{Re}(\\lambda_m)) \\Delta t)$ となるため、スペクトル半径を1以下に保ち発散を防ぐための工夫です。\n",
    "\n",
    "固有値 $\\Lambda$ は、`A`または`log_A_real, A_imag` の両方で実現されています。\n",
    "今回実軸対称な固有値のみを考えるので、虚部が正のもののみを計算し、2倍して実部を取れば `num_basis` 次元のSSMの計算と等価な計算ができます。\n",
    "これが固有値の生成時の引数において `num_basis // 2` が用いられ、かつ `C = (2 * C) * ...` の箇所で2倍されている理由です。\n",
    "また学習パラメータとする場合 (`tunable_A=True` を指定)では、固有値が実部と虚部に分けられています。\n",
    "特に`-torch.exp(self.log_A_real)` で実部を負の値に制限していますが、これも同様にスペクトル半径を1以下に保ち、SSMのダイナミクスを安定化させる効果があります。\n",
    "\n",
    "続けて(iii) の畳み込みを計算する `S4DLayer` を実装しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4DLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_ssm,\n",
    "        num_basis,\n",
    "        rnd=None,\n",
    "        dtype=default_type,\n",
    "        dropout=0.0,\n",
    "        activation_cls=nn.GELU,\n",
    "        kernel_cls=S4DKernel,\n",
    "        **kernel_args,\n",
    "    ):\n",
    "        super(S4DLayer, self).__init__()\n",
    "        rnd = np.random.default_rng() if rnd is None else rnd\n",
    "        self.D = nn.Parameter(torch.from_numpy(rnd.normal(0.0, 1.0, size=(num_ssm,)).astype(dtype)))  # (H,)\n",
    "\n",
    "        # SSM Kernel\n",
    "        self.kernel = kernel_cls(num_ssm, num_basis, rnd=rnd, dtype=dtype, **kernel_args)\n",
    "\n",
    "        # Position-wise output transform to mix features\n",
    "        self.output_linear = nn.Sequential(\n",
    "            activation_cls(),\n",
    "            nn.Dropout(dropout) if dropout > 0.0 else nn.Identity(),\n",
    "            nn.Conv1d(num_ssm, num_ssm, kernel_size=1),\n",
    "            activation_cls(),\n",
    "        )\n",
    "\n",
    "    def forward(self, u):\n",
    "        # Compute SSM Kernel\n",
    "        time_steps = u.size(-1)\n",
    "        k = self.kernel(time_steps)  # (H, T)\n",
    "\n",
    "        # Convolution\n",
    "        k_f = torch.fft.rfft(k, n=2 * time_steps)  # (H, T)\n",
    "        u_f = torch.fft.rfft(u, n=2 * time_steps)  # (B, H, T)\n",
    "        y = torch.fft.irfft(u_f * k_f, n=2 * time_steps)[..., :time_steps]  # (B, H, T)\n",
    "\n",
    "        # Compute D term in state space equation - essentially a skip connection.\n",
    "        y = y + u * self.D[..., None]  # (B, H, T)\n",
    "        y = self.output_linear(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f67d518",
   "metadata": {},
   "source": [
    "`S4DLayer` では $D$ と $W$ が学習可能なパラメータとして保持されます。\n",
    "$W$ の実装のため `nn.Conv1D` を用いていますがこれは実質的に線形変換と等価です。\n",
    "すなわち $H$ 個のSSMの出力を混ぜ合わせ、新たに $H$ 個の出力を生成する線形変換を効率的に計算します。\n",
    "そして、そのパラメータは $H \\times H$ の行列として表現されます。\n",
    "\n",
    "この `S4DLayer` を並べて多層SSMを実現する `DeepSSM` を実装しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd67282",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSSM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        output_dim,\n",
    "        num_layer=1,\n",
    "        num_ssm=1,\n",
    "        num_basis=64,\n",
    "        dropout=0.0,\n",
    "        prenorm=False,\n",
    "        average_pool=True,\n",
    "        **s4d_layer_kwargs,\n",
    "    ):\n",
    "        super(DeepSSM, self).__init__()\n",
    "        self.prenorm = prenorm\n",
    "        self.average_pool = average_pool\n",
    "\n",
    "        # Linear encoder (input_dim = 1 for grayscale and 3 for RGB)\n",
    "        self.encoder = nn.Linear(input_dim, num_ssm)\n",
    "\n",
    "        # Stack S4 layers as residual blocks\n",
    "        self.s4_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        for _ in range(num_layer):\n",
    "            self.norms.append(nn.LayerNorm(num_ssm))\n",
    "            self.s4_layers.append(S4DLayer(num_ssm, num_basis, dropout=dropout, **s4d_layer_kwargs))\n",
    "            self.dropouts.append(nn.Dropout(dropout) if dropout > 0.0 else nn.Identity())\n",
    "\n",
    "        # Linear decoder\n",
    "        self.decoder = nn.Linear(num_ssm, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)  # (B, T, input_dim) -> (B, T, H)\n",
    "        x = x.transpose(-1, -2)  # (B, T, H) -> (B, H, T)\n",
    "        for norm, layer, dropout in zip(self.norms, self.s4_layers, self.dropouts, strict=True):\n",
    "            # Each iteration will map (B, H, T) -> (B, H, T)\n",
    "            z = x\n",
    "            if self.prenorm:\n",
    "                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
    "            z = layer(z)  # Apply S4 block\n",
    "            z = dropout(z)  # Dropout on the output of the S4 block\n",
    "            x = z + x  # Residual connection\n",
    "            if not self.prenorm:\n",
    "                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
    "        x = x.transpose(-1, -2)  # (B, H, T) -> (B, T, H)\n",
    "\n",
    "        if self.average_pool:\n",
    "            # Average pooling over the sequence length\n",
    "            out = x.mean(dim=1)  # (B, T, H) -> (B, H)\n",
    "        else:\n",
    "            # Taking the last output\n",
    "            out = x[:, -1, :]  # (B, T, H) -> (B, H)\n",
    "\n",
    "        # Decode the outputs\n",
    "        out = self.decoder(out)  # (B, H) -> (B, output_dim)\n",
    "\n",
    "        # Return the output (B, output_dim)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebc5694",
   "metadata": {},
   "source": [
    "非常にシンプルで`input_dim` 次元の入力を `num_layers` 層のSSMに通し、最終的に `output_dim` 次元の出力を生成します。\n",
    "`self.decoder` は `nn.Linear` で実装される線形変換で、最終層の全 $H$ 個のSSMの出力から`output_dim` 次元の値を生成します。\n",
    "`average_pool=True` のときは全時間の平均値 $\\frac{1}{T} \\sum_{k=0}^{T-1} u^{(L, m)}[k]$ を、そうでないときは各時刻 $T-1$ の最終層の状態 $u^{(L, m)}[T-1]$ に対して線形変換を通して出力を計算します。\n",
    "デフォルトでは `average_pool=False`、すなわち後者が指定されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b44a60",
   "metadata": {},
   "source": [
    "#### Permuted MNIST の学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee2c22b",
   "metadata": {},
   "source": [
    "##### データセットと学習のコードの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1e441",
   "metadata": {},
   "source": [
    "ここまででニューラルネットワークのクラスを構築できたので、まずは $28 \\times 28$ サイズの手書き画像を入力に10個の数字を分類するタスクであるMNIST<sup>[14]</sup>を学習してみましょう。\n",
    "MNISTの学習は簡単で単純な全結合型のニューラルネットワークでも98~99%の精度が得られるので、ここではMNISTを難しくした Permuted MNISTを用います。\n",
    "Permuted MNISTでは各画像のピクセルが784次元の時系列として扱われ、さらにその784次元の時系列は時間方向にランダムにシャッフルされてから入力として与えられます。\n",
    "この処理によりPermuted MNISTは、長期記憶が必要なより難しいタスクに変化します (ちなみに単に784次元の時系列に置き換えたものはSequence MNISTと呼ばれます。難易度はPermuted MNISTより少し低いですが依然として長期記憶が必要なタスクです)。\n",
    "\n",
    "まずはデータセットを準備・管理する関数 `load_dataset` と、学習・評価を行う関数 `run_train_and_test`、ならびに描画を管理するクラス`ProgressCallback`を準備しましょう。\n",
    "コード自体は[標準的なPyTorchによる実装](https://docs.pytorch.org/tutorials/beginner/basics/intro.html)とほぼ同じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5335e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from IPython.display import clear_output, display\n",
    "from ipywidgets import Output\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def load_dataset(dataset_type: str) -> datasets.VisionDataset:\n",
    "    def dataset__init__(self, *args, preprocess=True, **kwargs):\n",
    "        super(type(self), self).__init__(*args, **kwargs)\n",
    "        self.preprocess = preprocess\n",
    "        if self.preprocess:\n",
    "            if self.transform is not None:\n",
    "                self.data = self.transform(self.data)\n",
    "            if self.target_transform is not None:\n",
    "                self.targets = self.target_transform(self.targets)\n",
    "\n",
    "    def dataset__getitem__(self, index: int | slice) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int | slice): Index or slice object.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        if not self.preprocess:\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "        return img, target\n",
    "\n",
    "    dataset_cls = getattr(datasets, dataset_type)\n",
    "    assert issubclass(dataset_cls, datasets.VisionDataset)\n",
    "    dataset_cls = type(\n",
    "        dataset_cls.__name__,\n",
    "        (dataset_cls,),\n",
    "        {\"__init__\": dataset__init__, \"__getitem__\": dataset__getitem__},\n",
    "    )\n",
    "    return dataset_cls\n",
    "\n",
    "\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    loss_all = []\n",
    "    pbar = tqdm(loader, leave=False)\n",
    "    for image, target in pbar:\n",
    "        output = model(image)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        loss_all.append(loss.item())\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix(loss=loss_all[-1])\n",
    "    return loss_all\n",
    "\n",
    "\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, leave=False)\n",
    "        for image, target in pbar:\n",
    "            output = model(image)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def run_train_and_test(model, train_loader, test_loader, optimizer, criterion, num_epochs=20, callback=None):\n",
    "    best_acc = 0.0\n",
    "    results = dict()\n",
    "    pbar = trange(1, num_epochs + 1)\n",
    "    for epoch in pbar:\n",
    "        loss_all = train(model, train_loader, optimizer, criterion)\n",
    "        acc = test(model, test_loader)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "        for name, value in dict(loss=loss_all, acc=acc).items():\n",
    "            value = np.asarray(value)\n",
    "            if name not in results:\n",
    "                results[name] = np.zeros((num_epochs, *value.shape), dtype=value.dtype)\n",
    "                results[name][:] = np.nan\n",
    "            results[name][epoch - 1] = value\n",
    "        pbar.set_postfix(acc=acc, best_acc=best_acc)\n",
    "        if callback is not None:\n",
    "            callback(epoch, results)\n",
    "    return results\n",
    "\n",
    "\n",
    "class ProgressCallback:\n",
    "    def __init__(self):\n",
    "        self.out = Output()\n",
    "        with self.out:\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(16, 5), gridspec_kw={\"wspace\": 0.2})\n",
    "            ax0, ax1 = ax\n",
    "            ax0.set_title(\"Training loss\", fontsize=14)\n",
    "            ax1.set_title(\"Test error\", fontsize=14)\n",
    "            self.fig, self.ax0, self.ax1 = fig, ax0, ax1\n",
    "        for ax in (self.ax0, self.ax1):\n",
    "            ax.tick_params(axis=\"both\", which=\"both\", labelsize=12)\n",
    "            ax.grid(which=\"both\", ls=\":\")\n",
    "            ax.set_yscale(\"log\")\n",
    "\n",
    "    def __call__(self, epoch, results):\n",
    "        if epoch == 1:\n",
    "            display(self.out)\n",
    "        loss_all = results[\"loss\"][epoch - 1]\n",
    "        best_acc_pos = np.nanargmax(results[\"acc\"][:epoch])\n",
    "        best_acc = results[\"acc\"][best_acc_pos]\n",
    "        self.ax0.plot(np.linspace(epoch - 1, epoch, len(loss_all) + 1)[:-1], loss_all, lw=0.5)\n",
    "        self.ax0.set_xticks(np.arange(0, epoch + 1, max(epoch // 5, 1)))\n",
    "\n",
    "        self.ax1.cla()\n",
    "        self.ax1.plot(np.arange(1, len(results[\"acc\"]) + 1), 1 - results[\"acc\"], lw=1)\n",
    "        self.ax1.scatter(best_acc_pos + 1, 1 - best_acc, color=\"red\", s=40, marker=\"*\")\n",
    "        self.ax1.set_xticks(np.arange(1, epoch + 1, max(epoch // 5, 1)))\n",
    "        self.ax1.set_title(f\"Test error (best: {1 - best_acc:.4f})\", fontsize=14)\n",
    "        self.ax1.grid(which=\"both\", ls=\":\")\n",
    "        self.ax1.set_yscale(\"log\")\n",
    "        with self.out:\n",
    "            clear_output(wait=True)\n",
    "            display(self.fig)\n",
    "\n",
    "    def close(self):\n",
    "        plt.close(self.fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bb3ac7",
   "metadata": {},
   "source": [
    "試しにPermuted MNISTのデータセットを描画して、タスクの内容を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4077ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_id = np.random.default_rng(1234).permutation(784)  # Shuffle pixels.\n",
    "dataset_cls = load_dataset(\"MNIST\")\n",
    "transform_kwargs = dict(\n",
    "    preprocess=True,  # Preprocess the data when loading the dataset.\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Lambda(lambda x: torch.from_numpy(np.asarray(x)).to(\"cpu\")),\n",
    "            transforms.Lambda(\n",
    "                lambda x: x.view(-1, 784, 1)[:, shuffle_id, :] / 255.0\n",
    "            ),  # Shuffle and normalize to [0, 1].\n",
    "        ]\n",
    "    ),\n",
    "    target_transform=transforms.Lambda(lambda x: torch.from_numpy(np.asarray(x, dtype=np.uint8)).to(\"cpu\")),\n",
    ")\n",
    "dataset = dataset_cls(root=\"./.cache\", train=False, download=True, **transform_kwargs)\n",
    "\n",
    "plot_num = 4\n",
    "plot_label = [2, 3, 5, 7]\n",
    "width_ratio, cell_size = 4.0, 1.5\n",
    "grid_size = (len(plot_label), plot_num * 2 + 1)\n",
    "fig, ax = plt.subplots(\n",
    "    *grid_size,\n",
    "    figsize=((plot_num * 2 + width_ratio) * cell_size, grid_size[0] * cell_size),\n",
    "    gridspec_kw={\"wspace\": 0.05, \"hspace\": 0.05, \"width_ratios\": [1] * plot_num * 2 + [width_ratio]},\n",
    ")\n",
    "\n",
    "recover_id = np.argsort(shuffle_id)  # Recover the original pixel order.\n",
    "for idx, label in enumerate(plot_label):\n",
    "    indices = np.where(dataset.targets.numpy() == label)[0][:plot_num]\n",
    "    ax[idx, 0].set_ylabel(f\"Label: {label}\", fontsize=12)\n",
    "    for idy, index in enumerate(indices):\n",
    "        image, target = dataset[index]\n",
    "        ax[idx, idy].imshow(image[recover_id].view(28, 28).numpy(), cmap=\"gray\", vmin=0.0, vmax=1.0)\n",
    "        ax[idx, idy].axis(\"off\")\n",
    "        ax[idx, idy + plot_num].imshow(image.view(28, 28).numpy(), cmap=\"gray\", vmin=0.0, vmax=1.0)\n",
    "        ax[idx, idy + plot_num].axis(\"off\")\n",
    "        ax[idx, -1].plot(image.view(784).numpy(), lw=0.5)\n",
    "    ax[idx, -1].set_yticklabels([])\n",
    "    if idx < len(plot_label) - 1:\n",
    "        ax[idx, -1].set_xticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046c66e",
   "metadata": {},
   "source": [
    "左側には元の画像、中央にはシャッフルされた画像、そして右側には784次元の時系列として描画された入力データが示されています。\n",
    "左側の画像からは数字を認識できますが、中央のシャッフルされた画像からはもはや何の数字か読み取れません。\n",
    "ただ全結合型のニューラルネットワークでは、一度に784次元の入力を受け取れるので、シャッフルの仕方が固定ならば問題なく学習が可能です。\n",
    "そこでPermuted MNISTでは784ステップの時系列として順次が入力として与えてタスクの難しさを担保しています。\n",
    "タスクの難しさを直感的に理解できましたでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05909483",
   "metadata": {},
   "source": [
    "##### 学習の実行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c7c3b9",
   "metadata": {},
   "source": [
    "さて準備が整ったので学習に移ります。\n",
    "以下のコードはデフォルトでは　$(L, H, N) = (4, 64, 64)$ の条件でDeepSSMを構築し、最終ステップの最終層のSSMの出力のみを参照(`average_pool=False`)して、Permuted MNISTの学習を行います。\n",
    "果たしてどの程度の精度が得られるでしょうか？(実行環境にもよりますがGPUを用いて数分程度で学習が完了すると思います)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e56240",
   "metadata": {},
   "outputs": [],
   "source": [
    "permute_seed, model_seed, training_seed = 123, 456, 789  # Random seeds (permute pixels, model init, training)\n",
    "input_dim, output_dim = 1, 10\n",
    "num_layer, num_ssm, num_basis = 4, 64, 64\n",
    "dt_min, dt_max = 1e-4, 1e-2  # Range of time scales for S4D layers\n",
    "dropout, prenorm, average_pool = 0.0, False, False\n",
    "tunable_dt, tunable_basis = False, False  # Whether to make dt and basis parameters trainable (default: False)\n",
    "basis_type = \"s4d_inv\"  # \"s4d_inv\" or \"s4d_lin\"\n",
    "epoch_num, batch_size = 20, 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Load dataset\n",
    "shuffle_id = np.random.default_rng(permute_seed).permutation(784)  # Shuffle pixels\n",
    "# shuffle_id = np.arange(784)  # No shuffle (a.k.a. Sequential MNIST), uncomment it to see the effect of pixel permutation.\n",
    "dataset_cls = load_dataset(\"MNIST\")  # You can change it to \"FashionMNIST\" or \"KMNIST\".\n",
    "transform_kwargs = dict(\n",
    "    preprocess=True,  # Preprocess the data when loading the dataset.\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Lambda(lambda x: torch.from_numpy(np.asarray(x)).to(device)),\n",
    "            transforms.Lambda(\n",
    "                lambda x: x.view(-1, 784, 1)[:, shuffle_id, :] / 255.0\n",
    "            ),  # Shuffle and normalize to [0, 1].\n",
    "        ]\n",
    "    ),\n",
    "    target_transform=transforms.Lambda(lambda x: torch.from_numpy(np.asarray(x, dtype=np.uint8)).to(device)),\n",
    ")\n",
    "train_dataset = dataset_cls(root=\"./.cache\", train=True, download=True, **transform_kwargs)\n",
    "test_dataset = dataset_cls(root=\"./.cache\", train=False, download=True, **transform_kwargs)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Build model\n",
    "model = DeepSSM(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_layer=num_layer,\n",
    "    num_ssm=num_ssm,\n",
    "    num_basis=num_basis,\n",
    "    dropout=dropout,\n",
    "    prenorm=prenorm,\n",
    "    average_pool=average_pool,\n",
    "    rnd=np.random.default_rng(model_seed),\n",
    "    dt_min=dt_min,\n",
    "    dt_max=dt_max,\n",
    "    basis_type=basis_type,\n",
    "    tunable_dt=tunable_dt,\n",
    "    tunable_basis=tunable_basis,\n",
    ")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# run training\n",
    "torch.manual_seed(training_seed)\n",
    "callback = ProgressCallback()\n",
    "results = run_train_and_test(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    num_epochs=epoch_num,\n",
    "    callback=callback,\n",
    ")\n",
    "callback.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fbfa93",
   "metadata": {},
   "source": [
    "デフォルトではおそらく `s4d-inv`と`s4d-lin` のいずれも94〜96%程度の精度 (4〜6%程度の誤差率) に到達すると思います。\n",
    "時間がかかってしまいますが、SSMのパラメータ数を調整する $L, H, N$ やエポック数を増やしたり、学習率やドロップアウト率を調整するとさらに精度が向上し、97〜98%程度の精度が得られると思います。\n",
    "このあたり実際に試行錯誤してみてハイパーパラメータと精度の関係を確認してください。\n",
    "\n",
    "また $\\Delta t$ のレンジを調整する `dt_min, dt_max` の値が特に学習に影響を与えます。\n",
    "というのも今回長さ784ステップの時系列データを扱っているため、これより早いスケール (1/784程度 以上の値) で $\\Delta t$ を設定してしまうと、SSMのダイナミクスの減衰が早すぎて十分に情報を保持できないからです。\n",
    "試しに `dt_min=1e-3, dt_max=1e-1` と設定して学習が破綻する様子も確認してください。\n",
    "\n",
    "以下のセルは各SSM $(A, B)$ (すなわちLESN $(\\bar{A}, \\bar{B})$ ) の固有値を描画します。\n",
    "うまく行く場合と行かない場合で固有値がどう変化するか確認してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91096ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diag_eigen(ax, values):\n",
    "    ts = np.linspace(0, 2 * np.pi, 1001)\n",
    "    cs = np.linspace(0, 1.0, len(values))\n",
    "    color = plt.get_cmap(\"hsv\")(cs)\n",
    "\n",
    "    ax.tick_params(axis=\"both\", labelsize=12)\n",
    "    ax.scatter(values.real, values.imag, s=3.0, c=color)\n",
    "    ax.scatter(values.real, -values.imag, s=3.0, c=color)\n",
    "    ax.plot(np.cos(ts), np.sin(ts), lw=0.5, ls=\"--\", color=\"k\")\n",
    "    ax.set_aspect(\"equal\", adjustable=\"datalim\")\n",
    "    ax.grid(which=\"both\", ls=\":\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_diag_eigen_all(model, plot_num=4):\n",
    "    plot_num = min(plot_num, model.s4_layers[0].kernel.eigs.size(0))\n",
    "    grid_size = (len(model.s4_layers), plot_num)\n",
    "    fig, ax = plt.subplots(\n",
    "        *grid_size, figsize=(2 * grid_size[1], 2 * grid_size[0]), gridspec_kw={\"hspace\": 0.25, \"wspace\": 0.25}\n",
    "    )\n",
    "\n",
    "    for idx, layer in enumerate(model.s4_layers):\n",
    "        eigs = layer.kernel.eigs.detach().cpu().numpy()\n",
    "        for pos in range(plot_num):\n",
    "            plot_diag_eigen(ax[idx, pos % plot_num], eigs[pos])\n",
    "            if idx == 0:\n",
    "                ax[idx, pos % plot_num].set_title(f\"$\\\\Delta t_{{{pos}}}$\", fontsize=14)\n",
    "        ax[idx, 0].set_ylabel(f\"Layer {idx + 1}\", fontsize=12)\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "fig, ax = plot_diag_eigen_all(model, plot_num=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7eadff",
   "metadata": {},
   "source": [
    "##### 標準的なLESNとの比較"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41416de7",
   "metadata": {},
   "source": [
    "ここまで扱った固有値の初期化の工夫の効果を確認するため、標準的なLESNを用いたDeepSSMと比較してみましょう。\n",
    "これまでの章で扱いましたが、ESNは通常正規分布 $\\mathcal{N}(0, 1/N)$ からサンプリングされたランダム行列を内部結合行列として使用します。\n",
    "これは、このようにしてサンプルされた行列は、サイズが十分に大きい場合、複素平面上の単位円板上で一様に分布する固有値を持つので (Cf. [円則](https://en.wikipedia.org/wiki/Circular_law))、概ね内部行列のスペクトル半径を1に設定できるからです。\n",
    "逆に言えば、複素平面上での単位円板上で一様に分布するように複素数を $N$ 個サンプリングし構築されたカーネルは、標準的なLESNによるそれと実質的に同等であると考えられます。\n",
    "ただ通常LESNでは時間幅 $\\Delta t$ を持たず、内部結合の違いで時定数が表現されるので、その固有値を改めて $\\{\\bar{\\lambda}_n\\}_n$ として以下の形でカーネルを計算します。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bar{K}^{(T)} &=\n",
    "\\left(C_n \\bar{\\lambda}_n^k\\right)_{k,n}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f61a67c",
   "metadata": {},
   "source": [
    "これまでは $\\exp\\left(\\lambda_n \\Delta t\\right)$ が $\\bar{A}$ の固有値でしたが、ここでは $\\bar{\\lambda}_n$ のみで全体を表現している点に注意してください。\n",
    "以下のセルでそのようなカーネルの計算を行う `LESNKernel` を実装してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0a134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "default_type = \"float32\"\n",
    "\n",
    "\n",
    "def sample_random_eigs(num_ssm, num_basis, rnd=None, dtype=default_type, radius_min=0.0, radius_max=0.95, **_kwargs):\n",
    "    rnd = np.random.default_rng() if rnd is None else rnd\n",
    "    radius = rnd.uniform(radius_min**2, radius_max**2, size=(num_ssm, num_basis // 2)).astype(dtype) ** 0.5\n",
    "    angle = rnd.uniform(0.0, np.pi, size=(num_ssm, num_basis // 2)).astype(dtype)\n",
    "    eigs = radius * np.exp(1j * angle)\n",
    "    return eigs\n",
    "\n",
    "\n",
    "class LESNKernel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_ssm,\n",
    "        num_basis,\n",
    "        rnd=None,\n",
    "        eigen_func=sample_random_eigs,\n",
    "        dtype=default_type,\n",
    "        tunable_eigen=False,\n",
    "        **eigen_func_kwargs,\n",
    "    ):\n",
    "        super(LESNKernel, self).__init__()\n",
    "        assert num_basis % 2 == 0, \"N must be even\"\n",
    "        rnd = np.random.default_rng() if rnd is None else rnd\n",
    "\n",
    "        # Complex output matrix\n",
    "        C = rnd.normal(0, 1, size=(num_ssm, num_basis // 2, 2)).astype(dtype)  # (H, N/2, 2)\n",
    "        self.register_parameter(\"C\", nn.Parameter(torch.view_as_complex(torch.from_numpy(C))))\n",
    "\n",
    "        # Eigenvalues\n",
    "        eigs = eigen_func(num_ssm, num_basis, rnd=rnd, dtype=dtype, **eigen_func_kwargs)  # (H, N/2)\n",
    "        if tunable_eigen:\n",
    "            A_abs, A_angle = np.abs(eigs), np.angle(eigs)\n",
    "            self.register_parameter(\"log_log_A_abs\", nn.Parameter(torch.from_numpy(np.log(-np.log(A_abs)))))\n",
    "            self.register_parameter(\"A_angle\", nn.Parameter(torch.from_numpy(A_angle)))\n",
    "        else:\n",
    "            self.register_buffer(\"A\", torch.from_numpy(eigs))\n",
    "\n",
    "    @property\n",
    "    def A(self):\n",
    "        return torch.exp(-torch.exp(self.log_log_A_abs) + 1j * self.A_angle)\n",
    "\n",
    "    @property\n",
    "    def eigs(self):\n",
    "        return self.A\n",
    "\n",
    "    def forward(self, time_steps):\n",
    "        A = self.A  # (H, N/2)\n",
    "        C = self.C  # (H, N/2)\n",
    "        # Vandermonde multiplication\n",
    "        K = A[..., None] ** torch.arange(time_steps, device=A.device)  # (H, N/2, T)\n",
    "        K = torch.einsum(\"hn, hnt -> ht\", 2 * C, K).real\n",
    "        return K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a2b00a",
   "metadata": {},
   "source": [
    "`sample_random_eigs` はノルムが $(r_\\mathrm{min}, r_\\mathrm{max})$ の範囲、偏角が $[0, \\pi)$ の範囲で一様に分布するように複素数をサンプリングします。\n",
    "したがって例えば `r_min=0.0, r_max=0.9` とすると、スペクトル半径が0.9未満程度のLESNを模倣したLESNによるカーネルが得られます。\n",
    "この条件でPermuted MNISTを学習してみましょう。\n",
    "以下のコードは、カーネルの設定以外は先ほどのセルと同じ条件で学習を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "permute_seed, model_seed, training_seed = 123, 456, 789  # Random seeds (permute pixels, model init, training)\n",
    "input_dim, output_dim = 1, 10\n",
    "num_layer, num_ssm, num_basis = 4, 64, 64\n",
    "dropout, prenorm, average_pool = 0.0, False, False\n",
    "tunable_eigen = False  # Whether to make basis parameters trainable (default: False)\n",
    "epoch_num, batch_size = 20, 128\n",
    "learning_rate = 1e-3\n",
    "radius_min, radius_max = 0.0, 0.9  # Range of radius of eigenvalues for LESNKernel\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Load dataset\n",
    "shuffle_id = np.random.default_rng(permute_seed).permutation(784)  # Shuffle pixels\n",
    "# shuffle_id = np.arange(784)  # No shuffle (a.k.a. Sequential MNIST), uncomment it to see the effect of pixel permutation\n",
    "dataset_cls = load_dataset(\"MNIST\")  # You can change it to \"FashionMNIST\" or \"KMNIST\"\n",
    "transform_kwargs = dict(\n",
    "    preprocess=True,  # Preprocess the data when loading the dataset\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Lambda(lambda x: torch.from_numpy(np.asarray(x)).to(device)),\n",
    "            transforms.Lambda(\n",
    "                lambda x: x.view(-1, 784, 1)[:, shuffle_id, :] / 255.0\n",
    "            ),  # Shuffle and normalize to [0, 1]\n",
    "        ]\n",
    "    ),\n",
    "    target_transform=transforms.Lambda(lambda x: torch.from_numpy(np.asarray(x, dtype=np.uint8)).to(device)),\n",
    ")\n",
    "train_dataset = dataset_cls(root=\"./.cache\", train=True, download=True, **transform_kwargs)\n",
    "test_dataset = dataset_cls(root=\"./.cache\", train=False, download=True, **transform_kwargs)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Build model\n",
    "model = DeepSSM(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_layer=num_layer,\n",
    "    num_ssm=num_ssm,\n",
    "    num_basis=num_basis,\n",
    "    dropout=dropout,\n",
    "    prenorm=prenorm,\n",
    "    average_pool=average_pool,\n",
    "    kernel_cls=LESNKernel,  # NOTE: Use LESNKernel instead of S4DKernel.\n",
    "    rnd=np.random.default_rng(model_seed),\n",
    "    tunable_eigen=tunable_eigen,\n",
    "    eigen_func=sample_random_eigs,  # Argument for LESNKernel\n",
    "    radius_min=radius_min,\n",
    "    radius_max=radius_max,\n",
    ")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training phase\n",
    "torch.manual_seed(training_seed)\n",
    "callback = ProgressCallback()\n",
    "results = run_train_and_test(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    num_epochs=epoch_num,\n",
    "    callback=callback,\n",
    ")\n",
    "callback.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e6c67",
   "metadata": {},
   "source": [
    "精度が90%程度で頭打ちとなる様子が確認できると思います。\n",
    "通常のMNISTが線形モデルでも92%程度の精度が得られるので、これはあまり良い結果とは言えません。\n",
    "この原因ですが固有値の分布が複素平面上の円板上に一様に分布しており、Permuted MNISTに必要な時定数の長い成分が不足しているためです。\n",
    "実際にS4Dでの固有値がほぼ単位円周上に分布していた点を思い出してください (`plot_diag_eigen_all(model)`を実行するとここでも固有値の分布を確認できます)。\n",
    "\n",
    "したがってこの設定でも $r_\\mathrm{min}, r_\\mathrm{max}$ を1に近づけると精度が向上すると期待されます。\n",
    "試しに、このS4Dの設定に倣って `radius_min=0.99, radius_max=1.0` として再実行してみてください。\n",
    "先程のS4Dと同等、あるいはそれ以上の精度が得られると思います。\n",
    "\n",
    "このようにSSMの固有値の初期化次第で、SSMの長期記憶特性と学習の成否が大きく変わります。\n",
    "そしてSSMの長期記憶特性は、緩やかに減衰する様々な周波数特性をもつ固有値 (絶対値が1に近く偏角が多様) によって実現されていると考えられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a9012",
   "metadata": {},
   "source": [
    "#### その他のタスク (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee3b01",
   "metadata": {},
   "source": [
    "ここまでPermuted MNISTを例に学習の様子を確認しましたが、冒頭に述べられたように、SSMは長期記憶特性がより必要な様々なタスクに応用できます。\n",
    "またPermuted MNISTのような識別タスクのみならず、音声合成や画像生成などの生成タスクにも応用できます。\n",
    "ただしPermuted MNISTに比べて計算リソースが必要になる場合が多いので、ここでは紹介のみに留めます。\n",
    "以下発展課題として記載しておくので、興味があればぜひ試してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87033877",
   "metadata": {},
   "source": [
    "Q5.1. (Advanced)\n",
    "\n",
    "- CIFAR10は $32 \\times 32$ サイズのRGB画像を10クラスに分類するタスクである。このCIFAR10に関してもMNIST同様にSequential / Permuted CIFAR10を構築し、DeepSSMを用いて学習を行え。\n",
    "  - メモ1: 入力の次元が3 (RGB) なので、`input_dim=3` とする。\n",
    "  - メモ2: 画像を $32 \\times 32 = 1024$ ステップの時系列として扱う。\n",
    "  - メモ3: S4論文<sup>[5]</sup>では、$(L, H, N)=(6,1024,64)$ の条件でかつ`dropout=0.25`を指定して学習している。実行環境によっては時間がかかるので、適宜パラメータを調整しながら参考にせよ。\n",
    "  - メモ4: 読み込みに関しては以下のコードを参考にせよ。\n",
    "\n",
    "```python\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "shuffle_id = np.arange(1024)  # Sequential CIFAR10\n",
    "# shuffle_id = np.random.default_rng(1234).permutation(1024)  # Permuted CIFAR10\n",
    "dataset_cls = load_dataset(\"CIFAR10\")\n",
    "transform_kwargs = dict(\n",
    "    preprocess=True,  # Preprocess the data when loading the dataset\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Lambda(lambda x: torch.from_numpy(np.asarray(x)).to(device)),\n",
    "            transforms.Lambda(\n",
    "                lambda x: x.view(-1, 1024, 3)[:, shuffle_id, :] / 255.0\n",
    "            ),  # Shuffle and normalize to [0, 1]\n",
    "        ]\n",
    "    ),\n",
    "    target_transform=transforms.Lambda(lambda x: torch.from_numpy(np.asarray(x, dtype=np.uint8)).to(device)),\n",
    ")\n",
    "train_dataset = dataset_cls(root=\"./.cache\", train=True, download=True, **transform_kwargs)\n",
    "test_dataset = dataset_cls(root=\"./.cache\", train=False, download=True, **transform_kwargs)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for image, target in train_loader:\n",
    "    print(image.shape, image.dtype, image.device)\n",
    "    print(target.shape, target.dtype, target.device)\n",
    "    break\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28561a6",
   "metadata": {},
   "source": [
    "Q5.2. (Advanced)\n",
    "\n",
    "- [LRA](https://github.com/google-research/long-range-arena)のPathfinderに関してにも同様に`DeepSSM`を用いて学習を行え。\n",
    "    - メモ1: https://storage.googleapis.com/long-range-arena/lra_release.gz からダウンロードできる。ただしファイルのサイズ(8GB程度)が大きくかつ解凍に時間がかかるので注意。\n",
    "    - メモ2: 入力の次元が1 (グレースケール) なので、`input_dim=1` とする。\n",
    "    - メモ3: Pathfinderは画像中の２つのマーカーが線によってつながっているか否かを分類するタスクである。したがって2クラス分類なので`output_dim=2`とする。\n",
    "    - メモ4: 難易度に応じて画像サイズが $N=32, 64, 128, 256$ となる4種類のタスクが用意されている。いずれもピクセルをランダムにシャッフルしてから $N^2$ ステップの時系列として扱う 。まずは $N=32$ から試し、徐々にサイズを大きくしていくとよい。\n",
    "    - メモ5: 読み込みのためのデータローダーを実装しなければならない。[Pytorch公式のチュートリアル](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files)を参考にせよ。\n",
    "- LRAの他のタスクに関しても余力があれば実装せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2362855",
   "metadata": {},
   "source": [
    "Q5.3. (Advanced)\n",
    "\n",
    "- S4・S4Dは識別タスクだけでなく、生成タスクにも応用できる。[元論文のS4のデモンストレーション](https://srush.github.io/annotated-s4/)に倣って音声、あるいは画像生成モデルを実装せよ。\n",
    "    - メモ1: 実はこれまで用意したコードの僅かな改変で実現できる。すなわち識別モデル(`DeepSSM`の`self.decoder`に相当する部分)を、例えば次の時刻のピクセル、あるいは音声波形を予測するようなデコーダモデルに置き換えればよい。\n",
    "    - メモ2: 学習時は今までのコードとおり、全時系列 $u[k]$ を与えて、得られた $x[k]$ から $u[k+1]$ を予測するようにデコーダモデルを学習すればよい。これはこれまで学習したopen-loopでデータを収集し、closed-loopを構築するRCの学習方法と似ている。\n",
    "    - メモ3: 生成時はTransformerなどの自己回帰モデルと同様に、時々刻々処理する形式に変更する必要がある。この場合FFTを用いるよりも、直接ダイナミクスを計算して時々刻々出力させるほうが効率的である。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2620429",
   "metadata": {},
   "source": [
    "## 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281bac74",
   "metadata": {},
   "source": [
    "[1] Jaeger, H. (2001). *Short term memory in echo state networks*. GMD Forschungszentrum Informationstechnik. https://doi.org/10.24406/publica-fhg-291107\n",
    "\n",
    "[2] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). *Language models are unsupervised multitask learners.* OpenAI blog, 1(8), 9.\n",
    "\n",
    "[3] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2023). *Attention Is All You Need* (No. arXiv:1706.03762). arXiv. https://doi.org/10.48550/arXiv.1706.03762\n",
    "\n",
    "[4] Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., & Metzler, D. (2020). *Long Range Arena: A Benchmark for Efficient Transformers* (No. arXiv:2011.04006). arXiv. https://doi.org/10.48550/arXiv.2011.04006\n",
    "\n",
    "[5] Gu, A., Dao, T., Ermon, S., Rudra, A., & Re, C. (2020). *HiPPO: Recurrent Memory with Optimal Polynomial Projections* (No. arXiv:2008.07669). arXiv. https://doi.org/10.48550/arXiv.2008.07669\n",
    "\n",
    "[6] Gu, A., Goel, K., & Ré, C. (2022). *Efficiently Modeling Long Sequences with Structured State Spaces* (No. arXiv:2111.00396). arXiv. https://doi.org/10.48550/arXiv.2111.00396\n",
    "\n",
    "[7] Gu, A., Gupta, A., Goel, K., & Ré, C. (2022). *On the Parameterization and Initialization of Diagonal State Space Models* (No. arXiv:2206.11893). arXiv. https://doi.org/10.48550/arXiv.2206.11893\n",
    "\n",
    "[8] Gu, A., Johnson, I., Timalsina, A., Rudra, A., & Ré, C. (2022). *How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections* (No. arXiv:2206.12037). arXiv. https://doi.org/10.48550/arXiv.2206.12037\n",
    "\n",
    "[9] Gu, A., & Dao, T. (2024). *Mamba: Linear-Time Sequence Modeling with Selective State Spaces* (No. arXiv:2312.00752). arXiv. https://doi.org/10.48550/arXiv.2312.00752\n",
    "\n",
    "[10] LeCun, Y., Bengio, Y., & Hinton, G. (2015). *Deep learning. Nature, 521(7553)*, 436–444. https://doi.org/10.1038/nature14539\n",
    "\n",
    "[11] Ma, Q., Shen, L., & Cottrell, G. W. (2017). *Deep-ESN: A Multiple Projection-encoding Hierarchical Reservoir Computing Framework* (No. arXiv:1711.05255). arXiv. https://doi.org/10.48550/arXiv.1711.05255\n",
    "\n",
    "[12] Guan, J., Kubota, T., Kuniyoshi, Y., & Nakajima, K. (2025). *Memory Determines Learning Direction: A Theory of Gradient-Based Optimization in State Space Models* (No. arXiv:2510.00563; Version 1). arXiv. https://doi.org/10.48550/arXiv.2510.00563\n",
    "\n",
    "[13] Gupta, A., Gu, A., & Berant, J. (2022). *Diagonal State Spaces are as Effective as Structured State Spaces* (No. arXiv:2203.14343). arXiv. https://doi.org/10.48550/arXiv.2203.14343\n",
    "\n",
    "[14] Deng, L. (2012). *The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]*. IEEE Signal Processing Magazine, 29(6), 141–142. https://doi.org/10.1109/MSP.2012.2211477"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rc-bootcamp (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
