{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6. 記憶容量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この章では、記憶容量、すなわち力学系が有する過去の入力を保持する能力を評価する指標の計算方法を学びます。\n",
    "特に記憶関数 (Memory Function; MF) と記憶容量 (Memory Capacity; MC) の２つを実装し、その使い方を学習します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前書き"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "記憶関数・記憶容量はH. Jaeger<sup>[1]</sup>らによって提案された指標で、力学系が過去の入力をどれほど保持できるかを定量化します。\n",
    "以下の式で表される $N$次元の入力あり力学系 $x[k]$ とある線形写像$g: \\mathbb{R}^N \\to \\mathbb{R}$ による出力 $\\hat{y}[k]$ を考えます。\n",
    "\n",
    "$$\n",
    "\\renewcommand{\\Tau}{\\mathrm{T}}\n",
    "\\renewcommand{\\Zeta}{\\mathrm{Z}}\n",
    "\\begin{align*}\n",
    "x[k+1] &= f \\left(x[k],\\zeta[k+1]\\right) \\\\\n",
    "\\hat{y}[k] &= g (x[k])\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g$は線形なので、ある結合パラメータ $W^\\mathrm{out} \\in \\mathbb{R}^{N+1}$ を用いて出力 $\\hat{y}[k]$ は次の式で表現できます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y}[k] &= \\hat{w} [1 ; x[k]] \\\\\n",
    "&= \\hat{w} {[1 \\quad x_1[k] \\quad \\cdots \\quad x_{N}[k]]}^\\top \\\\\n",
    "&= \\hat{w}_0 + \\sum_{i=1}^{N} \\hat{w}_i x_{i}[k]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力 $\\zeta[k]$、内部状態 $x[k]$ ともに定常的、すなわちその平均や分散が時間によらず一定であると仮定します。\n",
    "このとき記憶関数 $\\mathrm{MF}[\\tau]$ は特に $\\tau~(\\geq 0)$ ステップ前の入力 $\\zeta^\\tau[k]:=\\zeta[k-\\tau]$ を内部状態 $x[k]$ からどれほど再構成できるかを評価する指標で、次の式で定義されます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{MF}[\\tau] :=& \\max_{\\hat{w}} \\rho^2[\\zeta^\\tau, \\hat{y}] \\\\\n",
    "=& \\max_{\\hat{w}} \\frac{\\mathrm{Cov}^2[\\zeta^\\tau, \\hat{y}]}{\\mathrm{Var}[\\zeta^\\tau]\\mathrm{Var}[\\hat{y}]}\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで $\\rho$ は相関係数、 $\\mathrm{Cov}$ は共分散、$\\mathrm{Var}$ は分散を表します。\n",
    "相関係数の絶対値は $1$ 以下であるので、以下の不等式が成り立ちます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "0 \\leq \\mathrm{MF}[\\tau] \\leq 1\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に全過去入力に対する記憶関数の総和により、以下の式で記憶容量 $\\mathrm{MC}$ は定義されます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{MC} := \\sum_{\\tau=0}^{\\infty} \\mathrm{MF}[\\tau]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特に入力がi.i.d.、つまり各時刻で$\\zeta[k]$の値が独立にサンプルされる場合、以下の不等式の成立が知られています (導出は発展課題)。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{MC} \\leq r \\leq N\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで $r$ は $x[k]$ の階数を表し、式が示すとおり、高々線形独立な成分の数にその記憶容量が制限されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習問題と実演"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここからは演習問題とデモンストレーションに移ります。\n",
    "前回と同じライブラリの他、前回の演習で実装した`ESN`・`Linear`が`import`により利用できます。\n",
    "初めに次のセルを実行してください。\n",
    "\n",
    "なお`ESN`・`Linear`の内部実装を再確認するには、`import inspect`以下の行をコメントアウトするか`...?? / ??...`を使用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import drive  # type: ignore\n",
    "\n",
    "    if False:  # Set to True if you want to use Google Drive and save your work there.\n",
    "        drive.mount(\"/content/gdrive\")\n",
    "        %cd /content/gdrive/My Drive/rc-bootcamp/\n",
    "        # NOTE: Change it to your own path if you put the zip file elsewhere.\n",
    "        # e.g., %cd /content/gdrive/My Drive/[PATH_TO_EXTRACT]/rc-bootcamp/\n",
    "    else:\n",
    "        pass\n",
    "        %cd /content/\n",
    "        !git clone --branch ja https://github.com/rc-bootcamp/rc-bootcamp.git\n",
    "        %cd /content/rc-bootcamp/\n",
    "else:\n",
    "    sys.path.append(\".\")\n",
    "\n",
    "from utils.reservoir import ESN, Linear\n",
    "from utils.style_config import plt\n",
    "from utils.tester import load_from_chapter_name\n",
    "from utils.tqdm import tqdm, trange\n",
    "\n",
    "test_func, show_solution = load_from_chapter_name(\"06_memory_capacity\")\n",
    "\n",
    "\n",
    "# Uncomment it to see the implementations of `Linear` and `ESN`.\n",
    "# import inspect\n",
    "# print(inspect.getsource(Linear))\n",
    "# print(inspect.getsource(ESN))\n",
    "\n",
    "# Or just use ??.../...?? (uncomment the following lines).\n",
    "# Linear??\n",
    "# ESN??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 特異値分解を用いた記憶関数の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず記憶関数の実装を行いましょう。\n",
    "$\\mathrm{MF}[\\tau]$ は$\\zeta^\\tau$ と $\\hat{y}$ の相関係数の二乗の最大値として定義されます。\n",
    "一方で相関係数の二乗は、決定係数 $\\mathrm{R}^2[\\zeta^\\tau, \\hat{y}]$ と一致する (導出は発展課題) ので以下の式が成り立ちます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{MF}[\\tau] &= \\max_{\\hat{w}} \\mathrm{R}^2[\\zeta^\\tau, \\hat{y}] \\\\\n",
    "&= 1 - \\min_{\\hat{w}} \\frac{\\mathrm{E}[(\\zeta^\\tau - \\hat{y})^2]}{\\mathrm{Var}[\\zeta^\\tau]} \\\\\n",
    "&= 1 - \\frac{\\min_{\\hat{w}} \\mathrm{MSE}(\\zeta^\\tau, \\hat{y})}{\\mathrm{Var}[\\zeta^\\tau]}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今線型回帰を考えているので $\\mathrm{MSE}$ を最小化する $\\hat{w}$ は線型回帰により一意に導出され、$\\mathrm{MF}[\\tau]$ は $\\hat{y}$ を用いない以下の形式で表現されます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{MF}[\\tau] &= \\mathrm{R}^2[\\zeta^\\tau, x]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この式は、明示的に過去時系列を再構成する $g$ ならびに $\\hat{w}$ を計算せずとも、特異値分解 (Singular Value Decomposition; SVD) を用いて計算できます (導出は第2章 Q5.4. 参照)。\n",
    "つまり $T$ ステップに渡る内部状態のダイナミクスを格納した説明変数行列 $X=[x[0];x[1];~\\ldots;~x[T-1]]^\\top \\in \\mathbb{R}^{T \\times N}$ と、対応する目的変数行列 $\\Zeta^\\tau = [\\zeta[-\\tau],\\zeta[-\\tau+1],~\\ldots,~\\zeta[T-\\tau-1]]^\\top \\in \\mathbb{R}^{T \\times 1}$ に関して、$X=U\\Sigma V^\\top$ と分解された後、以下の式で $\\mathrm{MF}[\\tau]$ は計算されます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{MF}[\\tau] &= \\frac{\\|U^\\top \\Zeta^\\tau\\|^2}{\\|\\Zeta^\\tau\\|^2}\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ただし $X, \\Zeta^\\tau$ ともに正規化 (平均が零となるように変換) されているものとします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.1.\n",
    "\n",
    "説明変数$X\\in\\mathbb{R}^{T \\times N}$ が与えられる。Xを正規化したのち、$X=U\\Sigma V^\\top$ とSVDを行い、$U\\in\\mathbb{R}^{T \\times N}$ と$X$ の階数 $r$ を出力する関数 `calc_regression_and_rank`を実装せよ。\n",
    "\n",
    "- `calc_regression_and_rank`\n",
    "  - Argument(s):\n",
    "    - `X`: `np.ndarray`\n",
    "      - `shape`: `(..., t, n)`\n",
    "      - `dtype`: `np.float64`\n",
    "  - Return(s):\n",
    "    - `U`: `np.ndarray`\n",
    "      - `shape`: `(..., t, n)`\n",
    "      - `dtype`: `np.float64`\n",
    "    - `mask`: `np.ndarray`\n",
    "      - `shape`: `(..., n)`\n",
    "      - `dtype`: `np.boolean`\n",
    "    - `rank`: `np.ndarray`\n",
    "      - `shape`: `(...,)`\n",
    "      - `dtype`: `np.int64`\n",
    "  - $10^{2} \\leq T \\leq 10^{3}$\n",
    "  - $1 \\leq N \\leq 10^{2}$\n",
    "\n",
    "<details><summary>tips</summary>\n",
    "\n",
    "- [`np.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)\n",
    "- [`np.linalg.matrix_rank`](https://numpy.org/devdocs/reference/generated/numpy.linalg.matrix_rank.html)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_regression_and_rank(X):\n",
    "    T, N = X.shape[-2:]\n",
    "    X = ...  # TODO Calculate centered X.\n",
    "    U, sigma, V = ...  # TODO Use `np.linalg.svd` to perform SVD.\n",
    "    eps = np.finfo(X.dtype).eps\n",
    "    sigma_sq_max = np.max(sigma * sigma, axis=-1, keepdims=True)\n",
    "    eps = sigma_sq_max * (eps * max(T, N))\n",
    "    mask = sigma > eps\n",
    "    rank = ...  # TODO Calculate rank by counting number of singular values greater than `eps`.\n",
    "    return U, mask, rank\n",
    "\n",
    "\n",
    "test_func(calc_regression_and_rank, \"01_01\", multiple_output=True)\n",
    "# show_solution(\"01_01\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.2.\n",
    "\n",
    "`calc_regression_and_rank`で得られた $U \\in \\mathbb{R}^{T\\times N}$ を用いて、$\\mathrm{MF}[\\tau]$ を計算する関数 `calc_memory_function`を実装せよ。\n",
    "\n",
    "- `calc_memory_function`\n",
    "  - Argument(s):\n",
    "    - `U`: `np.ndarray`\n",
    "      - `shape`: `(..., t, n)`\n",
    "      - `dtype`: `np.float64`\n",
    "    - `mask`: `np.ndarray`\n",
    "      - `shape`: `(..., n)`\n",
    "      - `dtype`: `np.boolean`\n",
    "    - `zeta`: `np.ndarray`\n",
    "      - `shape`: `(..., t, 1)`\n",
    "      - `dtype`: `np.float64`\n",
    "  - Return(s):\n",
    "    - `r2`: `np.ndarray`\n",
    "      - `shape`: `(..., t, 1)`\n",
    "      - `dtype`: `np.float64`\n",
    "  - $10^{2} \\leq T \\leq 10^{3}$\n",
    "  - $1 \\leq N \\leq 10^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_memory_function(U, mask, zeta):\n",
    "    uzeta = ...  # TODO Calculate U^T * zeta.\n",
    "    dot = ((uzeta * uzeta) * mask[..., None]).sum(\n",
    "        axis=-2\n",
    "    )  # Calculate dot product considering only components where mask is True.\n",
    "    var = ...  # TODO Calculate variance of zeta.\n",
    "    r2 = ...  # TODO Calculate R^2 (`dot` divided by `var`).\n",
    "    return r2\n",
    "\n",
    "\n",
    "test_func(calc_memory_function, \"01_02\")\n",
    "# show_solution(\"01_02\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.3. (Advanced)\n",
    "\n",
    "- 相関係数 $\\rho^2$ と決定係数 $\\mathrm{R}^2$ の関係を導出し、それらの一致を確認せよ。\n",
    "- 線形ESN、すなわち活性化関数がなく、かつ入力時系列がi.i.d.であるとき、$\\tau$に対する$\\mathrm{MF}[\\tau]$ の単調減少性を示せ。\n",
    "- 入力時系列がi.i.d.であるとき、$\\mathrm{MC} \\leq N$ を示せ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ESNの記憶容量の計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまで実装した`calc_regression_and_rank`と`calc_memory_function`を用いて、記憶容量を図示化してみましょう。\n",
    "以下のセルはESNの記憶容量を計算し記憶関数をプロットします。\n",
    "入力時系列は一様乱数 $\\mathcal{U}([-1, 1])$ からサンプルし、複数のスペクトル半径 (Spectral Radius; SR) に対して図示化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "dim = 50\n",
    "t_washout = 1000\n",
    "t_sample = 20000\n",
    "t_total = t_washout + t_sample\n",
    "display = True\n",
    "\n",
    "rnd = np.random.default_rng(seed)\n",
    "srs = np.array([0.1, 0.5, 0.9])\n",
    "w_in = Linear(1, dim, bound=0.1, bias=0.0, rnd=rnd)\n",
    "\n",
    "net = ESN(dim, sr=srs[:, None], f=np.tanh, p=1, rnd=rnd)\n",
    "# net = ESN(dim, sr=srs[:, None], f=lambda t: t, p=1, rnd=rnd)  # Linear reservoir.\n",
    "# net.weight[:] = np.roll(np.eye(dim), 1, axis=0)  # Ring topology.\n",
    "\n",
    "x0 = np.zeros((srs.shape[0], dim))\n",
    "us = rnd.uniform(-1, 1, (t_total, 1))\n",
    "\n",
    "x = x0\n",
    "xs = np.zeros((t_total, *x0.shape))\n",
    "for idx in trange(t_total, display=display):\n",
    "    x = net(x, w_in(us[idx]))\n",
    "    xs[idx] = x\n",
    "\n",
    "taus = np.arange(0, 81)\n",
    "r2s = None\n",
    "U, mask, ranks = calc_regression_and_rank(xs[t_washout:].swapaxes(0, -2))\n",
    "for idx, tau in enumerate(tqdm(taus, display=display)):\n",
    "    zeta = us[t_washout - tau : t_total - tau]\n",
    "    r2 = calc_memory_function(U, mask, zeta)[..., 0]\n",
    "    if r2s is None:\n",
    "        r2s = np.zeros((taus.shape[0], *r2.shape))\n",
    "    r2s[idx] = r2\n",
    "mcs = np.sum(r2s, axis=0)\n",
    "labels = []\n",
    "for sr, rank, mc in zip(srs, ranks, mcs, strict=True):\n",
    "    labels.append(f\"SR={sr:.2f}, rank={rank}, MC={mc:.2f}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.plot(taus, r2s, \"o-\", label=labels)\n",
    "ax.set_xlim(taus.min() - 0.5, taus.max() + 0.5)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.legend(\n",
    "    loc=\"upper right\",\n",
    "    borderaxespad=0,\n",
    "    ncol=1,\n",
    "    fontsize=12,\n",
    "    frameon=False,\n",
    ")\n",
    "ax.set_xlabel(r\"$\\tau$\", fontsize=14)\n",
    "ax.set_ylabel(r\"$\\mathrm{MF}[\\tau]$\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今度はESNのスペクトル半径を細かく設定し、記憶容量との関係を調べてみましょう。\n",
    "以下のセルはESNの記憶容量を計算し、階数と記憶容量を同時に表示します (デフォルトではスペクトル半径を 0.0から1.5まで0.05刻みで設定されています)。\n",
    "記憶容量が階数を必ず下回る点、スペクトル半径が1に近づくほど記憶容量が大きくなる点を確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "dim = 50\n",
    "t_washout = 1000\n",
    "t_sample = 20000\n",
    "t_total = t_washout + t_sample\n",
    "display = True\n",
    "\n",
    "rnd = np.random.default_rng(seed)\n",
    "srs = np.linspace(0.0, 1.5, 31)\n",
    "w_in = Linear(1, dim, bound=0.1, bias=0.0, rnd=rnd)\n",
    "\n",
    "net = ESN(dim, sr=srs[:, None], f=np.tanh, p=1, rnd=rnd)\n",
    "# net = ESN(dim, sr=srs[:, None], f=lambda t: t, p=1, rnd=rnd)\n",
    "\n",
    "x0 = np.zeros((srs.shape[0], dim))\n",
    "us = rnd.uniform(-1, 1, (t_total, 1))\n",
    "\n",
    "x = x0\n",
    "xs = np.zeros((t_total, *x0.shape))\n",
    "for idx in trange(t_total, display=display):\n",
    "    x = net(x, w_in(us[idx]))\n",
    "    xs[idx] = x\n",
    "\n",
    "taus = np.arange(0, 81)\n",
    "r2s = None\n",
    "U, mask, ranks = calc_regression_and_rank(xs[t_washout:].swapaxes(0, -2))\n",
    "for idx, tau in enumerate(tqdm(taus, display=display)):\n",
    "    zeta = us[t_washout - tau : t_total - tau]\n",
    "    r2 = calc_memory_function(U, mask, zeta)[..., 0]\n",
    "    if r2s is None:\n",
    "        r2s = np.zeros((taus.shape[0], *r2.shape))\n",
    "    r2s[idx] = r2\n",
    "mcs = np.sum(r2s, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.plot(srs, mcs, \"o-\", label=\"MC\")\n",
    "ax.plot(srs, ranks, \"o-\", color=\"k\", label=\"rank\")\n",
    "ax.set_xlim(srs.min() - 0.05, srs.max() + 0.05)\n",
    "# ax.set_ylim(-0.1, dim + 0.1)\n",
    "ax.legend(\n",
    "    loc=\"upper left\",\n",
    "    borderaxespad=0,\n",
    "    bbox_to_anchor=(1.025, 1.0),\n",
    "    ncol=1,\n",
    "    fontsize=12,\n",
    "    frameon=False,\n",
    ")\n",
    "ax.set_xlabel(r\"SR\", fontsize=14)\n",
    "ax.set_ylabel(r\"$\\mathrm{MC}$\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前の章で学んだ最大リアプノフ指数との関連を調べてみましょう。\n",
    "次のセルはESNの最大リアプノフ指数も同時に計算し、MCとの関係を図示化します。\n",
    "スペクトル半径を0.01刻みで2.0まで200点設定しているため、環境によっては実行に若干時間がかかる点に注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "dim = 100\n",
    "eps = 1e-4\n",
    "t_washout = 1000\n",
    "t_sample = 10000\n",
    "t_total = t_washout + t_sample\n",
    "ts = np.arange(-t_washout, t_sample)\n",
    "display = True\n",
    "\n",
    "rnd = np.random.default_rng(seed)\n",
    "srs = np.linspace(0.01, 2.0, 200)\n",
    "w_in = Linear(1, dim, bound=0.1, bias=0.0, rnd=rnd)\n",
    "\n",
    "net = ESN(dim, sr=srs[:, None], f=np.tanh, p=1, rnd=rnd)\n",
    "# net = ESN(dim, sr=srs[:, None], f=lambda t: t, p=1, rnd=rnd)\n",
    "\n",
    "x0 = np.zeros((2, srs.shape[0], dim))\n",
    "us = rnd.uniform(-1, 1, (t_total, 1))\n",
    "\n",
    "x = x0\n",
    "xs = np.zeros((t_total, *x0.shape[1:]))\n",
    "lmbds = np.zeros((t_sample, srs.shape[0]))\n",
    "for idx, t in enumerate(tqdm(ts, display=display)):\n",
    "    if t == 0:\n",
    "        pert = rnd.uniform(-1, 1, x[0].shape)\n",
    "        pert = pert / np.linalg.norm(pert, axis=-1, keepdims=True)\n",
    "        x[1] = x[0] + pert * eps\n",
    "    x = net(x, w_in(us[idx]))\n",
    "    xs[idx] = x[0]\n",
    "    if t >= 0:\n",
    "        x_org, x_per = x[0], x[1]\n",
    "        x_diff = x_per - x_org\n",
    "        d_post = np.linalg.norm(x_diff, axis=-1, keepdims=True)\n",
    "        lmbd = np.log(np.abs(d_post / eps))\n",
    "        x_per[:] = x_org + x_diff * (eps / d_post)\n",
    "        lmbds[idx - t_washout] = lmbd[..., 0]\n",
    "\n",
    "taus = np.arange(0, 81)\n",
    "r2s = None\n",
    "U, mask, ranks = calc_regression_and_rank(xs[t_washout:].swapaxes(0, -2))\n",
    "for idx, tau in enumerate(tqdm(taus, display=display)):\n",
    "    zeta = us[t_washout - tau : t_total - tau]\n",
    "    r2 = calc_memory_function(U, mask, zeta)[..., 0]\n",
    "    if r2s is None:\n",
    "        r2s = np.zeros((taus.shape[0], *r2.shape))\n",
    "    r2s[idx] = r2\n",
    "mcs = np.sum(r2s, axis=0)\n",
    "\n",
    "\n",
    "def get_maxima_and_minima(xs, **kwargs):\n",
    "    id_maxima = sp.signal.find_peaks(xs, **kwargs)[0]\n",
    "    id_minima = sp.signal.find_peaks(-xs, **kwargs)[0]\n",
    "    return id_maxima, id_minima\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 8), gridspec_kw={\"hspace\": 0.05})\n",
    "axl = ax[0]\n",
    "axl.set_xlim(srs.min() - 0.01, srs.max() + 0.01)\n",
    "axl.set_xticklabels([])\n",
    "for idx, sr in enumerate(srs):\n",
    "    id_maxima, id_minima = get_maxima_and_minima(xs[t_washout:, idx, 0])\n",
    "    id_all = np.concatenate([id_maxima, id_minima])\n",
    "    peaks = xs[t_washout:, idx, 0][id_all]\n",
    "    axl.scatter(sr * np.ones(peaks.shape[0]), peaks, marker=\".\", s=0.01, color=\"k\")\n",
    "axl.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "axl.set_xlabel(r\"$x_0[k]$\", fontsize=14)\n",
    "axl.set_yticks([-1.0, 0.0, 1.0])\n",
    "axl.set_ylim(-1.1, 1.1)\n",
    "\n",
    "axr = ax[0].twinx()\n",
    "axr.plot(srs, lmbds.mean(axis=0), \"o-\", color=\"red\", label=\"MLE\")\n",
    "axr.set_yticks([-0.2, 0.0, 0.2])\n",
    "axr.set_ylim(-0.22, 0.22)\n",
    "axr.set_ylabel(r\"MLE: $\\lambda$\", fontsize=14)\n",
    "axr.set_xticklabels([])\n",
    "axr.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "\n",
    "ax[1].plot(srs, mcs, \"o-\", label=\"MC\")\n",
    "ax[1].set_xlim(srs.min() - 0.01, srs.max() + 0.01)\n",
    "ax[1].set_xlabel(r\"SR\", fontsize=14)\n",
    "ax[1].set_ylabel(r\"$\\mathrm{MC}$\", fontsize=14)\n",
    "ax[1].tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.1. (Advanced)\n",
    "\n",
    "- パラメータを様々に変更して挙動がどう変化するか観察せよ。特に活性化関数とMF/MCとの関係を調査せよ。\n",
    "- 入力分布は上のデモンストレーションでは一様乱数を用いているが、他の分布（例えば正規分布やベルヌーイ分布）を用いても良い。入力分布を変更した場合、MF/MCはどう変化するか観察せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Jaeger, H. (2001). *Short term memory in echo state networks*. GMD Forschungszentrum Informationstechnik. https://doi.org/10.24406/publica-fhg-291107"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rc-bootcamp (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
