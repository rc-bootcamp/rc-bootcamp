{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Information Processing Capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we will learn how to calculate the information processing capacity (IPC), an extended metric of the memory capacity introduced in the previous chapter.\n",
    "As in the previous chapter, we will also explore the relationship between the properties of dynamical systems, such as rank and dynamics, and the resulting IPC.\n",
    "\n",
    "**Note:** In the latter part of this chapter, an environment with GPU support is recommended.\n",
    "If your local PC does not have a GPU, it is recommended to run it on Google Colaboratory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IPC is an extended metric of memory capacity proposed by J. Dambre et al.<sup>[1]</sup>, which evaluates what kind of computations (memory and nonlinearity) a dynamical system performs on input time series.\n",
    "As in the previous chapter, we consider a single-input $N$-dimensional dynamical system $x[k]$ represented by the following equations and the output $\\hat{y}[k]$ obtained through a certain linear mapping $g: \\mathbb{R}^N \\to \\mathbb{R}$:\n",
    "\n",
    "$$\n",
    "\\renewcommand{\\Tau}{\\mathrm{T}}\n",
    "\\renewcommand{\\Zeta}{\\mathrm{Z}}\n",
    "\\begin{align*}\n",
    "x[k+1] &= f \\left(x[k],\\zeta[k+1]\\right) \\\\\n",
    "\\hat{y}[k] &= g(x[k])\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the input time series $\\zeta[k]$ is assumed to be zero-mean, stationary, and i.i.d.\n",
    "Here, we introduce a new capacity $\\mathrm{C}$ defined by the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{C}[x, z] := \\mathrm{R}^2[z, x]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the equation, $\\mathrm{C}(x, z)$ is a metric that quantifies how well the target time series $z$ can be reconstructed from $x$, calculated using the coefficient of determination $\\mathrm{R}^2$, and it takes values in the range [0, 1].\n",
    "The memory function, learned in the previous chapter, is expressed using $\\mathrm{C}$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{MF}[\\tau] &= \\mathrm{C}[x, \\zeta^\\tau]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory capacity $\\mathrm{MC}$ is calculated by summing $\\mathrm{C}$ over all past inputs by varying $\\tau$.\n",
    "In other words, memory capacity can be interpreted as a metric that quantifies how well the **linear** transformation of past inputs $[\\zeta[k], \\zeta[k-1], \\zeta[k-2],~\\ldots]$ can be recovered from the current state $x[k]$ (or its linear transformation).\n",
    "On the other hand, the calculation of IPC evaluates how well reconstruction can be achieved, extending the scope to include **nonlinear** transformations.\n",
    "\n",
    "Now, how should we construct the nonlinear transformation $z$ of past inputs that we aim to evaluate? The points to consider here are the orthogonality and completeness of the target time series, meaning that all patterns must be considered without duplication.\n",
    "This is because allowing linearly dependent targets would make the total capacity arbitrarily large.\n",
    "In the calculation of memory capacity, the simple summation of the values of each memory function $\\mathrm{C}[x, \\zeta^\\tau]$ was valid because the i.i.d. nature of the input was assumed.\n",
    "That is, for $\\tau_1 \\neq \\tau_2$, $\\zeta^{\\tau_1}$ and $\\zeta^{\\tau_2}$ were linearly independent and orthogonal ($\\mathrm{E}[\\zeta^{\\tau_1} \\zeta^{\\tau_2}] = 0$).\n",
    "\n",
    "In the calculation of IPC, a tool called **orthogonal polynomials**<sup>[2]</sup> is used to construct target time series comprehensively without duplication.\n",
    "Orthogonal polynomials have an integer parameter called **degree**.\n",
    "Generally, the larger the value of $d$, the stronger the nonlinearity (conversely, $d=0$ corresponds to a constant, and $d=1$ corresponds to a linear transformation).\n",
    "Orthogonal polynomials of degree $d$ are denoted as $\\mathcal{P}_d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the first-order target time series used in memory capacity can be expressed again in the following form using the first-degree orthogonal polynomial $\\mathcal{P}_1$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{P}_1(\\zeta^0),~\\mathcal{P}_1(\\zeta^1),~\\mathcal{P}_1(\\zeta^2),~\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^5),~\\ldots\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us consider second-order target time series.\n",
    "Here, using the second-degree orthogonal polynomial $\\mathcal{P}_2$, they are enumerated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\mathcal{P}_2(\\zeta^0),~\\mathcal{P}_2(\\zeta^1),~\\mathcal{P}_2(\\zeta^2),~\\mathcal{P}_2(\\zeta^3),~\\mathcal{P}_2(\\zeta^4),~\\mathcal{P}_2(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^1),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^2),~\\mathcal{P}_{1}(\\zeta^0)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^2),~\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^4)\\mathcal{P}_1(\\zeta^5),\\ldots\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, note that as second-order target time series, not only the elements using $\\mathcal{P}_2$ but also the products of $\\mathcal{P}_{1}$ (i.e., $1+1=2$) are included.\n",
    "Since they are orthogonal, all target time series are linearly independent, and the inner product between them is $0$.\n",
    "\n",
    "Third-order target time series are also enumerated similarly.\n",
    "Since there are three patterns of addition that result in a total degree of 3: $3,~2+1, 1+1+1$, the number of considered patterns increases as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\mathcal{P}_3(\\zeta^0),~\\mathcal{P}_3(\\zeta^1),~\\mathcal{P}_3(\\zeta^2),~\\mathcal{P}_3(\\zeta^3),~\\mathcal{P}_3(\\zeta^4),~\\mathcal{P}_3(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_2(\\zeta^0)\\mathcal{P}_1(\\zeta^1),~\\mathcal{P}_2(\\zeta^0)\\mathcal{P}_1(\\zeta^2),~\\mathcal{P}_2(\\zeta^0)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_2(\\zeta^0)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_2(\\zeta^0)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_2(\\zeta^1)\\mathcal{P}_1(\\zeta^2),~\\mathcal{P}_2(\\zeta^1)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_2(\\zeta^1)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_2(\\zeta^1)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_2(\\zeta^2)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_2(\\zeta^2)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_2(\\zeta^2)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_2(\\zeta^3)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_2(\\zeta^3)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_2(\\zeta^4)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^2),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^0)\\mathcal{P}_1(\\zeta^4)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^3),~\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^1)\\mathcal{P}_1(\\zeta^4)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^4),~\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^2)\\mathcal{P}_1(\\zeta^4)\\mathcal{P}_1(\\zeta^5),~\\ldots\\\\\n",
    "&\\mathcal{P}_1(\\zeta^3)\\mathcal{P}_1(\\zeta^4)\\mathcal{P}_1(\\zeta^5),~\\ldots\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combinations continue to increase explosively for $d = 4$ and beyond as well.\n",
    "We generalize these and define a target time series $\\zeta^{D,\\Tau}$ using the following equation, with $D=\\{d_1,d_2,~\\ldots,~d_m\\}$ being a set of integers representing degrees and $\\Tau=\\{\\tau_1, \\tau_2,~\\ldots,~\\tau_m\\}$ being a set of integers representing time delays, both having the same number of elements:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\zeta^{D,\\Tau}[k] :=& \\mathcal{P}_{d_1}(\\zeta[k-\\tau_1])\\mathcal{P}_{d_2}(\\zeta[k-\\tau_2])\\cdots \\mathcal{P}_{d_m}(\\zeta[k-\\tau_m]) \\\\\n",
    "=& \\prod_{i=1}^m \\mathcal{P}_{d_i}(\\zeta[k-\\tau_i])\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $d$-th order IPC $\\mathrm{C}^d$ is defined as the sum of capacities for target time series corresponding to $D$ such that $\\sum_i d_i = d$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{C}^d[x, \\zeta] := \\sum_{\\substack{D~\\mathrm{s.t.}\\\\ \\sum_i d_i=d}} \\sum_{\\Tau} \\mathrm{C}[x, \\zeta^{D,\\Tau}]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the definition, we have $\\mathrm{MC}=\\mathrm{C}^1$.\n",
    "This is why IPC is an extended metric of memory capacity.\n",
    "The total capacity $\\mathrm{C}^\\mathrm{tot}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{C}^\\mathrm{tot}[x, \\zeta] &:= \\sum_{d=1}^{\\infty} \\mathrm{C}^d[x, \\zeta]\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J. Dambre et al.<sup>[1]</sup> demonstrated the following inequality regarding the upper limit of this IPC (derivation is an advanced exercise):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{C}^\\mathrm{tot}[x, \\zeta] \\leq r \\leq N\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $r$ represents the rank of the dynamical system.\n",
    "This indicates that, similar to memory capacity, the upper limit of IPC is constrained by the number of linearly independent dimensions of the internal states.\n",
    "\n",
    "The results derived from this metric provide particularly important insights for physical reservoir computing (PRC).\n",
    "In PRC, the number of sensors typically installed directly corresponds to the dimensionality of the internal states, and the number of linearly independent sensor time series directly represents the upper limit of IPC.\n",
    "Moreover, since IPC comprehensively evaluates what kinds of transformations are performed, it is also useful for assessing the characteristics of the physical system itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises and demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to the exercises and demonstrations.\n",
    "Along with the basic libraries from the previous chapter, you can import and use the `ESN`, `Linear`, and `narma_func` we implemented earlier.\n",
    "Please run the following cell.\n",
    "\n",
    "You can view the implementations of `ESN`, `Linear`, and `narma_func` by uncommenting the lines after `import inspect` or by using `...?? / ??...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import drive  # type: ignore\n",
    "\n",
    "    if False:  # Set to True if you want to use Google Drive and save your work there.\n",
    "        drive.mount(\"/content/gdrive\")\n",
    "        %cd /content/gdrive/My Drive/rc-bootcamp/\n",
    "        # NOTE: Change it to your own path if you put the zip file elsewhere.\n",
    "        # e.g., %cd /content/gdrive/My Drive/[PATH_TO_EXTRACT]/rc-bootcamp/\n",
    "    else:\n",
    "        pass\n",
    "        %cd /content/\n",
    "        !git clone --branch en https://github.com/rc-bootcamp/rc-bootcamp.git\n",
    "        %cd /content/rc-bootcamp/\n",
    "else:\n",
    "    sys.path.append(\".\")\n",
    "\n",
    "from ipc_module.helper import visualize_dataframe\n",
    "from ipc_module.profiler import UnivariateProfiler, UnivariateViewer\n",
    "from utils.reservoir import ESN, Linear\n",
    "from utils.style_config import Figure, plt\n",
    "from utils.tester import load_from_chapter_name\n",
    "from utils.tqdm import tqdm, trange\n",
    "\n",
    "test_func, show_solution = load_from_chapter_name(\"07_information_processing_capacity\")\n",
    "\n",
    "# Uncomment it to see the implementations of `Linear` and `ESN`.\n",
    "# import inspect\n",
    "# print(inspect.getsource(Linear))\n",
    "# print(inspect.getsource(ESN))\n",
    "\n",
    "# Or just use ??.../...?? (uncomment the following lines).\n",
    "# Linear??\n",
    "# ESN??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Legendre polynomials and verification of orthogonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have explained orthogonal polynomials denoted as $\\mathcal{P}$, but let's introduce specific polynomials for discussion.\n",
    "These orthogonal polynomials depend on the distribution of the input time series.\n",
    "For example, if the input time series $\\zeta[k]$ follows a uniform random distribution $\\mathcal{U}([-1, 1])$, we can use the [Legendre polynomials](https://en.wikipedia.org/wiki/Legendre_polynomials) defined by the following recurrence relation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(n+1)\\mathcal{P}_{n+1}(z) &= (2n+1)z\\mathcal{P}_n(z) - n\\mathcal{P}_{n-1}(z)\n",
    ",\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathcal{P}_0(z)=1$ and $\\mathcal{P}_1(z)=z$.\n",
    "Expanding this equation yields the following polynomials:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{P}_0(z) &= 1 \\\\\n",
    "\\mathcal{P}_1(z) &= z \\\\\n",
    "\\mathcal{P}_2(z) &= \\frac{1}{2}(3z^2-1) \\\\\n",
    "\\mathcal{P}_3(z) &= \\frac{1}{2}(5z^3-3z) \\\\\n",
    "\\mathcal{P}_4(z) &= \\frac{1}{8}(35z^4-30z^2+3) \\\\\n",
    "\\mathcal{P}_5(z) &= \\frac{1}{8}(63z^5-70z^3+15z) \\\\\n",
    "\\mathcal{P}_6(z) &= \\frac{1}{48}(231z^6-315z^4+105z^2-5)\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orthogonality, on the other hand, can be evaluated based on the result of inner product calculations.\n",
    "Now, consider two target time series $\\zeta^A := \\zeta^{D_A,\\Tau_A}$ and $\\zeta^B := \\zeta^{D_B,\\Tau_B}$, from which two target time series matrices $\\Zeta^A = [\\zeta^A[0]; \\zeta^A[1];~\\ldots;~\\zeta^A[T-1]] \\in \\mathbb{R}^{T \\times 1}$ and $\\Zeta^B = [\\zeta^B[0]; \\zeta^B[1];~\\ldots;~\\zeta^B[T-1]] \\in \\mathbb{R}^{T \\times 1}$ are extracted for $T$ steps.\n",
    "The inner product $I(\\Zeta^A, \\Zeta^B)$ of $\\Zeta^A$ and $\\Zeta^B$ can be calculated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(\\Zeta^A, \\Zeta^B) &= \\sum_{t=1}^{T} \\frac{\\Zeta^A_t}{\\sqrt{\\sum_{t=1}^T (\\Zeta^A_t)^2}} \\cdot \\frac{\\Zeta^B_t}{\\sqrt{\\sum_{t=1}^T (\\Zeta^B_t)^2}}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected value of $I$ based on orthogonality is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{E}[I(\\Zeta^A, \\Zeta^B)] &= \\begin{cases}\n",
    "1 & \\mathrm{if}~\\zeta^A = \\zeta^B~(\\Leftrightarrow  D_A=D_B \\land \\Tau_A=\\Tau_B) \\\\\n",
    "0 & \\mathrm{if}~\\zeta^A \\perp \\zeta^B~(\\mathrm{otherwise}) \\\\\n",
    "\\end{cases}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement the Legendre polynomials and inner product calculations in the exercises to verify these properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.1\n",
    "\n",
    "Based on the above recurrence relation, complete the method `Legendre._calc` in the class `Legendre` to compute the Legendre polynomial. Note that `Legendre._calc` calculates $\\mathcal{P}_n(\\Zeta)$ for the time series $\\Zeta$. The result will be validated against a time series of length $T$ sampled from a uniform random distribution $\\mathcal{U}([-1, 1])$.\n",
    "\n",
    "- `Legendre._calc`\n",
    "  - Argument(s):\n",
    "    - `n`: `int`\n",
    "      - `n >= 0`\n",
    "  - Operation(s):\n",
    "    - Update `self._cache[n]`\n",
    "- $10 \\leq T \\leq 10^{3}$, $1\\leq n \\leq 20$\n",
    "\n",
    "<details><summary>tips</summary>\n\n",
    "$$\n",
    "\\begin{align*}\n",
    "n\\mathcal{P}_{n}(z) &= (2n-1)z\\mathcal{P}_{n-1}(z) - (n-1)\\mathcal{P}_{n-2}(z) ~\\mathrm{for}~n \\geq 2 \\\\\n",
    ".\\end{align*}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Legendre(object):\n",
    "    def __init__(self, xs):\n",
    "        self.xs = xs\n",
    "        self._caches = {}\n",
    "        self._caches[0] = 1\n",
    "        self._caches[1] = self.xs\n",
    "\n",
    "    def __getitem__(self, deg):\n",
    "        assert deg >= 0\n",
    "        if deg not in self._caches:\n",
    "            self._caches[deg] = self._calc(deg)\n",
    "        return self._caches[deg]\n",
    "\n",
    "    def _calc(self, n: int):\n",
    "        # TODO Use `self.xs` and `self[n-1]`, `self[n-2]` to calculate the n-th Legendre polynomial.\n",
    "        ...\n",
    "\n",
    "\n",
    "def solution(us, n):\n",
    "    # DO NOT CHANGE HERE.\n",
    "    poly = Legendre(us)\n",
    "    return poly[n]\n",
    "\n",
    "\n",
    "test_func(solution, \"01_01\")\n",
    "# show_solution(\"01_01\", \"Legendre\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.2.\n",
    "\n",
    "Based on the above equation, complete the method `calc_inner_product` to calculate the inner product of two time series $A \\in \\mathbb{R}^{T}$ and $B \\in \\mathbb{R}^{T}$.\n",
    "\n",
    "- `calc_inner_product`\n",
    "  - Argument(s):\n",
    "    - `a`: `np.ndarray`\n",
    "      - `shape`: `(t,)`\n",
    "      - `dtype`: `np.float64`\n",
    "  - Return(s):\n",
    "    - `b`: `np.ndarray`\n",
    "      - `shape`: `(t,)`\n",
    "      - `dtype`: `np.float64`\n",
    "- $10 \\leq T \\leq 10^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_inner_product(a, b):\n",
    "    # TODO Calculate and return the inner product between vectors a and b.\n",
    "    ...\n",
    "\n",
    "\n",
    "test_func(calc_inner_product, \"01_02\")\n",
    "# show_solution(\"01_02\")  # Uncomment it to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's verify their orthogonality.\n",
    "First, let's check that we can reproduce the [figure of Legendre polynomials from Wikipedia](https://en.wikipedia.org/wiki/Legendre_polynomials#/media/File:Legendrepolynomials6.svg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Legendre_polynomials\n",
    "us = np.linspace(-1, 1, 1000)\n",
    "poly = Legendre(us)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "for deg in range(1, 6):\n",
    "    ax.plot(us, poly[deg], label=r\"$\\mathcal{P}_\" + f\"{{{deg}}}$\", color=f\"C{deg}\")\n",
    "ax.legend(\n",
    "    loc=\"upper left\",\n",
    "    fontsize=12,\n",
    "    bbox_to_anchor=(1.025, 1.0),\n",
    "    borderaxespad=0,\n",
    "    frameon=False,\n",
    ")\n",
    "ax.tick_params(axis=\"both\", labelsize=12)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, generate a time series using the uniform random distribution $\\mathcal{U}([-1, 1])$, create various target time series using time delay and Legendre polynomials, and calculate the inner products between them.\n",
    "Each element of `degree_delay_list` specifies a pair of degree sequence $D$ and time delay sequence $\\Tau$ in $\\zeta^{D,\\Tau}$.\n",
    "Try changing them in various ways and check if orthogonality is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "t_washout, t_sample = 100, 10000\n",
    "\n",
    "rnd = np.random.default_rng(seed)\n",
    "t_total = t_washout + t_sample\n",
    "us = rnd.uniform(-1, 1, t_total)\n",
    "poly = Legendre(us)\n",
    "\n",
    "degree_delay_list = [\n",
    "    ([1], [0]),\n",
    "    ([1], [10]),\n",
    "    ([1, 1], [1, 2]),\n",
    "    ([2], [0]),\n",
    "    ([3], [5]),\n",
    "]  # You can add or change more combinations if you want.\n",
    "\n",
    "\n",
    "def create_poly(args):\n",
    "    degrees, taus = args\n",
    "    out = 1\n",
    "    for deg, tau in zip(degrees, taus, strict=True):\n",
    "        out *= poly[deg][t_washout - tau : t_total - tau]\n",
    "    return out\n",
    "\n",
    "\n",
    "def create_label(args):\n",
    "    degrees, taus = args\n",
    "    out = r\"$\"\n",
    "    for d, t in zip(degrees, taus, strict=True):\n",
    "        if t == 0:\n",
    "            out += f\"P_{{{d}}}(\\\\zeta)\"\n",
    "        else:\n",
    "            out += f\"P_{{{d}}}(\\\\zeta^{{{t}}})\"\n",
    "    out += r\"$\"\n",
    "    return out\n",
    "\n",
    "\n",
    "length = len(degree_delay_list)\n",
    "polys = list(map(create_poly, degree_delay_list))\n",
    "labels = list(map(create_label, degree_delay_list))\n",
    "products = np.zeros((length, length))\n",
    "\n",
    "for idx, idy in itertools.product(range(length), range(length)):\n",
    "    products[idx, idy] = calc_inner_product(polys[idx], polys[idy])\n",
    "\n",
    "fig = Figure(figsize=(8, 6))\n",
    "ax = fig[0]\n",
    "im, cb = ax.plot_matrix(\n",
    "    products,\n",
    "    cmap=\"Blues\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    aspect=\"equal\",\n",
    "    colorbar=True,\n",
    ")\n",
    "ax.set_xticks(range(length))\n",
    "ax.set_yticks(range(length))\n",
    "ax.set_xticklabels(labels, fontsize=10)\n",
    "ax.set_yticklabels(labels, fontsize=10)\n",
    "cb.ax.tick_params(labelsize=12)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.3 (Advanced)\n",
    "\n",
    "- In the above demo, the value of the inner product $I$ changes depending on the time series length `t_sample`. Confirm that when the time series length is short, the value of $I$ does not approach 0 even if they are orthogonal.\n",
    "- Instead of using the uniform random distribution $\\mathcal{U}([-1, 1])$, when using the standard normal distribution $\\mathcal{N}(0, 1)$, [Hermite polynomials](https://en.wikipedia.org/wiki/Hermite_polynomials#Recurrence_relation) must be used instead<sup>[2]</sup>. Implement the class `Hermite` to calculate Hermite polynomials and similarly verify their orthogonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementation and verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's actually calculate the IPC.\n",
    "First, prepare an ESN with $N=10$ and the activation function $\\tanh$, along with an input time series $\\zeta[k]$ that follows the uniform random distribution $\\mathcal{U}([-1, 1])$, and sample its dynamics $x[k]$.\n",
    "To ensure asymmetry, $\\zeta[k]$ is scaled to take values in the range $[0, 1]$.\n",
    "(The input is made asymmetric because $\\tanh$ is an odd function, and for symmetric input, only the odd-order components of the capacity appear [verification is an advanced task])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5678\n",
    "dim = 10\n",
    "t_washout = 1000\n",
    "t_sample = 1000000\n",
    "t_total = t_washout + t_sample\n",
    "display = True\n",
    "\n",
    "rnd = np.random.default_rng(seed)\n",
    "w_in = Linear(1, dim, bound=0.1, bias=0.0, rnd=rnd)\n",
    "net = ESN(dim, sr=0.1, f=np.tanh, p=1, rnd=rnd)\n",
    "\n",
    "x0 = np.zeros((dim,))\n",
    "us = rnd.uniform(-1, 1, (t_total, 1))\n",
    "\n",
    "x = x0\n",
    "xs = np.zeros((t_total, *x0.shape))\n",
    "for idx in trange(t_total, display=display):\n",
    "    x = net(x, w_in(0.5 * us[idx] + 0.5))\n",
    "    # x = net(x, w_in(us[idx]))  # Uncomment it for the symmetric case.\n",
    "    xs[idx] = x\n",
    "\n",
    "print(\"us:\", us.shape)\n",
    "print(\"xs:\", xs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the memory function calculations covered in the previous chapter, IPC is also computed using SVD.\n",
    "Thus, the same code can be reused (if you haven't reviewed it, please refer back to the previous chapter).\n",
    "The following `calc_regression_and_rank` and `calc_capacity` functions are reused implementations from the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_regression_and_rank(X):\n",
    "    T, N = X.shape[-2:]\n",
    "    X = X - X.mean(axis=-2, keepdims=True)\n",
    "    U, sigma, _V = np.linalg.svd(X, full_matrices=False)\n",
    "    eps = np.finfo(X.dtype).eps\n",
    "    sigma_sq_max = np.max(sigma * sigma, axis=-1, keepdims=True)\n",
    "    eps = sigma_sq_max * (eps * max(T, N))\n",
    "    mask = sigma > eps\n",
    "    rank = mask.sum(axis=-1)\n",
    "    return U, mask, rank\n",
    "\n",
    "\n",
    "def calc_capacity(U, mask, zeta):\n",
    "    uzeta = U.swapaxes(-2, -1) @ zeta\n",
    "    dot = ((uzeta * uzeta) * mask[..., None]).sum(axis=-2)\n",
    "    var = (zeta * zeta).sum(axis=-2)\n",
    "    r2 = dot / var\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's perform SVD and check the rank $r$.\n",
    "The rank serves as the upper limit of $\\mathrm{C}^\\mathrm{tot}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, mask, rank = calc_regression_and_rank(xs[t_washout:])\n",
    "print(\"rank:\", rank)\n",
    "\n",
    "poly = Legendre(us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of first-order capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's calculate the first-order capacity $C^1$.\n",
    "Specifically, since $\\mathcal{P}_1(z) = z$, the first-order target time series can be calculated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\zeta^{0}, \\zeta^{1}, \\zeta^{2}, \\zeta^{3},~\\ldots\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay1_max = 10\n",
    "\n",
    "taus = np.arange(0, delay1_max + 1)\n",
    "c1 = np.zeros(len(taus))\n",
    "for idx, tau in enumerate(tqdm(taus)):\n",
    "    zeta = poly[1][t_washout - tau : t_total - tau]\n",
    "    c1[idx] = calc_capacity(U, mask, zeta)[..., 0]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.plot(taus, c1, label=r\"$\\zeta^1$\", color=\"C0\", marker=\"o\")\n",
    "ax.set_xlim(-0.5, delay1_max + 0.5)\n",
    "ax.set_xlabel(r\"$\\tau$\", fontsize=14)\n",
    "ax.set_ylabel(r\"$\\mathrm{C}[x,\\zeta^\\tau]$\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "ax.set_title(r\"$\\mathrm{C}^1$=\" + f\"{c1.sum():.3f}\", fontsize=14)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of second-order capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the second order, there are two possible combinations: $D=\\{1,1\\}$ and $D=\\{2\\}$.\n",
    "Using $\\tau_1$ and $\\tau_2$ such that $\\tau_1 \\leq \\tau_2$, we can comprehensively prepare the orthogonal polynomials $z^{\\tau_1, \\tau_2}$ in the following form:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z^{\\tau_1, \\tau_2} = \\begin{cases}\n",
    "\\mathcal{P}_2(\\zeta^{\\tau_1}) &= \\frac{3}{2}(\\zeta^{\\tau_1})^2 - \\frac{1}{2} & \\mathrm{if}~\\tau_1 = \\tau_2 \\\\\n",
    "\\mathcal{P}_1(\\zeta^{\\tau_1})\\mathcal{P}_1(\\zeta^{\\tau_2}) &= \\zeta^{\\tau_1} \\zeta^{\\tau_2} & \\mathrm{if}~\\tau_1 \\leq \\tau_2 \\\\\n",
    "\\end{cases}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay2_max = 8\n",
    "\n",
    "taus = np.arange(0, delay2_max + 1)\n",
    "c2 = np.zeros((len(taus), len(taus)))\n",
    "\n",
    "cands = list(itertools.product(enumerate(taus), repeat=2))\n",
    "for (idx, tau1), (idy, tau2) in tqdm(cands):\n",
    "    if not (idx <= idy):\n",
    "        continue\n",
    "    if idx == idy:\n",
    "        zeta = poly[2][t_washout - tau1 : t_total - tau1]\n",
    "    else:\n",
    "        zeta1 = poly[1][t_washout - tau1 : t_total - tau1]\n",
    "        zeta2 = poly[1][t_washout - tau2 : t_total - tau2]\n",
    "        zeta = zeta1 * zeta2\n",
    "    c2[idx, idy] = calc_capacity(U, mask, zeta)[..., 0]\n",
    "\n",
    "fig = Figure(figsize=(8, 6))\n",
    "ax = fig[0]\n",
    "im, cb = ax.plot_matrix(\n",
    "    c2,\n",
    "    x=taus,\n",
    "    y=taus,\n",
    "    cmap=\"viridis\",\n",
    "    zscale=\"log\",\n",
    "    vmax=1,\n",
    "    vmin=1e-3,\n",
    "    aspect=\"equal\",\n",
    "    colorbar=True,\n",
    "    xticks_kws=dict(num_tick=len(taus)),\n",
    "    yticks_kws=dict(num_tick=len(taus)),\n",
    ")\n",
    "ax.grid(False)\n",
    "ax.set_xlim(-1, len(taus))\n",
    "ax.set_ylim(-1, len(taus))\n",
    "ax.set_xlabel(r\"$\\tau_2$\", fontsize=14)\n",
    "ax.set_ylabel(r\"$\\tau_1$\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "cb.ax.tick_params(labelsize=12)\n",
    "cb.set_label(r\"$\\mathrm{C}[x,z^{\\tau_1,\\tau_2}]$\", fontsize=14)\n",
    "ax.set_title(r\"$\\mathrm{C}^2$=\" + f\"{c2.sum():.3f}\", fontsize=14)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of third-order capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the third order, there are three possible combinations: $D=\\{1,1,1\\}$, $D=\\{2, 1\\}$, and $D=\\{3\\}$.\n",
    "Similar to the second-order case, we prepare $\\tau_1, \\tau_2, \\tau_3$ such that $\\tau_1 \\leq \\tau_2 \\leq \\tau_3$ and construct the third-order target time series $z^{\\tau_1, \\tau_2, \\tau_3}$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z^{\\tau_1, \\tau_2, \\tau_3} = \\begin{cases}\n",
    "\\mathcal{P}_3(\\zeta^{\\tau_1}) &= \\frac{5}{2}(\\zeta^{\\tau_1})^3 - \\frac{3}{2}\\zeta^{\\tau_1} & \\mathrm{if}~\\tau_1 = \\tau_2 = \\tau_3 \\\\\n",
    "\\mathcal{P}_2(\\zeta^{\\tau_1})\\mathcal{P}_1(\\zeta^{\\tau_3}) &= \\left(\\frac{3}{2}(\\zeta^{\\tau_1})^2 - \\frac{1}{2}\\right)\\zeta^{\\tau_3} & \\mathrm{if}~\\tau_1 = \\tau_2 < \\tau_3 \\\\\n",
    "\\mathcal{P}_1(\\zeta^{\\tau_1})\\mathcal{P}_2(\\zeta^{\\tau_2}) &= \\zeta^{\\tau_1}\\left(\\frac{3}{2}(\\zeta^{\\tau_2})^2 - \\frac{1}{2}\\right) & \\mathrm{if}~\\tau_1 < \\tau_2 = \\tau_3 \\\\\n",
    "\\mathcal{P}_1(\\zeta^{\\tau_1})\\mathcal{P}_1(\\zeta^{\\tau_2})\\mathcal{P}_1(\\zeta^{\\tau_3}) &= \\zeta^{\\tau_1}\\zeta^{\\tau_2}\\zeta^{\\tau_3} & \\mathrm{if}~\\tau_1 < \\tau_2 < \\tau_3 \\\\\n",
    "\\end{cases}\n",
    ".\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay3_max = 6\n",
    "\n",
    "taus = np.arange(0, delay3_max + 1)\n",
    "c3 = np.zeros((len(taus), len(taus), len(taus)))\n",
    "\n",
    "cands = list(itertools.product(enumerate(taus), repeat=3))\n",
    "for (idx, tau1), (idy, tau2), (idz, tau3) in tqdm(cands):\n",
    "    if not (idx <= idy <= idz):\n",
    "        continue\n",
    "    if idx == idy == idz:\n",
    "        zeta = poly[3][t_washout - tau1 : t_total - tau1]\n",
    "    elif idx == idy:\n",
    "        zeta1 = poly[2][t_washout - tau1 : t_total - tau1]\n",
    "        zeta2 = poly[1][t_washout - tau3 : t_total - tau3]\n",
    "        zeta = zeta1 * zeta2\n",
    "    elif idy == idz:\n",
    "        zeta1 = poly[1][t_washout - tau1 : t_total - tau1]\n",
    "        zeta2 = poly[2][t_washout - tau2 : t_total - tau2]\n",
    "        zeta = zeta1 * zeta2\n",
    "    else:\n",
    "        zeta1 = poly[1][t_washout - tau1 : t_total - tau1]\n",
    "        zeta2 = poly[1][t_washout - tau2 : t_total - tau2]\n",
    "        zeta3 = poly[1][t_washout - tau3 : t_total - tau3]\n",
    "        zeta = zeta1 * zeta2 * zeta3\n",
    "    c3[idx, idy, idz] = calc_capacity(U, mask, zeta)[..., 0]\n",
    "\n",
    "\n",
    "num_col = math.ceil(len(taus) / 2)\n",
    "grid_size = (2, num_col)\n",
    "fig = Figure(figsize=(grid_size[1] * 3, grid_size[0] * 3))\n",
    "fig.create_grid(*grid_size, hspace=0.35, wspace=0.3)\n",
    "\n",
    "for pos in range(len(taus)):\n",
    "    ax = fig[pos // num_col, pos % num_col]\n",
    "    res = ax.plot_matrix(\n",
    "        c3[pos],\n",
    "        x=taus,\n",
    "        y=taus,\n",
    "        cmap=\"viridis\",\n",
    "        zscale=\"log\",\n",
    "        vmax=1,\n",
    "        vmin=1e-3,\n",
    "        aspect=\"equal\",\n",
    "        colorbar=len(taus) == (pos + 1),\n",
    "        xticks_kws=dict(num_tick=len(taus)),\n",
    "        yticks_kws=dict(num_tick=len(taus)),\n",
    "    )\n",
    "    ax.grid(False)\n",
    "    ax.set_xlim(-1, len(taus))\n",
    "    ax.set_ylim(-1, len(taus))\n",
    "    if pos % num_col == 0:\n",
    "        ax.set_ylabel(r\"$\\tau_2$\", fontsize=14)\n",
    "    if (pos // num_col) == grid_size[0] - 1:\n",
    "        ax.set_xlabel(r\"$\\tau_3$\", fontsize=14)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax.set_title(r\"$\\tau_1$=\" + f\"{taus[pos]}\", fontsize=14)\n",
    "    if len(taus) == (pos + 1):\n",
    "        cb = res[1]\n",
    "        cb.ax.set_position([0.9, 0.1, 0.03, 0.8])\n",
    "        cb.ax.tick_params(labelsize=12)\n",
    "        cb.set_label(r\"$\\mathrm{C}[x,z^{\\tau_1,\\tau_2,\\tau_3}]$\", fontsize=14)\n",
    "if len(taus) < (grid_size[0] * grid_size[1]):\n",
    "    for pos in range(len(taus), grid_size[0] * grid_size[1]):\n",
    "        fig.delaxes(fig[pos // num_col, pos % num_col])\n",
    "fig.suptitle(r\"$\\mathrm{C}^3$=\" + f\"{c3.sum():.3f}\", fontsize=16)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we sum up the calculated $\\mathrm{C}^1, \\mathrm{C}^2, \\mathrm{C}^3$ to compute the overall IPC $\\mathrm{C}^\\mathrm{tot}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_tot = np.sum(c1) + np.sum(c2) + np.sum(c3)\n",
    "print(\"total_capacity\", c_tot, \"rank\", rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.1. (Advanced)\n",
    "\n",
    "- Read reference [1] and verify the derivation of $\\mathrm{C}^\\mathrm{tot} \\leq r$.\n",
    "- Verify that the even-order components of $\\mathrm{C}^d$ (particularly $d=2$) vanish when the input is made symmetric. Also, consider and explain the reason for this.\n",
    "- Change the activation function to an even function and similarly verify its behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.2. (Advanced)\n",
    "\n",
    "- Implement code to efficiently output a set of degrees $D=\\{d_i\\}_i$ such that $\\sum_{i} d_i = d$ for a given integer $d$ (Cf. [Young tableau](https://en.wikipedia.org/wiki/Young_tableau)).\n",
    "    - Alternatively, you may use the [`make_degree_list`](https://github.com/rc-bootcamp/ipc-module/blob/main/src/ipc_module/helper.py#L60) implemented in the `ipc-module` mentioned later.\n",
    "    - It can be imported and used with `from ipc_module.helper import make_degree_list`.\n",
    "- Implement code to enumerate all possible combinations of time delays $\\Tau=\\{\\tau_i\\}_i$ within the range of $\\tau_\\mathrm{max} \\geq 1$, given a set of degrees $D$ and a maximum time delay $\\tau_\\mathrm{max}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fast IPC computation using libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environmental setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As confirmed so far, calculating the IPC requires an exhaustive search over combinations of orthogonal polynomials and time delays, leading to an explosive increase in computational cost as the degree grows.\n",
    "Additionally, as the degree increases, the number of ways to partition the degree grows exponentially, making it very challenging to implement each case individually as done in the previous section (it is known that the general term for the number of partitions asymptotically approaches $p(n)\\sim\\frac{1}{4\\sqrt{3}n}e^{\\pi\\sqrt{\\frac{2n}{3}}}$<sup>[3]</sup>).\n",
    "Therefore, in this exercise, we will learn how to calculate IPC using the library [`ipc-module`](https://rc-bootcamp.github.io/ipc-module/), which was developed within the laboratory.\n",
    "\n",
    "`ipc-module` supports GPU-accelerated tensor computation libraries such as `pytorch` and `cupy` in addition to `numpy`, enabling more efficient calculations compared to CPU-only operations.\n",
    "It also provides functions for organizing and visualizing results, allowing you to easily examine the IPC of a given dynamical system.\n",
    "It is [published on PyPI](https://pypi.org/project/ipc-module/) and can be installed via `pip install ipc-module`, but in this notebook, we directly include the source code to make it easier to check and modify locally.\n",
    "For specific implementations, refer to the Python code in the `./ipc_module` folder.\n",
    "\n",
    "Note that the following code will run on a CPU as is, but it will take considerable time.\n",
    "Therefore, if you have access to a GPU environment, **using a GPU is strongly recommended** (if you don't have one locally, we recommend running it on Google Colaboratory).\n",
    "In that case, follow the guide below for additional setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Preparations for calculations in a GPU environment</summary>\n",
    "\n",
    "`pytorch` is very convenient, so the following explains how to set it up.\n",
    "\n",
    "1. In an online environment (Google Colaboratory):\n",
    "\n",
    "   Even with the free version, GPU can be used by default, and `pytorch` is already installed, so no additional setup is required. However, you can verify that the GPU is enabled using the following steps:\n",
    "   \"Edit\" > \"Notebook settings\" > \"Hardware accelerator\" > \"GPU\"\n",
    "\n",
    "2. In a local environment (for uv):\n",
    "\n",
    "   For NVIDIA GPUs, first install the drivers.\n",
    "   You can check if they are installed using the `nvidia-smi` command.\n",
    "   If not installed, you can download them from the official [distribution page](https://www.nvidia.com/en-us/drivers/).\n",
    "   Then, you can install them using the following command:\n",
    "   ```bash\n",
    "   uv sync --extra gpu\n",
    "   ```\n",
    "   This will automatically start the installation of `pytorch`.\n",
    "    ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of basic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once ready, let's perform calculations using `ipc_module`.\n",
    "First, prepare an ESN with $N=50$ dimensions, vary the spectral radius from 0.1 to 1.7 in increments of 0.1, and simultaneously sample its dynamics.\n",
    "As before, prepare an input time series $\\zeta$ from a uniform random distribution $\\mathcal{U}([-1, 1])$, scale it to the range $[0, 1]$ to make it asymmetric, and input it into the ESN.\n",
    "(The memory requirement is large, so if you encounter a memory shortage error, reduce `t_sample` or `dim` as needed.\n",
    "In general, longer sample lengths lead to higher calculation accuracy.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5678\n",
    "dim = 50\n",
    "t_washout = 10000\n",
    "t_sample = 100000\n",
    "srs = np.linspace(0.1, 1.7, 17)\n",
    "t_total = t_washout + t_sample\n",
    "display = True\n",
    "\n",
    "rnd = np.random.default_rng(seed)\n",
    "w_in = Linear(1, dim, bound=0.1, bias=0.0, rnd=rnd)\n",
    "net = ESN(dim, sr=srs[:, None], f=np.tanh, p=1, rnd=rnd)\n",
    "\n",
    "x0 = np.zeros((srs.shape[0], dim))\n",
    "us = rnd.uniform(-1, 1, (t_total, 1))\n",
    "\n",
    "x = x0\n",
    "xs = np.zeros((t_total, *x0.shape))\n",
    "for idx in trange(t_total, display=display):\n",
    "    x = net(x, w_in(0.5 * us[idx] + 0.5))\n",
    "    xs[idx] = x\n",
    "\n",
    "print(\"us:\", us.shape)\n",
    "print(\"xs:\", xs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`UnivariateProfiler`](https://rc-bootcamp.github.io/ipc-module/profiler/#ipc_module.profiler.UnivariateProfiler) is a class equipped with various methods for calculating IPC.\n",
    "You pass a one-dimensional input time series and the corresponding state time series, and then calculate the IPC within the specified range of degrees and time delays using [`UnivariateProfiler.calc`](https://rc-bootcamp.github.io/ipc-module/profiler/#ipc_module.profiler.UnivariateProfiler.calc).\n",
    "\n",
    "<details><summary> Details of the arguments</summary>\n",
    "\n",
    "- `us`: `np.ndarray | torch.Tensor | cupy.ndarray`\n",
    "    - Input time series\n",
    "    - Must have the shape `(t, ..., 1)`\n",
    "- `xs`: `np.ndarray | torch.Tensor | cupy.ndarray`\n",
    "    - Corresponding state time series\n",
    "    - Must have the shape `(t, ..., N)`\n",
    "- `poly_name`: `str`\n",
    "    - Name of the polynomial to use\n",
    "        - [`Legendre`](https://rc-bootcamp.github.io/ipc-module/polynomial/#ipc_module.polynomial.Legendre): Legendre polynomial\n",
    "        - [`Hermite`](https://rc-bootcamp.github.io/ipc-module/polynomial/#ipc_module.polynomial.Hermite): Hermite polynomial\n",
    "        - [`GramSchmidt`](https://rc-bootcamp.github.io/ipc-module/polynomial/#ipc_module.polynomial.GramSchmidt): Polynomial expansion using the Gram-Schmidt method\n",
    "- `offset`: `int`\n",
    "    - Time delay offset (specifies the index where $t=0$)\n",
    "    - Default is 0\n",
    "- `surrogate_num`: `int`\n",
    "    - Number of surrogate samples\n",
    "    - Default is 1000\n",
    "- `surrogate_seed`: `int`\n",
    "    - Seed for surrogate samples\n",
    "    - Default is 0\n",
    "- `axis1`: `int`\n",
    "    - Axis corresponding to the time dimension\n",
    "    - The same axis is used for both `us` and `xs`\n",
    "    - Default is 0\n",
    "- `axis2`: `int`\n",
    "    - Axis corresponding to the state dimension\n",
    "    - The same axis is used for both `us` and `xs`\n",
    "    - Default is -1\n",
    "</details>\n",
    "\n",
    "Let’s try using it.\n",
    "First, create an instance of the `UnivariateProfiler` class, named `profiler`.\n",
    "If you are in an environment where a GPU cannot be used, set `use_gpu` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True  # NOTE: Set it to False to run on CPU.\n",
    "\n",
    "if use_gpu:\n",
    "    import torch\n",
    "\n",
    "    assert torch.cuda.is_available(), \"CUDA is not available\"\n",
    "    us_c = torch.from_numpy(us).cuda()\n",
    "    xs_c = torch.from_numpy(xs).cuda()\n",
    "    args = (us_c, xs_c)\n",
    "else:\n",
    "    args = (us, xs)\n",
    "\n",
    "profiler = UnivariateProfiler(\n",
    "    *args,\n",
    "    \"Legendre\",\n",
    "    offset=t_washout,\n",
    "    surrogate_num=1000,\n",
    "    axis1=0,\n",
    "    axis2=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple explanation of the algorithm:\n",
    "1. When creating an instance of the `UnivariateProfiler` class, the state time series provided as the second argument (in this case, `xs`) is normalized, followed by performing SVD and simultaneously measuring the rank (the implementation is almost the same as `calc_regression_and_rank`).\n",
    "2. Immediately afterward, shuffled indices to be used for surrogate data (described later) are generated (as specified by `surrogate_num`).\n",
    "3. The `UnivariateProfiler.calc` method is executed to calculate the IPC within the specified range of degrees and time delays based on the SVD results. The calculation of the target time series (and the orthogonal polynomials required for its construction) is done lazily, meaning it is computed only when needed, and the results are cached.\n",
    "\n",
    "Since steps 1 and 2 have already been completed in the previous cell, let’s execute the `UnivariateProfiler.calc` method in the following cell to calculate the first-order capacity $C^1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.calc(1, 1001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, $\\Tau \\in \\{\\{0\\}, \\{1\\}, \\{2\\},~\\ldots,~\\{1000\\}\\}$, that is, for each target time series $z \\in \\{\\zeta^0, \\zeta^1, \\zeta^2,~\\ldots,~\\zeta^{1000}\\}$, $\\mathrm{C}[x,z]$ is being calculated (`zero_offset=False` is an option specifying 1-based indexing).\n",
    "The calculation results can be retrieved in the form of `profiler[key]` by specifying the degree $D$ (note that the content of `key` is a `tuple`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays, ipc, surr = profiler[(1,)]\n",
    "\n",
    "print(\"delays:\", *delays[:3], \"...\", *delays[-3:])\n",
    "print(\"ipc:\", ipc.shape)\n",
    "print(\"surr:\", surr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, `profiler` holds three pieces of information for each `key`.\n",
    "The first, `delays`, is a list of $\\Tau$.\n",
    "`ipc` stores the calculated $\\mathrm{C}[x,z]$ in the form of a `np.array`, where the SR corresponds to the second axis.\n",
    "`surr` represents $\\mathrm{C}[x,z]$ for the surrogate data calculated simultaneously.\n",
    "First, let's check the contents of `ipc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 301\n",
    "\n",
    "fig = Figure(figsize=(8, 6))\n",
    "ax = fig[0]\n",
    "im, cb = ax.plot_matrix(\n",
    "    ipc[:time_step, :, 0],\n",
    "    y=np.array(delays)[:time_step, 0],\n",
    "    x=srs,\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"jet\",\n",
    "    vmin=1e-4,\n",
    "    vmax=1,\n",
    "    zscale=\"log\",\n",
    "    yticks_kws=dict(num_tick=4),\n",
    "    xticks_kws=dict(num_tick=3),\n",
    ")\n",
    "ax.set_xlabel(\"SR\", fontsize=14)\n",
    "ax.set_ylabel(r\"$\\tau$\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "cb.ax.tick_params(labelsize=12)\n",
    "cb.set_label(r\"$\\mathrm{C}[x,\\zeta^\\tau]$\", fontsize=14)\n",
    "ax.set_title(r\"$D=\\{1\\}$\", fontsize=14)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is none other than the memory function learned in the previous chapter.\n",
    "This graph displays the color range on a logarithmic scale with a minimum value of $10^{-4}$.\n",
    "Even when $\\tau$ is sufficiently large, the values do not become completely zero but remain slightly small.\n",
    "This phenomenon is called **spurious correlation**, which often arises due to the constraints of numerical computations that can only handle finite data.\n",
    "The values of capacity $\\mathrm{C}$ in regions suspected to be spurious correlation are very small, but they can have a non-negligible impact in calculating IPC, since a large number of $\\mathrm{C}$ types must be summed (e.g., $\\mathrm{C}^\\mathrm{tot}>r$ may occur).\n",
    "\n",
    "**Surrogate data** is used to distinguish between \"significant\" components and \"insignificant\" spurious correlations.\n",
    "Surrogate data is generated by shuffling the original time series, preserving the statistical properties while eliminating temporal dependencies.\n",
    "Here, surrogate data is prepared in the number specified by `surrogate_num`, and the capacity $\\mathrm{C}$ is measured in the same way, with the maximum value used as the threshold.\n",
    "Only components exceeding this threshold are considered distinguishable from random noise.\n",
    "This method is known as the random-shuffle method<sup>[4]</sup>, originally introduced in reference [5].\n",
    "For 1000 data points, this can be regarded as a test with a significance level of $1/1000=0.001$.\n",
    "Let's examine the surrogate data for the component with SR=1.0 as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_id = 9  # 9 is the index of the SR = 1.0 in `srs`.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "for value in surr[:, sr_id, :]:\n",
    "    ax.line_y(value, color=\"#333333\", alpha=0.5, lw=0.1)\n",
    "ax.line_y(surr[:, sr_id, :].max(), color=\"red\", lw=1)\n",
    "ax.plot(np.arange(0, 1001), ipc[:, 9, 0], lw=1)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylim([None, 1e-2])  # Comment it out to zoom out.\n",
    "ax.set_xlim([0, 1000])\n",
    "ax.set_ylabel(r\"$\\mathrm{C}[x,\\zeta^\\tau]$\", fontsize=14)\n",
    "ax.set_xlabel(r\"$\\tau$\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "ax.set_title(f\"SR={srs[sr_id]:.2f}\", fontsize=14)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gray lines represent the capacity $\\mathrm{C}$ for each surrogate data, and the red line shows the maximum value among them.\n",
    "In `ipc_module`, the maximum value of the surrogate data is used as a reference, and a constant multiple of it is set as the threshold to extract significant components (the scaling is often needed because stricter criteria are required when the number of target time series is large).\n",
    "Let's set the threshold using the surrogate data and redraw the previous graph.\n",
    "The components below the threshold should appear as hollow.\n",
    "At the same time, vary the constant multiplier `max_scale` to see how the graph changes with the threshold size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 301\n",
    "max_scale = 1.0\n",
    "\n",
    "ipc_trunc = ipc * (ipc > surr.max(axis=0, keepdims=True) * max_scale)\n",
    "fig = Figure(figsize=(8, 6))\n",
    "ax = fig[0]\n",
    "im, cb = ax.plot_matrix(\n",
    "    ipc_trunc[:time_step, :, 0],\n",
    "    y=np.array(delays)[:time_step, 0],\n",
    "    x=srs,\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"jet\",\n",
    "    vmin=1e-4,\n",
    "    vmax=1,\n",
    "    zscale=\"log\",\n",
    "    yticks_kws=dict(num_tick=4),\n",
    "    xticks_kws=dict(num_tick=3),\n",
    ")\n",
    "ax.set_xlabel(\"SR\", fontsize=14)\n",
    "ax.set_ylabel(r\"$\\tau$\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "cb.ax.tick_params(labelsize=12)\n",
    "cb.set_label(r\"$\\mathrm{C}[x,\\zeta^\\tau]$\", fontsize=14)\n",
    "ax.set_title(r\"$D=\\{1\\}$\" + f\", scale={max_scale:.2f}\", fontsize=14)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have calculated and examined the details for $d=1$ as an example, let’s perform similar calculations for other degrees.\n",
    "The next cell calculates the capacity for $d=2,3,4,5$.\n",
    "For each, $\\tau_\\mathrm{max}=300,50,30,15$ is specified.\n",
    "For example, when $D=\\{1,1\\}$, the combinations of $\\{\\tau_1, \\tau_2\\}$ that are candidates for $\\Tau$ are $\\{0,1\\}, \\{0,2\\},~\\ldots,~\\{0,300\\}, \\{1,2\\}, \\{1,3\\}, \\{1,4\\},~\\ldots,~\\{1,300\\},~\\ldots,~\\{299,300\\}$.\n",
    "In this way, all combinations of time delays below $\\tau_\\mathrm{max}=300$ are exhaustively calculated.\n",
    "Since the number of combinations is large, this may take some time, so please wait patiently.\n",
    "Once the calculation is complete, the list of calculated $D$ will be displayed (you can check it with `profiler.keys()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [2, 3, 4, 5]\n",
    "taus = [300, 50, 30, 15]\n",
    "for deg, tau in zip(degrees, taus, strict=True):\n",
    "    profiler.calc(deg, tau + 1)\n",
    "\n",
    "print(profiler.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the above cell completes, let's save the calculation results.\n",
    "In general, calculating IPC takes considerable time, so it is advisable to save results periodically to avoid losing them.\n",
    "Using the [`UnivariateProfiler.save`](https://rc-bootcamp.github.io/ipc-module/profiler/#ipc_module.profiler.UnivariateViewer.save) method, results can be saved to a file in either `npz` or `pkl` format (`npz` is recommended for better compression and easier verification).\n",
    "The format is determined by the file extension.\n",
    "You can include additional information to save using `**kwargs`.\n",
    "Here, let's also save the spectral radii `srs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.save(\"./result/ipc_asym.npz\", srs=srs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`UnivariateViewer`](https://rc-bootcamp.github.io/ipc-module/profiler/#ipc_module.profiler.UnivariateViewer) class is used to retrieve data.\n",
    "`UnivariateViewer` is the parent class of `UnivariateProfiler` and can retrieve results in the same way as `UnivariateProfiler`.\n",
    "However, note that it does not retain the original time series or the results of the SVD, so additional calculations cannot be performed (the `calc` function cannot be called).\n",
    "In the cell below, confirm that the same results can be obtained using the `UnivariateViewer` instance `viewer` as when using `profiler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = UnivariateViewer(\"./result/ipc_asym.npz\")\n",
    "srs = viewer.info[\"srs\"]  # NOTE: Keyword options are stored on `info`.\n",
    "print(viewer.keys())\n",
    "delays, ipc, surr = viewer[(1,)]\n",
    "\n",
    "print(\"delays:\", *delays[:3], \"...\", *delays[-3:])\n",
    "print(\"ipc:\", ipc.shape)\n",
    "print(\"surr:\", surr.shape)\n",
    "print(\"srs:\", srs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data visualization and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the case of $d=1$, visualization for higher orders is not straightforward.\n",
    "For $d=2$, as handled in the previous section, $C$ could be visualized as a color map on the $(\\tau_1, \\tau_2)$ plane.\n",
    "However, for higher orders, more graphs are required, making it challenging.\n",
    "To address this, `ipc_module` provides several functions for visualization.\n",
    "The [`visualize_dataframe`](https://rc-bootcamp.github.io/ipc-module/helper/#ipc_module.helper.visualize_dataframe) function, used in the next cell, is a function that visualizes the total capacity as a bar graph (using degree/component as groups).\n",
    "\n",
    "<details><summary> Details of the arguments</summary>\n",
    "\n",
    "- `ax`: `Axes`\n",
    "    - The Axes to draw on\n",
    "- `df`: `polars.DataFrame`\n",
    "    - The DataFrame to visualize\n",
    "- `ranks`: `Any | None`\n",
    "    - A list of ranks\n",
    "    - If provided, the values are filtered accordingly\n",
    "- `xticks`: `Any | None`\n",
    "    - Values for the x-axis\n",
    "- `group_by`: `str`\n",
    "    - The grouping method for visualization\n",
    "        - `degree`: Grouped by degree $d$\n",
    "        - `component`: Grouped by the set of degrees $D$\n",
    "        - `detail`: Grouped by the set of degrees $D$ and the set of time delays $\\Tau$ (Note: visualization may take time if `threshold` is not specified!)\n",
    "- `threshold`: `float`\n",
    "    - Threshold for components summarized as `rest`\n",
    "- `sort_by`: `Any`\n",
    "    - Sorting method\n",
    "        - `np.nanmax`: Sort by maximum value\n",
    "        - `np.nanmean`: Sort by mean value\n",
    "        - `np.nansum`: Sort by total value\n",
    "- `cmap`: `str`\n",
    "    - Color map\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, rank = viewer.to_dataframe(max_scale=2.0)  # NOTE: Threshold is scaled by max_scale.\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), gridspec_kw=dict(hspace=0.5))\n",
    "\n",
    "for idx, group_by in enumerate([\"degree\", \"component\"]):\n",
    "    ax = axes[idx]\n",
    "    visualize_dataframe(\n",
    "        ax,\n",
    "        df,\n",
    "        xticks=srs,\n",
    "        threshold=0.1,\n",
    "        cmap=\"tab10\",\n",
    "        group_by=group_by,  # NOTE: Either \"degree\" or \"component\" are available.\n",
    "        fontsize=12,\n",
    "    )\n",
    "    ax.legend(\n",
    "        loc=\"upper right\",\n",
    "        fontsize=12,\n",
    "        bbox_to_anchor=(0.99, 0.9),\n",
    "        borderaxespad=0,\n",
    "        frameon=False,\n",
    "    )\n",
    "    ax.plot(srs, rank, ls=\":\", color=\"k\")\n",
    "    ax.set_xticks([0.0, 0.5, 1.0, 1.5])\n",
    "    ax.set_xlabel(\"SR\", fontsize=14)\n",
    "    ax.set_ylabel(r\"$\\mathrm{C}$\", fontsize=14)\n",
    "axes[0].set_title(r\"group by $d$: degree\")\n",
    "axes[1].set_title(r\"group by $D$: the set of degree\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier in the above cell, the [`UnivariateViewer.to_dataframe`](https://rc-bootcamp.github.io/ipc-module/profiler/#ipc_module.profiler.UnivariateViewer.to_dataframe) method can be used to retrieve the calculation results in the [`polars.DataFrame`](https://docs.pola.rs/py-polars/html/reference/dataframe/) format.\n",
    "[`polars`](https://pola.rs/) is a data analysis library similar to [`pandas`](https://pandas.pydata.org/), but it operates much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, rank = viewer.to_dataframe(max_scale=2.0)  # NOTE: Threshold is scaled by max_scale.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the [official documentation](https://docs.pola.rs/user-guide/getting-started/) for detailed usage of `polars`.\n",
    "The next section introduces representative visualization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization for $|D|=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the number of elements in $D$ is 1, i.e., $D \\in \\{\\{1\\}, \\{2\\}, \\{3\\},~\\ldots\\}$, the capacity $\\mathrm{C}$ can be visualized as a one-dimensional graph.\n",
    "By specifying a negative value as an argument to `viewer.to_dataframe`, you can extract only those with the absolute value of the number of elements in $D$ (e.g., `df = viewer.to_dataframe(-1)`).\n",
    "In the graph below, $\\mathrm{C}[x, \\mathcal{P}_d(\\zeta^\\tau)]$ is visualized for the data of each spectral radius.\n",
    "Additionally, instead of summing the elements in the `DataFrame`, you can use the `viewer.total` method to calculate the total capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_scale = 2.0\n",
    "degrees = [1, 2, 3]\n",
    "sr_ids = [4, 9, 14]\n",
    "\n",
    "df, rank = viewer.to_dataframe(-1, max_scale=0.0)  # NOTE: No truncation.\n",
    "\n",
    "grid_size = (len(sr_ids), len(degrees))\n",
    "fig, axes = plt.subplots(\n",
    "    *grid_size, figsize=(grid_size[1] * 4, grid_size[0] * 3), gridspec_kw=dict(hspace=0.1, wspace=0.1)\n",
    ")\n",
    "\n",
    "for (idy, sr_id), (idx, degree) in itertools.product(enumerate(sr_ids), enumerate(degrees)):\n",
    "    scale = viewer.calc_surr_max((degree,), max_scale=max_scale)[sr_id, 0]\n",
    "    capacity = viewer.total((degree,), max_scale=max_scale)[sr_id]\n",
    "    ax = axes[idy, idx]\n",
    "    df_sub = df.filter(df[\"degree\"] == degree).sort(\"del_0\")  # NOTE: Filter by degree.\n",
    "    delays = df_sub[\"del_0\"]\n",
    "    ipc = df_sub[f\"ipc_{sr_id}\"]\n",
    "    ax.plot(\n",
    "        delays,\n",
    "        ipc,\n",
    "        color=f\"C{idx}\",\n",
    "        lw=1,\n",
    "        label=f\"SR={srs[sr_id]:.2f}\",\n",
    "    )\n",
    "    ax.line_y(scale, color=\"red\", lw=1, ls=\"--\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_ylim([1e-4, 1.1])\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    if idy == 0:\n",
    "        ax.set_title(f\"$d={degree}$\", fontsize=14)\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel(\"SR=\" + f\"{srs[sr_id]:.2f}\", fontsize=14)\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "    if idy < len(sr_ids) - 1:\n",
    "        ax.set_xticklabels([])\n",
    "    else:\n",
    "        ax.set_xlabel(r\"$\\tau$\", fontsize=14)\n",
    "    ax.text(\n",
    "        0.95,\n",
    "        0.95,\n",
    "        \"C={:.2f}\".format(capacity),\n",
    "        fontsize=12,\n",
    "        ha=\"right\",\n",
    "        va=\"top\",\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "fig.suptitle(r\"$\\mathrm{C}[x,\\mathcal{P}_d(\\zeta^\\tau)]$\" + f\", scale={max_scale:.2f}\", fontsize=16)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization for $D=\\{1,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us consider the case where $D=\\{1,1\\}$.\n",
    "Unlike when $|D|=1$, since $D$ has two elements, it can be visualized on a two-dimensional plane.\n",
    "By using `df = viewer.to_dataframe((1, 1))`, you can obtain a `DataFrame` that extracts only the components for $D=\\{1,1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_scale = 2.0\n",
    "\n",
    "df, rank = viewer.to_dataframe((1, 1), max_scale=max_scale)\n",
    "capacities = viewer.total((1, 1), max_scale=max_scale)\n",
    "grid_size = (4, 5)\n",
    "fig = Figure(figsize=(grid_size[1] * 3, grid_size[0] * 3))\n",
    "fig.create_grid(*grid_size, hspace=0.3, wspace=0.3)\n",
    "\n",
    "pos = len(srs)\n",
    "ax_last = fig[pos // grid_size[1], pos % grid_size[1]]\n",
    "ax_last.create_grid(1, 2, width_ratios=[1, 20])\n",
    "cax = ax_last[0]\n",
    "cax.tick_params(labelsize=12)\n",
    "for idx, sr in enumerate(srs):\n",
    "    ax = fig[idx // grid_size[1], idx % grid_size[1]]\n",
    "    df_pivot = df.sort(\"del_0\", \"del_1\").pivot(\n",
    "        \"del_1\",\n",
    "        index=\"del_0\",\n",
    "        values=f\"ipc_{idx}\",\n",
    "    )\n",
    "    mat = df_pivot[:, 1:]\n",
    "    index = df_pivot[:, 0]\n",
    "    columns = list(map(int, df_pivot.columns[1:]))\n",
    "    ax.plot_matrix(\n",
    "        mat,\n",
    "        index=index,\n",
    "        column=columns,\n",
    "        cmap=\"viridis\",\n",
    "        zscale=\"log\",\n",
    "        vmin=1e-4,\n",
    "        vmax=1,\n",
    "        aspect=\"equal\",\n",
    "        xticks_kws=dict(num_tick=7),\n",
    "        yticks_kws=dict(num_tick=7),\n",
    "        colorbar=(idx == len(srs) - 1),\n",
    "        cax=cax if (idx == len(srs) - 1) else None,\n",
    "    )\n",
    "    ax.set_xlim(-0.5, 50.5)\n",
    "    ax.set_ylim(-0.5, 50.5)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax.text(\n",
    "        0.95,\n",
    "        0.95,\n",
    "        \"C={:.2f}\".format(capacities[idx]),\n",
    "        fontsize=12,\n",
    "        ha=\"right\",\n",
    "        va=\"top\",\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    ax.set_title(f\"SR={sr:.2f}\", fontsize=14)\n",
    "\n",
    "for idx in range(len(srs), grid_size[0] * grid_size[1]):\n",
    "    fig.delaxes(fig[idx // grid_size[1], idx % grid_size[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application example: confirming the effect of input symmetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as an example to demonstrate the effectiveness of IPC, let’s examine the impact on symmetric input.\n",
    "Using the same ESN conditions as before, provide the input to the ESN while keeping the scale at $[-1, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5678\n",
    "dim = 50\n",
    "t_washout = 10000\n",
    "t_sample = 100000\n",
    "srs = np.linspace(0.1, 1.7, 17)\n",
    "t_total = t_washout + t_sample\n",
    "display = True\n",
    "\n",
    "rnd = np.random.default_rng(seed)\n",
    "w_in = Linear(1, dim, bound=0.1, bias=0.0, rnd=rnd)\n",
    "net = ESN(dim, sr=srs[:, None], f=np.tanh, p=1, rnd=rnd)\n",
    "\n",
    "x0 = np.zeros((srs.shape[0], dim))\n",
    "us = rnd.uniform(-1, 1, (t_total, 1))\n",
    "\n",
    "x = x0\n",
    "xs = np.zeros((t_total, *x0.shape))\n",
    "for idx in trange(t_total, display=display):\n",
    "    x = net(x, w_in(us[idx]))  # NOTE: Use us[idx] for the symmetric case\n",
    "    xs[idx] = x\n",
    "\n",
    "print(\"us:\", us.shape)\n",
    "print(\"xs:\", xs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will measure the IPC under the same conditions as before.\n",
    "This will also take some time, so please wait.\n",
    "The results will be saved as `./output/ipc_symm.npz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True  # NOTE: Set it to False to run on CPU.\n",
    "\n",
    "if use_gpu:\n",
    "    import torch\n",
    "\n",
    "    assert torch.cuda.is_available(), \"CUDA is not available\"\n",
    "    us_c = torch.from_numpy(us).cuda()\n",
    "    xs_c = torch.from_numpy(xs).cuda()\n",
    "    args = (us_c, xs_c)\n",
    "else:\n",
    "    args = (us, xs)\n",
    "\n",
    "profiler = UnivariateProfiler(\n",
    "    *args,\n",
    "    \"Legendre\",\n",
    "    offset=t_washout,\n",
    "    surrogate_num=1000,\n",
    "    axis1=0,\n",
    "    axis2=-1,\n",
    ")\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "taus = [1000, 300, 50, 30, 15]\n",
    "for deg, tau in zip(degrees, taus, strict=True):\n",
    "    profiler.calc(deg, tau + 1)\n",
    "\n",
    "print(profiler.keys())\n",
    "\n",
    "profiler.save(\"./result/ipc_symm.npz\", srs=srs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads data from both `ipc_asym.npz` and `ipc_symm.npz` and compares them in a figure.\n",
    "For `ipc_symm.npz`, which uses symmetric input, the even-order components are expected to vanish because the activation function $\\tanh$ is an odd function.\n",
    "What will the result be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"./result/ipc_asym.npz\", \"./result/ipc_symm.npz\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(files), figsize=(16, 6), gridspec_kw=dict(hspace=0.5))\n",
    "for idx, file in enumerate(files):\n",
    "    viewer = UnivariateViewer(file)\n",
    "    srs = viewer.info[\"srs\"]\n",
    "    df, rank = viewer.to_dataframe(max_scale=2.0)  # NOTE: Threshold is scaled by max_scale.\n",
    "    ax = axes[idx]\n",
    "    visualize_dataframe(\n",
    "        ax,\n",
    "        df,\n",
    "        xticks=srs,\n",
    "        threshold=0.1,\n",
    "        cmap=\"tab10\",\n",
    "        group_by=\"component\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    ax.legend(\n",
    "        loc=\"upper right\",\n",
    "        fontsize=12,\n",
    "        bbox_to_anchor=(0.99, 0.9),\n",
    "        borderaxespad=0,\n",
    "        frameon=False,\n",
    "    )\n",
    "    ax.plot(srs, rank, ls=\":\", color=\"k\")\n",
    "    ax.set_xticks([0.0, 0.5, 1.0, 1.5])\n",
    "    ax.set_xlabel(\"SR\", fontsize=14)\n",
    "    ax.set_ylabel(r\"$\\mathrm{C}$\", fontsize=14)\n",
    "    ax.set_title(file, fontsize=16)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous chapter, let’s plot and compare the bifurcation diagram along with the calculation of the conditional Lyapunov exponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-4\n",
    "net.sr = np.linspace(0.1, 1.7, 161)[:, None]\n",
    "\n",
    "x0 = np.zeros((2, net.sr.shape[0], dim))\n",
    "us = rnd.uniform(-1, 1, (t_total, 1))\n",
    "\n",
    "t_washout, t_sample = 1000, 20000\n",
    "ts = np.arange(-t_washout, t_sample)\n",
    "\n",
    "x = x0\n",
    "xs = np.zeros((t_total, *x0.shape[1:]))\n",
    "lmbds = np.zeros((t_sample, net.sr.shape[0]))\n",
    "for idx, t in enumerate(tqdm(ts, display=display)):\n",
    "    if t == 0:\n",
    "        pert = rnd.uniform(-1, 1, x[0].shape)\n",
    "        pert = pert / np.linalg.norm(pert, axis=-1, keepdims=True)\n",
    "        x[1] = x[0] + pert * eps\n",
    "    x = net(x, w_in(us[idx]))\n",
    "    xs[idx] = x[0]\n",
    "    if t >= 0:\n",
    "        x_org, x_per = x[0], x[1]\n",
    "        x_diff = x_per - x_org\n",
    "        d_post = np.linalg.norm(x_diff, axis=-1, keepdims=True)\n",
    "        lmbd = np.log(np.abs(d_post / eps))\n",
    "        x_per[:] = x_org + x_diff * (eps / d_post)\n",
    "        lmbds[idx - t_washout] = lmbd[..., 0]\n",
    "\n",
    "\n",
    "def get_maxima_and_minima(xs, **kwargs):\n",
    "    id_maxima = sp.signal.find_peaks(xs, **kwargs)[0]\n",
    "    id_minima = sp.signal.find_peaks(-xs, **kwargs)[0]\n",
    "    return id_maxima, id_minima\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, sharex=True, figsize=(8, 10), gridspec_kw=dict(hspace=0.05))\n",
    "axl = axes[0]\n",
    "axl.set_xticklabels([])\n",
    "for idx, sr in enumerate(net.sr):\n",
    "    id_maxima, id_minima = get_maxima_and_minima(xs[t_washout:, idx, 0])\n",
    "    id_all = np.concatenate([id_maxima, id_minima])\n",
    "    peaks = xs[t_washout:, idx, 0][id_all]\n",
    "    axl.scatter(sr * np.ones(peaks.shape[0]), peaks, marker=\".\", s=0.01, color=\"k\")\n",
    "axl.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "axl.set_ylabel(r\"$x_0[k]$\", fontsize=14)\n",
    "axl.set_yticks([-1.0, 0.0, 1.0])\n",
    "axl.set_ylim(-1.1, 1.1)\n",
    "\n",
    "axr = axes[0].twinx()\n",
    "axr.plot(net.sr, lmbds.mean(axis=0), \"o-\", color=\"red\", label=\"MLE\")\n",
    "axr.set_yticks([-0.2, 0.0, 0.2])\n",
    "axr.set_ylim(-0.22, 0.22)\n",
    "axr.set_ylabel(r\"MLE: $\\lambda$\", fontsize=14)\n",
    "axr.set_xticklabels([])\n",
    "axr.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "\n",
    "viewer = UnivariateViewer(\"./result/ipc_symm.npz\")\n",
    "srs = viewer.info[\"srs\"]\n",
    "df, rank = viewer.to_dataframe(max_scale=2.0)  # NOTE: Threshold is scaled by max_scale.\n",
    "ax = axes[1]\n",
    "visualize_dataframe(\n",
    "    ax,\n",
    "    df,\n",
    "    xticks=srs,\n",
    "    threshold=0.1,\n",
    "    cmap=\"tab10\",\n",
    "    group_by=\"component\",\n",
    "    fontsize=12,\n",
    ")\n",
    "ax.legend(\n",
    "    loc=\"upper right\",\n",
    "    fontsize=12,\n",
    "    bbox_to_anchor=(0.99, 0.9),\n",
    "    borderaxespad=0,\n",
    "    frameon=False,\n",
    ")\n",
    "ax.set_xlabel(\"SR\", fontsize=14)\n",
    "ax.set_ylabel(r\"$\\mathrm{C}$\", fontsize=14)\n",
    "\n",
    "for ax in [axl, axr, axes[1]]:\n",
    "    ax.plot(srs, rank, ls=\":\", color=\"k\")\n",
    "    ax.set_xticks([0.0, 0.5, 1.0, 1.5, 2.0])\n",
    "    ax.set_xlim(srs.min() - 0.1, srs.max() + 0.1)\n",
    "fig.align_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use `narma_func` to measure the IPC for NARMA10 and reproduce the results in Figure 3 of [5].\n",
    "- Compare the IPC of NARMA and ESN, and discuss the conditions under which the ESN can solve NARMA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Dambre, J., Verstraeten, D., Schrauwen, B., & Massar, S. (2012). *Information Processing Capacity of Dynamical Systems*. Scientific Reports, 2(1), 514. https://doi.org/10.1038/srep00514\n",
    "\n",
    "[2] Xiu, D., & Karniadakis, G. E. (2002). *The Wiener--Askey Polynomial Chaos for Stochastic Differential Equations*. SIAM Journal on Scientific Computing, 24(2), 619–644. https://doi.org/10.1137/S1064827501387826\n",
    "\n",
    "[3] Hardy, G. H., & Ramanujan, S. (1918). *Asymptotic Formulaæ in Combinatory Analysis*. Proceedings of the London Mathematical Society, s2-17(1), 75–115. https://doi.org/10.1112/plms/s2-17.1.75\n",
    "\n",
    "[4] Theiler, J., Eubank, S., Longtin, A., Galdrikian, B., & Doyne Farmer, J. (1992). *Testing for Nonlinearity in Time Series: The Method of Surrogate Data*. Physica D: Nonlinear Phenomena, 58(1), 77–94. https://doi.org/10.1016/0167-2789(92)90102-S\n",
    "\n",
    "[5] Kubota, T., Takahashi, H., & Nakajima, K. (2021). *Unifying Framework for Information Processing in Stochastically Driven Dynamical Systems*. Physical Review Research, 3(4), 043135. https://doi.org/10.1103/PhysRevResearch.3.043135"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rc-bootcamp (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
